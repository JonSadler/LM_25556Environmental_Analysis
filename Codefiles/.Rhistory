y <- c(13, 7, 5, 12, 9, 15, 6, 11, 9, 7, 12) # create some basic data
plot(y, ylim = c(0, 20))    # plot it as a scatter limiting the axis 0 - 20
How can we quantify this scatter? We could use the range
ozone <- read.csv(file.choose())
# look at it
str(ozone)
head(ozone)
# But first we need to subset the data to pull out the bits we need for the comparison. We did this last week with the same data
GardA <- ozone[ozone$Garden == "A", ]
# And the same using the subset function
GardB <- subset(ozone, Garden == "B")
# Make a picture and check the assumptions
par(mfrow = c(2, 1))
#Add histograms with a fixed range for the X axis
hist(GardA$O3, xlim = c(0, 7), main = "", col = "Darkblue", xlab = "Garden A")
hist(GardB$O3, xlim = c(0, 7), main = "", col = "Lightblue", xlab = "Garden B")
# turn off former graphics device
dev.off()
GardB
var(GardB$O3) # for garden B
library(car)
install.packages("CAar")
install.packages("car")
library(car)
leveneTest(O3 ~ Garden, data = ozone)
t.test(GardA$O3, GardB$O3)
t.test(O3 ~ Garden, data = ozone) # Note - the data argument and '~' (tilde) operator (you'll see a lot of this)
wilcox.test(O3 ~ Garden, data = ozone)
install.packages("coin")
library(coin)
wilcox_test(O3 ~ Garden, data = ozone, distribution = "exact")
H <- c(1,2,2,5,6,5,2,1)   					# Create samples
Sam <- c(1,1,1,2,2,2,3,3) 					# Categorize samples
A.test <- data.frame(H, Sam)				# Create data frame
A.test$Sam <- as.factor(A.test$Sam)			# Make sure sample is a factor - remember we entered as a vector of integers
str(A.test)
A.test
hist(Sam, xlab = "Sample", main = "", col = "grey")
View(A.test)
Anova.run <- aov(H ~ Sam, data = A.test)
summary(Anova.run)
TukeyHSD(Anova.run)
TukeyHSD(Anova.run)
summary(Anova.run)
KW.test <- kruskal.test(A.test$H ~ A.test$Sam)
KW.test # Look at the results
summary(Anova.run)
Graze <- read.csv(file.choose())
table(Graze$Field, Graze$Grazing)
replications(Abund ~ Grazing * Field , data = Graze)
!is.list(replications(Abund ~ Grazing * Field , data = Graze)) # We're safe !!!!
hist(Graze$Abund)
Field1 <- subset(Graze,Field == "Lower")
Field2 <- subset(Graze, Field == "Top")
hist(Graze$Abund)
par(mfrow = c(2, 1))
par(mfrow = c(2, 1))
hist(Field1$Abund, xlim = c(0, 40), main = "", col = "Darkblue", xlab = "Field 1")
hist(Field2$Abund, xlim = c(0, 40), main = "", col = "Lightblue", xlab = "Field 2")
par(mfrow = c(2, 1))
par(mfrow = c(2, 1))
par(mfrow = c(2, 1))
hist(Field1$Abund, xlim = c(0, 40), main = "", col = "Darkblue", xlab = "Field 1")
hist(Field2$Abund, xlim = c(0, 40), main = "", col = "Lightblue", xlab = "Field 2")
par(mfrow = c(2, 1))
boxplot(Abund ~ Field, data = Graze)
boxplot(Abund ~ Grazing, data = Graze)
boxplot(Abund ~ Field * Grazing, ylab =  "Abundance of Rye Grass", xlab = "Contrasts of field and grazing level", data = Graze)
coplot(Abund ~ Grazing | Field, panel = panel.smooth, xlab = "Grazing Level", ylab = "Rye grass abundance", data = Graze)
Graze$Grazing <- factor(Graze$Grazing, levels = c("Low", "Mid", "High"), ordered = TRUE)
Graze$Grazing <- factor(Graze$Grazing, levels = c("Low", "Mid", "High"), ordered = TRUE)
Grazeaov <- aov(Abund ~ Field + Grazing, data = Graze)
summary(Grazeaov)
Grazeaov1 <- aov(Abund ~ Field * Grazing, data = Graze)
summary(Grazeaov1)
interaction.plot(Graze$Grazing, Graze$Field, Graze$Abund, col=c(2,3), xlab = "Grazing Regime", ylab = "Rye Grass abundance", trace.label = "Field")
library(dplyr)
install.packages("tidyverse")
Fit <- c(31,45,78,987,12,456)
X <- c(52.2,57,4,78,98,43)
Y <- c(1,2,4,5,8,2)
data <- as.dataframe(cbind(c(fit, x,y)))
data <- as.data.frame(cbind(c(fit, x,y)))
data <- as.data.frame(cbind(c(Fit, x,y)))
data <- as.data.frame(cbind(c(Fit, X,Y)))
View(data)
data <- as.data.frame(rbind(c(Fit, X,Y)))
data <- as.data.frame(Fit,X,Y)
View(data)
data <-
data.frame(Fit,X,Y)
View(data)
# Check the data structure using the scatterplotMatrix function in the car library
library(car)
library(tidyverse)
gotelli <- read.csv(file.choose())
glimpse(gotelli)
scatterplotMatrix(~Srich + as.factor(Habitat) + Latitude + Elevation, data = gotelli, diag = list(method = "boxplot"))
# Now check for inflation using VIFs. Run the model and look at the VIFs
gotelli.glm <- glm(Srich ~ Habitat + Latitude + Elevation, family = poisson, data  = gotelli)
# VIFs
vif(gotelli.glm)
gotelli$cLatitude <- scale(gotelli$Latitude, scale = T)
gotelli$cElevation <- scale(gotelli$Elevation, scale = T)
# Rerun the VIF code with the new variables
gotelli.glm <- glm(Srich ~ as.factor(Habitat) * cLatitude * cElevation, family = poisson, data  = gotelli)
vif(gotelli.glm)  # They are bit high but okay so we'll go with it....
# Check for influential data points outliers
# first we'll use influence measures to do it....
influence.measures(gotelli.glm)    # There are a couple of large cook values but they are not near 1!
# graphically plot the Cooks distances
plot(gotelli.glm, which = 4)  # few biggies but not too worrying
# check for over dispersion - recall we are looking for values around 1 (i.e. certainly not over 2 nor under 0.5)
# this is the model deviance / degrees of freedom of the residuals
gotelli.glm$deviance/gotelli.glm$df.resid
# so everything is okay so look at the results
summary(gotelli.glm)
# Lots of variables here so go for some model averaging....
library(MuMIn)
options(na.action=na.fail) # set options in Base R concerning missing values
summary(model.avg(dredge(gotelli.glm), fit = TRUE, subset = TRUE))
options(na.action = "na.omit") # reset base R options
#Â Best model includes latitude, elevation and habitat
# recreate the best model. Notice we go to the uncentred variables because we want them axes to be the original measurements
gotelli.glm <- glm(Srich ~ Habitat + Latitude + Elevation, family=poisson, data=gotelli)
summary(gotelli.glm)
# Check diagnostic plots
op <- par(mfrow = c(2, 2))
plot(gotelli.glm) # looking good
par(op)
library(ggfortify)
autoplot(gotelli.glm, which=1:6)
xs <- seq(40, 45, l = 1000)
plot(Srich ~ Latitude, data = gotelli, xlab = "Latitude", ylab = "Ant Species Richness")
# Plot the points and predicted trends
points(Srich ~ Latitude, data = gotelli, subset = Habitat == "Forest", pch = 16)
pred <- predict(gotelli.glm, type = "response", se = TRUE, newdata = data.frame(Latitude = xs, Habitat = "Forest", Elevation = mean(gotelli$Elevation)))
lines(pred$fit ~ xs)
points(Srich ~ Latitude, data = gotelli, subset = Habitat == "Bog", pch = 21)
pred <- predict(gotelli.glm, type = "response", se = TRUE, newdata = data.frame(Latitude = xs, Habitat = "Bog", Elevation = mean(gotelli$Elevation)))
lines(pred$fit ~ xs)
legend("topright", legend = c("Forest", "Bog"), pch = c(16, 21), title = "Habitat", bty = "n")
box(bty = "l")
# Environment setting to ensure you have rights to install a library on a network computer
.libPaths("C:/Program Files/R/R-3.3.0/library")
#Check where R is looking
getwd()
View(gotelli)
LengthCT <- c(75, 85, 91.6, 95, NA, 105.1, 106)
# Also create a variable that contains the Tb values
Tb <- c(0, 0, 1, NA, 0, 0, 0)
# What's the average length, sd of the length of the cattle on the farms?
Av.length <-mean(LengthCT, na.rm = TRUE)  # You need the na.rm = TRUE because there is a missing value in the vector
Sd.length <-sd(LengthCT, na.rm = TRUE)
# Display the calculations
Av.length
Sd.length
source("~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Codefiles/Week 6_RScript.R", echo=TRUE)
Loyn <- read.csv("~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/loyn.csv")
require(car)
scatterplotMatrix(~ABUND + AREA + YR.ISOL + DIST + LDIST +
GRAZE + ALT, data = Loyn, diag = list(method = "boxplot"))
# Okay so let's repeat this with some transformationsrequire(car)
scatterplotMatrix(~ABUND + log10(AREA) + YR.ISOL + log10(DIST) + log10(LDIST) +
GRAZE + ALT, data = Loyn, diag = list(method = "boxplot"))
# 2. Check for multicollinearity
# First by correlation
cor(Loyn[, 2:7])		# select columns 2 - 7 all the explanatory variables!
# Then use Variation inflation Factors (VIFs) and Tolerances to examine whether they'll impact the linear regression
vif(lm(ABUND ~ log10(AREA) + YR.ISOL + log10(DIST) + log10(LDIST) +
GRAZE + ALT, data = Loyn))
??update
# Then use Variation inflation Factors (VIFs) and Tolerances to examine whether they'll impact the linear regression
vif(lm(ABUND ~ log10(AREA) + YR.ISOL + log10(DIST) + log10(LDIST) +
GRAZE + ALT, data = Loyn))
# We want our VIFs to be less than 3
Loyn.lm <- lm(ABUND ~ log10(AREA) + YR.ISOL + log10(DIST) + log10(LDIST) +
GRAZE + ALT, data = Loyn)
# Validate the model
op <- par(mfrow = c(2,2))
plot(Loyn.lm)
par(op)
# In ggfortigy
library(ggfortify)
autoplot(Loyn.lm)
# Check residuals against explanatory variables
op <- par(mfrow = c(3,3))
plot(Loyn.lm$resid ~ log10(Loyn$AREA)) 	# Looks okay
plot(Loyn.lm$resid ~ Loyn$YR.ISOL)		# Looks okay
plot(Loyn.lm$resid ~ log10(Loyn$DIST))	# Looks okay
plot(Loyn.lm$resid ~ log10(Loyn$LDIST))	# Looks okay
plot(Loyn.lm$resid ~ Loyn$ALT)			# Looks okay
boxplot(Loyn.lm$resid ~ Loyn$GRAZE)		# Don't like the look of this one. We can deal with another way - but not today
par(op)
# We assume everything is good (as the published paper indicated) and look at the results
summary(Loyn.lm)
# To finish we plot the slope partials....
avPlots(Loyn.lm)			# This is a call to the car package
Pareulo <- read.csv(file.choose())
Pareulo <- read.csv("~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/paruelo.csv")
scatterplotMatrix(~ C3 + MAP + MAT + JJAMAP + DJFMAP +
LONG + LAT, data = Pareulo, diag = list(method = "boxplot"))
# So we log the response and repeat...
scatterplotMatrix(~log10(C3 + 0.1) + MAP + MAT + JJAMAP + DJFMAP +
LONG + LAT, data = Pareulo, diag = list(method = "boxplot"))
# 2. Check for multicollinearity
# First by correlation
cor(Pareulo[, 2:7])		# select columns 2 - 7 all the explanatory variables!
# Then use Variation inflation Factors (VIFs) to examine whether they'll impact the linear regression
vif(lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP + DJFMAP +
LONG + LAT, data = Pareulo))
# Solution is to remove the offending variables. We can start with LAT and LONG
# because climate variables are always correlated to geographic distance
vif(lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP + DJFMAP, data = Pareulo))
# Solution is to remove the offending variables. We can start with LAT and LONG
# because climate variables are always correlated to geographic distance
vif(lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP + DJFMAP, data = Pareulo))
# Solution is to remove the offending variables. We can start with LAT and LONG
# because climate variables are always correlated to geographic distance
vif(lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP + DJFMAP, data = Pareulo))
# Much better but still the two summer and winter ppt figures are a problem (they are intercorrelated)
# Use some ecological thinking here. summer ppt should be more important to C3 plants so remove DJMAP
vif(lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP, data = Pareulo))
# Now we are good to go.....we are planning to look for interactions so the full model is:
M1 <- lm(log10(C3 + 0.1) ~ MAP * MAT * JJAMAP, data = Pareulo) # full model
# Look at it...
summary(M1)
M2 <- lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP+ MAP:MAT + MAP:JJAMAP + MAT:JJAMAP, data = Pareulo)   # remove 3-term first
summary(M2)
M3 <- lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP+ MAT:JJAMAP, data = Pareulo) 	# Remove non-sig. 2-way terms
summary(M3)
M4 <- lm(log10(C3 + 0.1) ~ MAP + JJAMAP+ MAT:JJAMAP, data = Pareulo) 	# Remove least significant main term MAT
summary(M4)
autoplot(M4)
# This is our best model...check assumptions
autoplot(M4)
op <- par(mfrow = c(2,2))
plot(M4)
par(op)
# These look okay but scale - location is slightly humped. Made worse by red line ; points look okay
# plot residuals against explanatory variables
op <- par(mfrow = c(2,2))
plot(M4$resid ~ Pareulo$MAP) 	# Looks okay
plot(M4$resid ~ Pareulo$MAT)	# Looks okay
plot(M4$resid ~ Pareulo$JJAMAP)	# Looks okay
par(op)
# Look at the partial plots....
avPlots(M4)
# Let's start by using the lyon.csv data and using an automated step function disregarding Whittingham et al.
# for the time being...
# Load data if you haven't already done so (loyn.csv)
Loyn <- read.csv(file.choose())
# Let's start by using the lyon.csv data and using an automated step function disregarding Whittingham et al.
# for the time being...
# Load data if you haven't already done so (loyn.csv)
Loyn <- read.csv("~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/loyn.csv")
# We'll use the model we were happy with after the analyses above. Run it again:
Loyn.lm <- lm(ABUND ~ log10(AREA) + YR.ISOL + log10(DIST) + log10(LDIST) +
GRAZE + ALT, data = Loyn)
# Look at the output  - you'll see some variables are not significant influences on Y (our response)
summary(Loyn.lm)
step(Loyn.lm)
Loyn.lm1 <- lm(ABUND ~ log10(AREA) + YR.ISOL + GRAZE, data = Loyn)
summary(Loyn.lm1)
# Load the package
if(!require(MuMIn)){install.packages("MuMIn")} # Checks to see if you need to install the package.
library(MuMIn) # New package do some reading...
options(na.action=na.fail) # set options in Base R concerning missing values
Loyn.lm2 <- model.avg(dredge(Loyn.lm, rank = "AICc")) # code introduces model.avg(), get.models and dredge functions
summary(Loyn.lm2)
Loyn.lm.final <- lm(ABUND ~ log10(AREA) + GRAZE, data = Loyn)
summary(Loyn.lm.final)
options(na.action = "na.omit") # reset base R options
# Validate
op <- par(mfrow = c(2,2))
plot(Loyn.lm.final)
par(op)
# Look at the partials
avPlots(Loyn.lm.final)
# Check the coefficients
coef(Loyn.lm.final)
# These look okay but scale - location is slightly humped. Made worse by red line ; points look okay
# plot residuals against explanatory variables
op <- par(mfrow = c(2,2))
plot(M4$resid ~ Pareulo$MAP) 	# Looks okay
plot(M4$resid ~ Pareulo$MAT)	# Looks okay
plot(M4$resid ~ Pareulo$JJAMAP)	# Looks okay
par(op)
# Solution is to remove the offending variables. We can start with LAT and LONG
# because climate variables are always correlated to geographic distance
gvif(lm(log10(C3 + 0.1) ~ MAP + MAT + JJAMAP + DJFMAP, data = Pareulo))
# Look at the partial plots....
avPlots(M4)
