# Dealing with data heterogeneity using mixed models

Last week we covered the application of **Generalised Additive Models (GAMs)** to data that did not fit a key assumption of GLMs, i.e. linearity or the $x$ `~` $y$ relationship. This week, for our final session, we focus how we might deal with situations where we have structural dependencies in our data that do no meet the key assumption for linear, GLM and GAM models: **independence of sample points**. Deciding whether or not to use **mixed models** not as straightforward as it seems because it depends on the study design, objectives and data structure.

But let's start by thinking of them as an extension to ANOVA/ANCOVA and a means of dealing with heterogeneity in our data. We'll explore how heterogeneity arises in the data. If you are unlucky it arises because you have messed up your sample design. Hopefully, this is not the case, rather, it is a response to dependence issues that are factored into your design. Figure 9.1. shows two simple scenarios where the application of mixed modelling (either ANOVA or regression models) is a necessity. In panel (A), we have a block experiment that mirrors our fertilizer example from week 4 with one important difference. We have 3 subsamples from within each treatment within each block. The subsamples are nested with the treatments and not independent from each other. We need to factor this into our regression, otherwise our samples are **pseudoreplicated** [@Hurlbert1984].

[TASK: Take a few mins chatting with your neighbour thinking about why this is the case \[\~ 5 min\]]{style="color:red;"}.

Panel (B), shows the situation where we have repeated measurement at each site over three time periods. Let's say we are counting bird species at each site in Spring, Summer and Autumn and we take one sample in each season. We need to account for this because repeated measurements from the individual sites are not independent. The samples across the seasons, *within* each site will be more similar to each other seasonal samples *between* the sites (a characteristic know as **temporal autocorrelation**). Please note there are numerous ways of dealing with autocorrelation, such as Geographically Weighted Regression (GRWs) or Spatial Regression Models (SRMs) for spatial issues and Time Series analysis for temporal dependencies. There are is an example in the book Appendix for time series work.

```{r}
#| label: mixed_model
#| echo: false
knitr::include_graphics("images/mixed_models_examples.png")
```

**Fig. 9.1: Mixed model example sample designs where some form of mixed model (either mixed model ANOVA or mixed regression models) is necessary**

## Today's Session

Today's work is all about sorting out these potential analytical problems. And in doing so, hopefully we can support you in your project analyses over summer!

### Learning Outcomes

By the end of this session you will be able to:

-   Explore data to look for dependencies and potential autocorrelative structures\\

-   Use straightforward linear mixed effects models to resolve the potential issues

-   Anything more complex and we will need to discuss options and approaches!

### Load libraries

```{r}
# List of packages
packages <- c("tidyverse", "ggfortify", "performance", "car", "skimr", "patchwork", "broom", 
"ggeffects","MASS","MuMIn", "lme4","glmmTMB", "DHARMa")
# Load all packages and install the packages we have no previously installed on the system
lapply(packages, library, character.only = TRUE)
```

The new packages this week are **lme4**, **glmmTMB** and **DHARMa:**

-   **lme4** - The lme4 package provides functions to fit and analyze linear mixed models, generalized linear mixed models and nonlinear mixed models [@Bates2015d].
-   **glmmTMB** - An extremely powerful and complex multi-level modelling package for Generalised Linear Mixed Models (GLMMs) [@Brooks2017b]. Permits the application of zero-inflated mixed models.
-   **DHARMa** - Provides functions that provide simulated residual diagnostics for mixed models along with numerous tests for zero-inflation, over dispersion and so on [@Hartig2024].
-   **patchwork** - allow the multipanel composition of plots including, graphics files and ggplot objects \[Pedersen2024\].

## Essential Reading

Read these three introductory papers on mixed modelling, if you haven't already done so (after the class would make sense!):

-   Start with chapter 8, 'Mixed Models', in @Zuur2007.

-   Then @Harrison2018a.

-   Finally, the classic introduction by @Bolker2009.

## What are mixed models?

You are all familiar with applying regression models to data using **fixed effects**, the $x$ (or explanatory) components in the models. **Fixed effects** are effects of predictors that are assumed to be the same across all units (e.g. treatment, temperature, altitude). A **mixed model** is a regression model that includes both:

-   **Fixed effects** and
-   **Random effects**. **Random effects** allow certain parameters to vary across groups / clusters / units (e.g. intercepts, slopes for individual subjects, plots, regions). They model correlation or non-independence in the data.

They are useful when data are hierarchically structured (e.g. measurements nested within subjects, spatial clusters, repeated measurements over time) or when you have grouped / clustered observations.

### Variance Partitioning

-   Fixed effects explain systematic, population-wide variation.

-   Random effects explain the clustering/nesting, and the

-   Residuals capture leftover within-group variation.

So the model partitions variance into:

-   Between-groups (random effects)

-   Within-groups (residuals).

### Visualising a mixed model

You will remember from numerous sessions that a regression generates two coefficients, an intercept and a slope (for all the explanatory variables). Mixed models use the random structure in the model to capture variability in two ways. You can use them to generate:

-   a **random intercept model** - where we allow the intercepts of our explanatory variables to vary, or

-   a **random intercept and slope model** - where both the intercept and slopes can vary (Fig. 9.2).

```{r, echo=FALSE}
# SIMULATION PARAMETERS: Define the rules for our data 
# Code generated by Gemini 2.0 - modified by JPS

# Use set.seed() to make our "random" data reproducible.
set.seed(42)

# Define the basic parameters for the experiment
n_groups <- 5      # Number of groups (e.g., 5 different individuals)
n_obs_per_group <- 11 # Number of observations per group (from X=0 to X=10)
x <- 0:10          # The predictor variable values

# Define the "fixed" or population-level effects (the thick black line)
pop_intercept <- 2.0  # The overall average intercept
pop_slope <- 0.5     # The overall average slope
residual_sd <- 0.8   # The random "noise" for each individual point

# Define the variance of the "random" effects
intercept_sd <- 2.0  # How much the group intercepts vary around the mean
slope_sd <- 0.8      # How much the group slopes vary (increased for more visual difference)


# --- DATA GENERATION: Create the two datasets ---

# First, generate the unique random effects for each of the 5 groups
random_intercepts <- rnorm(n_groups, mean = 0, sd = intercept_sd)
random_slopes <- rnorm(n_groups, mean = 0, sd = slope_sd)

# Generate the "Random Intercepts" dataset
data_ri <- map_df(1:n_groups, ~{
  tibble(
    group = as.character(.x),
    x = x,
    # Formula: y = (overall_intercept + group_specific_intercept) + (overall_slope * x) + error
    y = (pop_intercept + random_intercepts[.x]) + (pop_slope * x) + rnorm(n_obs_per_group, 0, residual_sd),
    model_type = "Random Intercepts Model"
  )
})

# Generate the "Random Slopes and Intercepts" dataset
data_rs <- map_df(1:n_groups, ~{
  tibble(
    group = as.character(.x),
    x = x,
    # Formula: y = (overall_intercept + group_specific_intercept) + (overall_slope + group_specific_slope) * x + error
    y = (pop_intercept + random_intercepts[.x]) + (pop_slope + random_slopes[.x]) * x + rnorm(n_obs_per_group, 0, residual_sd),
    model_type = "Random Slopes + Intercepts Model"
  )
})

# --- PLOTTING: Combine data and create the final figure ---

# Combine both datasets into a single data frame for plotting
full_data <- bind_rows(data_ri, data_rs)

# Create a small data frame to hold the annotation text and arrow coordinates
annotations <- tibble(
  model_type = c("Random Intercepts Model", "Random Slopes + Intercepts Model"),
  label = c("Groups differ\nin baseline (intercept)", "Groups differ in\nboth baseline & slope"),
  x_arrow = c(0, 7),
  y_arrow = c(10.5, 10.5),
  x_text = c(0.5, 3),
  y_text = c(11, 11),
  x_arrow_end = c(0, 7),
  y_arrow_end = c(5, 5) 
)

# Create the final plot using ggplot2
ggplot(full_data, aes(x = x, y = y, color = group, group = group)) +
  
  # Add the individual group lines (dashed and now fully opaque with alpha=1)
  geom_line(linetype = "dashed", alpha = 1) +
  
  # Add the individual data points (also fully opaque)
  geom_point(alpha = 1) +
  
  # Add the overall "fixed effect" or population-level line
  geom_abline(intercept = pop_intercept, slope = pop_slope, 
              color = "black", linewidth = 1.5) +
              
  # Add the arrows using the annotations data frame
  geom_segment(
    data = annotations,
    aes(x = x_arrow, y = y_arrow, xend = x_arrow_end, yend = y_arrow_end),
    arrow = arrow(length = unit(0.3, "cm")),
    inherit.aes = FALSE # This prevents ggplot from trying to color the arrows
  ) +
  
  # Add the text labels using the annotations data frame
  geom_text(
    data = annotations,
    aes(x = x_text, y = y_text, label = label),
    hjust = 0, vjust = 0,
    inherit.aes = FALSE
  ) +
  
  # Use facet_wrap to create the two side-by-side plots
  facet_wrap(~ model_type) +
  
  # --- Final aesthetic touches ---
  scale_color_brewer(palette = "Set2") +
  theme_bw() +
  labs(x = "Predictor (X)", y = "Response (Y)") +
  theme(
    legend.position = "none", # Hide the legend as it's redundant
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(linetype = "dashed", color = "grey85"),
    strip.background = element_blank(), # Remove facet title background
    strip.text = element_text(face = "bold", size = 14), # Style facet titles
    axis.title = element_text(size = 12)
  )
```

**Fig. 9.2: Examples of a random intercept (left panel) and a random intercept and random slope model (right panel).The *black line* = fixed effect (population regression line). The *coloured lines* represent the various groups**

The left panel in Figure 9.2 shows the situation where the slopes of the various group measurements are the same but where the intercepts differ. Look at where the lines sit on the Y axis. The panel on the right shows the situation where both the intercepts and the slopes vary.

### A little bit of maths

We can represent these models in same mathematical way as we did with LMs and GLMs. The extra element is the addition of the grouping variable (this will be a factor class variable):

**General Form (Linear Mixed Model, LMM)**

$$
y_{ij} = \beta_0 + \beta_1 x_{ij} + u_{0j} + u_{1j}x_{ij} + \varepsilon_{ij}
$$ **Where:**

-   $y_{ij}$: outcome for observation *i* in group *j*\
-   $\beta_0, \beta_1$: fixed effects (population-level intercept and slope)\
-   $u_{0j}, u_{1j}$: random effects (group-level deviations, e.g. each subject has its own intercept/slope)\
-   $\varepsilon_{ij}$: residual error (within-group variability)

**General Form (Generalized Linear Mixed Model, GLMM):**

The model for the expected outcome $\mu_{ij} = E[y_{ij}]$ is:

$$
g(\mu_{ij}) = \beta_0 + \beta_1 x_{ij} + u_{0j} + u_{1j}x_{ij}
$$

**Where:**

-   $y_{ij}$: the observed response (e.g., a count of events).
-   $g(\cdot)$: link function (e.g., logit for binary data, log for counts).
-   $\beta_0, \beta_1$: fixed effects (population-level intercept and slope).
-   $u_{0j}, u_{1j}$: random effects (group-level deviations).

## Mixed models

We use the 'RIKZ.csv' data set. You should be familiar with it because we explored it in Week 3 of the module. This has been extensively used by the books authored by Alain Zuur (see readings). This first bit of code comes from @Zuur2009a.

Load the dataset.

```{r}
Benthic <- read_csv("~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/RIKZ.csv")
```

[TASK: YOU MUST DO THIS. Have a look at it to refresh yourself about its structure. Use what ever function you prefer, `glimpse()`, `str()`, and `skim()` but also make sure you use `unique()` for the number of levels in the factor and `table()` to count the cases in each level \[\~ 5 min\]]{style="color:red;"}.

You may recall that the datafile has the following variables (Table 9.1):

**Table 9.1: Description of variables in the 'RIKZ.csv' data set**

| Variable      | Description                                                                                                                                    | Type              |
|---------------|------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|
| **Sample**    | Identifier for the sample unit (1–45). Each represents a sampling station on a beach.                                                          | Integer / numeric |
| **Richness**  | Number of species (species richness) found in the sample.                                                                                      | Numeric           |
| **Exposure**  | Ordinal index (1–11) of beach exposure to wave action, length of surf zone, from sheltered (1) to very exposed (11).                           | Ordinal numeric   |
| **NAP**       | “Normaal Amsterdams Peil” — elevation relative to Dutch datum (m). Negative values indicate lower beach positions (more frequently submerged). | Numeric           |
| **Beach**     | Identifier for the beach (1–9). The target 'random factor'.                                                                                    | Numeric           |
| **grainsize** | Mean sediment grain size at the sampling site. Larger values indicate coarser sand and higher energy environments.                             | Numeric           |
| **humus**     | Percentage of organic matter in the sediment (proxy for nutrient content). Higher values may indicate more decomposed material.                | Numeric           |
| **chalk**     | Percentage of calcium carbonate (CaCO₃) in the sediment, often related to shell fragments or calcareous sand.                                  | Numeric           |
| **sorting1**  | Sediment sorting coefficient — lower values indicate well-sorted sediments; higher values indicate mixed grain sizes.                          | Numeric           |

We want our grouping factor to be a factor. Is it currently as numeric variable. Notice also that Exposure is a numeric variable and although it could range from 0-11, we only have three values 8, 10, 11 and they are unbalanced i.e. fewer 8s (5 as opposed to 20 and 20).

```{r}
unique(Benthic$Exposure)
table(Benthic$Exposure)
```

Our factors must be factors and we must sort out the imbalance issue. We can use `mutate()` from **dplyr** to do that. We change 'Beach' to a factor called 'fbeach'; and concatenate exposure to a new variable with 2 levels 10 and 11 but making the 8s conform to 10s and call it 'fExp'. This balances the number of cases because initially we have only a few sample points where the Exposure = 8. See the output of the two `table()` calls at the bottom of the code snippet.

```{r}
# Code from Zuur et al. 2009.
Benthic <- Benthic |>
  mutate(
    # Convert 'Beach' to a factor. R will now know it's a grouping variable.
    fbeach = as.factor(Beach),
    fExp = case_when(
      Exposure == 8 ~ 10,   # replace 8 with 10
      TRUE ~ Exposure       # keep other values as is
    ),
    fExp = factor(fExp, levels = c(10, 11)))

# JPS Code
table(Benthic$Exposure)
table(Benthic$fExp)
```

### EDA to explore for heterogeneity in data sets

First up we ought to look at the variability in our response across the 9 beaches. We will use boxplots to do that (Fig. 9.3). You have the code for this in Week 3's workshop!

```{r}
ggplot(Benthic, aes(x = factor(Beach), y = Richness)) +

geom_boxplot(
    fill = "skyblue",
    alpha = 0.7,
    outlier.shape = NA # This tells the boxplot geom to NOT draw the outlier points.This important or it will plot all the datapoints and superimpose the outliers, creating too many points!
  ) +
  
  # plot all points.
  geom_jitter(
    width = 0.2,
    alpha = 0.5
  ) +
  
  # Add labels and a clean theme 
  labs(x = "Beach", y = "Species Richness"
  ) +
  theme_bw()
```

**Fig. 9.3: Boxplots of species richness v beaches**

We see a lot of variability both between and within individual beaches. This means we need capture this in our analyses. We'll explore how below.

The best way to tackle issues about dependencies is to facet your plots and isolate the regression lines for each element of the grouping factor. The code for this is also in Week 3. What we are looking for are different slopes and different intercepts or both. Go and find it. Your plot should look something like this (Fig. 9.4). You can see that the slopes for each beach differ as do many of the intercepts. You could also use the base R function called `coplot()`. Search the help file on this but typing: `?coplot` in the console window. We also note that we have 5 observations per level. This is enough for calculating a slope. (you need 3 or more) but it isn't a huge sample.

```{r, echo=FALSE}
ggplot(Benthic, aes(x = NAP, y = Richness)) +
  geom_point(alpha = 0.6) + # Added some transparency to the points
  geom_smooth(
    method = "lm", 
    se = TRUE, # add in some confidence interval
    color = "red",       # Change line color to red
    linewidth = 1.2,    # Make the line slightly thicker
    formula = 'y ~ x',
    ) +
  facet_wrap(
    ~ fbeach
  ) +
  # Set the y-axis viewing window to start at 0. You cannot have a negative count of species richness.
  coord_cartesian(ylim = c(0, NA)) +
  
  theme_bw()
```

**Fig. 9.4: Plot of the regression lines for each individual beach**

### Linear model variants

We will start with a basic model using just the NAP variable and ignoring the fact that the sample spans 9 beaches. This will highlight some of the problems. Approach is based on @Zuur2009a (Chapter 5).

```{r}
M1 <- beach.lm <- lm(Richness ~ NAP, data = Benthic)
```

Let's look at the output.

```{r}
summary(M1)
```

We see a strong negative effect of NAP, with a decent effect size, an adjusted $R^2$ of 0.3088. What we are doing here is use one population mean to capture variability across all the samples. This isn't especially sensible because we know that the effect of the explanatory variable differs across the nine beaches (Figs. 9.3-4). The implications of that are visible in the validation plots.

[TASK: You should already know the code for this. Plot the R summary validation plots, 4 to one page and also the covariable residual spread. What do these plots tell you about the model \[\~ 5 min\]]{style="color:red;"}.

The 4-panel plot should look like this (Fig. 9.5):

```{r, echo=FALSE}
# Plot validation graph
op <- par(mfrow = c(2,2))
plot(M1)
par(op)
```

**Fig. 9.5: Validation plots for the single factor linear model (M1)**

And here is the residual plot for the NAP covariate (Fig. 9.6).

```{r, echo=FALSE}
# strip out the residual
E <- residuals(M1) # straight standard residuals are okay  as it is a lm() model.
#plot against the NAP covariate
 plot(Benthic$NAP, E)
```

**Fig. 9.6: Plot of residual spread for the covariate NAP**

We can also check for outliers using Cook's $D$ (Fig. 9.7).

```{r}
plot(M1, which = 4)
```

**Fig. 9.7: Cook's** $D$ for the NAP variable

Maybe we can make things better by adding beach in as a covariate to create an ANCOVA model? We want the interactions because we want to allow the intercepts and slopes to vary, so we run with the full *multiplicative* model. You may recall from week 5 that when you have many levels in a (grouping) factor you lose one degree of freedom for each level in the variable. And the interactions needed for the ANCOVA adds more still. So, in total, we lose 27 degrees of freedom, over half of those that are available to us with a n = 45.

Recall, also from Week 6, that we had a 'rule of thumb' that the number of covariates should not be **more than a 1/3** of the available degrees of freedom, as this would increase the possibility of a **type 1** error; i.e. a false positive outcome. We will run the model to illustrate the issues and examine the output.

```{r}
# We could try to add a covariate in for beach as a form of ANCOVA
M2 <- lm(Richness ~ NAP * fbeach, data = Benthic)
summary(M2)
```

We observe that there are differences across the beaches but not all of them differ significantly from the baseline (our intercept term). We can see also that some but not all of the interactions are significant. Finding coherent ecological explanations of that pattern are likely to be problematic. What do the validation plots look like (Fig. 9.8)?

[TASK: Plot the R summary validation plots, 4 to one page and also the covariable residual spreads. What do these plots tell you about the model \[\~ 5 min\]]{style="color:red;"}.

Here are the validation plots (Figs 9.8).

```{r, echo=FALSE}
# Plot residual - it hasn't improved that either!
op <- par(mfrow = c(2,2))
plot(M2)
par(op)
# not much better....
```

**Fig. 9.8: Validation plots for the ANCOVA model (M2)**

Here are the residual patterns for the two covariates (Fig. 9.9). Remember you need a boxplot for the beach variable.

```{r, echo=FALSE}
E <- resid(M2)
op <- par(mfrow = c(1,2))
plot(E~Benthic$NAP,
     xlab = "NAP",
     ylab="Standard Residuals")
boxplot(E ~ Benthic$Beach,
        xlab = "Beach",
     ylab="Standard Residuals")
par(op)

```

**Fig. 9.9: Residual plots for your explanatory variables from model M2**

These are not great, there is a lot of variability across the beaches. **Now use the code above to look at the Cook's** $D$.

### Applying a Mixed Model

As mentioned above these data were analysed in @Zuur2009a (Chapter 5)nand we are following that analysis but with some differences. In their workflow, they first estimated the best random effects structure with a GLS model (with an appropriate variance structure) and a LMM with a random intercept, then a random intercept and random slope (using the **nlme** package), *before* they sorted out the best fixed effects. They do not provide validation for the model. You could use the **lme4** package and the `lmer()` function to apply a similar model. The beauty of lme4 is that you can fit both LMMs and GLMMs, with just a time modification in the code: i.e. `glmer()` for the GLMM.

But we'll use a more recent and flexible called **glmmTMB** today and start with a GLMM because we are using count data. We will loosely follow the protocol outlined by @Zuur2009a though. So our first task is to figure out what is the most appropriate random effect structure. The protocol asks that we establish the best random effect element first and then fit the best fixed structure in the second step. We compare the random structures by selecting a maximal fixed effects structure (a saturated model). We need to be careful here because we do not have a lot of samples, so we need to be choosy about what variables to include. We make our decisions on ecological grounds, after a little more EDA.

We'll adapt the code from Week 7 to look at the variables. We don't need 'Exposure' because we factorised it earlier to fExp.Here is the plot (Fig. 9.10).

```{r}
# Reshape the data for plotting
# We select the response variable (Richness) and all the numerical predictors
# we want to plot against it.
benthic_long <- Benthic |>
  # Select the columns of interest. We need dplyr::select because MASS is loaded as a library to avoid the conflict.
  dplyr::select(
    Richness, 
    NAP,
    grainsize, 
    humus, 
    chalk, 
    sorting1
  ) |>
  pivot_longer(
    cols = -Richness,             # Pivot all columns EXCEPT Richness
    names_to = "predictor_name",  # New column for the name of the predictors
    values_to = "predictor_value" # New column for the value of the predictors
  )


# Create the faceted scatterplot matrix 
ggplot(benthic_long, aes(x = predictor_value, y = Richness)) +
 
  #  plot points
  geom_point(alpha = 0.6) +
  
  # regression fit with CIs
  geom_smooth(method = "lm", se = TRUE, color = "blue", formula = 'y ~ x') +
  
  # LOESS smoother to check for non-linearity
  geom_smooth(method = "loess", se = FALSE, color = "red", formula = 'y ~ x',
              span = 1.2) + # span > 1 makes it less "wiggly"
  
  # This creates a separate plot for each covariate.
  facet_wrap(~ predictor_name, scales = "free_x") +
  
  # Add labels and a title
  labs(
    title = "Species Richness vs. Environmental Predictors",
    subtitle = "Comparing Linear Fit (Blue) to a LOESS Smoother (Red)",
    x = "Predictor Value",
    y = "Species Richness"
  ) +
  
  # Change the theme, add blue facet header
  theme_bw() +
  theme(
    strip.background = element_rect(fill = "lightblue") 
  )
```

**Fig. 9.10: linear and curvilinear regressions for the model covariates**

The data are patchy and clumped, especially those for 'chalk' and there is a hint of a possible outlier (extreme values) in 'humus' and 'sorting'. It seems likely that sorting and 'grain size' will be linked as they are part of the fluvial processes on the beachand looking at the patterns we might have multicollinearity issues with the particle variables and NAP, so we run with NAP alone. We will retain 'humus' because this provides a good indicator of food availability in benthic communities.

We specify a maximal model with interactions `glmmTMB(Richness NAP * fExp * humus)`.

-   Random intercept only (M3) - `glmmTMB(Richness NAP * fExt * humus + (1 | fbeach)`

-   Random intercept and slope (M4) - `lmer(Richness ~ NAP * fExp * humus + (NAP | fbeach)`

At this point we need to introduce one further bit of statistics (it's the last element - I promise). As you know from Week 5, linear models estimate their parameters (coefficients, variances, etc.) using the method of least squares — by minimising the sum of squared residuals to estimate the model parameters (eg. coefficients, variances etc). GLMs use [**Maximum Likelihood (ML)**](https://www.youtube.com/watch?v=XepXtl9YKwc) to estimate the parameters (excuse the silly song at the start of the video!) by finding the parameter values that make the observed data most probable under the model.

When we move to mixed models, parameter estimation can be done using either Maximum **Likelihood (ML)** or **Restricted Maximum Likelihood (REML).** REML is very similar to ML but adjusts for the loss of degrees of freedom that occurs when estimating the fixed effects (the means) before the variances. In then uses the non-fixed factor variances to more accurately estimate the random effects. Choosing which method to use — and when — depends on the model’s purpose.

#### Rules Model Component Comparisons

1.  If you are comparing models with **different fixed effects** with a LMM using the **lme4** package (e.g., y \~ x1 vs. y \~ x1 + x2):

    -   You **MUST** fit the models using **Maximum Likelihood (REML = FALSE)**.

    -   You can then compare the models using AIC, BIC, or a Likelihood Ratio Test (`anova()`).

2.  If you are comparing models with the **same fixed effects** but **different random effects** (e.g., y \~ x1 + (1 \| g) vs. y \~ x1 + (1 + x1 \| g)):

    -   You should fit the models using **REML (REML = TRUE)**, as it gives you better estimates of the random effect variances.

3.  In the case of a LMM, after you have selected your final, best model, you ought to re-fit that one model using **REML** to get the best, unbiased estimates for your final parameters. GLMMs default to ML methods (see below), so this is not necessary.

#### Applying a GLMM using glmmTMB

We will use **glmmTMB** to fit the mixed model due to it's flexibility for fitting various error terms. We compare the random effects first and assess which is the best model by using AIC. The best model will have the lowest AIC. Any model with $\delta <2$ AIC is equally likely to be a best model.

```{r}
# We are using the glmmTMB package to do this
# The default method here is REML so do not need to tell it to use REML.

M3 <- glmmTMB(Richness ~ NAP * fExp * humus  + (1 | fbeach),
	data = Benthic, family = poisson)	# Random slope model

M4 <- glmmTMB(Richness ~ NAP * fExp * humus + (NAP | fbeach), family=poisson,
	data = Benthic) # Random intercept and slope

aicM3 <- AIC(M3)
aicM4 <- AIC(M4)
AIC(M3,M4)
# Compare models with AIC

aicM4-aicM3
```

These results suggest that the more complex interactive model (M4) confers no advantage over M3, which only has a random intercept. Our exploration suggested otherwise but we'll go with the simpler model.

```{r}
summary(M3)
```

Now we need to establish the best fixed effects structure for the model. We could do this by backward selection, dropping the terms that are not significant but this flies in the face of the rationale for using AIC in the first place [@WHITTINGHAM2006]. So we go for some model averaging and compare the nested models. We'll use **MuMIN** for this task.

```{r}
# for reference library(MuMIn)
options(na.action=na.fail) # set options in Base R concerning missing values
summary(model.avg(dredge(M3), fit = TRUE, subset = TRUE), method = "ML")
options(na.action = "na.omit") # reset base R options
```

MuMIn indicates that the models with 'fExp\` + 'humus' + 'NAP' (variables 1,2 and 3 in the models list) and 'fExp' and 'NAP' are equally likely, so we go with most simple of the two. We refit the model with two variables.

```{r}
Final_M <- glmmTMB(Richness ~ fExp + humus + NAP  + humus: NAP + (1 | fbeach), data = Benthic, family = poisson)
performance(Final_M)
```

# You need to explain the above!

GLMMs are complex beasts so we are going to use a new package called **DHARMa** to test our models for overdispersion and generate the validation plots. It uses simulations to create standardized residuals for a wide range of statistical models, especially complex models like GLMMs. It is a major problem solving tool for non-linear models (like Poisson or Binomial), where standard residuals are often difficult to interpret cannot be used. This is how it works:

-   It takes the fitted model and uses it to simulate hundreds or thousands of new datasets that follow your model's specifications and assumptions perfectly.

-   For each observed data point, it compares its value to the distribution of the simulated values at that same point.

-   It calculates a "standardized residual" for each data point, which is essentially its rank (or quantile) within its own simulated distribution. If the model is a perfect fit, these new residuals will follow a perfect uniform distribution from 0 to 1 (i.e. no patterning will be visible in their spreads).

-   It also has tests for overdispersion, outliers, zero-inflation and so on.

#### Interpreting the DHARMa Plot

-   Left Panel (Q-Q Plot): This is the most important part. It plots the observed residuals against the expected residuals from a perfectly fitted model. You want to see the points sitting along the red diagonal line, with only limited deviations. This is a judgment call, as with base R's `plot()` function. Deviations from this line indicate problems with the model's overall fit or distribution.

-   Right Panel (Residuals vs. Predicted): This checks for heteroscedasticity. You want to see a random "starry night" of points. The lines for the quantiles (red lines) should be horizontal and not show a funnel shape or be humped.

-   Statistical Tests: DHARMa also prints the results of formal statistical tests in the plot title. Look for the "Kolmogorov-Smirnov test" for correct distribution and the "Dispersion test" (for over- and under-dispersion). You want the p-values for these tests to be large (`p > 0.05`). A significant p-value indicates a problem.

We create the simulated residuals using the `simulateResiduals()` function and then use the `plot()` function get the basic plots (Fig. 9.11).

```{r}
# Create the DHARMa simulation object
simulation_output <- simulateResiduals(fittedModel = Final_M)
# This one plot checks for multiple issues at once. Notice is the same as the base R function call
plot(simulation_output)
```

**Fig. 9.11: The DHARMa base residual plots**

This all looks good the KS normality test is not significant nor is the dispersion test.

Now we test for dispersion issues to get the plot (Fig. 9.12).

```{r}
# add tests for dispersion 
# A significant p-value here means the dispersion is different from what's expected.
testDispersion(simulation_output)
```

**Fig. 9.12: The dispersion test plot**

This shows that there are no dispersion issues. The dispersion parameter is 1.26, perhaps a little high, but DHARMa does not generate a significant finding. We can use the poisson model.

And finally for outliers to generate the plot (Fig. 9.13).

```{r}
# Test for outliers
# A significant p-value suggests the presence of more outliers than expected.
testOutliers(simulation_output)
```

**Fig. 9.13: The outlier test plot**

This test is not significant. Again, this confirms a poisson model is appropriate.

Our final validation tests relate to examining the residual spreads of the covariates in the models (Fig. 9.14).

```{r}
# Plot validation graph
op <- par(mfrow = c(2, 2))

plotResiduals(simulation_output, form = Benthic$NAP)
plotResiduals(simulation_output, form = Benthic$fExp)
plotResiduals(simulation_output, form = Benthic$humus)
# Reset the plotting parameters
par(op)
```

**Fig. 9.14: Residual spreads for the covariates in the 'Final_M' model**

# Figures correct to here

We see an issue with the variances on the 'fExp' factor (top left); everything else is fine. Let's try and see why. We'll use boxplots, scatterplots and the `table()` function to explore the issue.

```{r}
# boxplot of the errant factor fExp
ggplot(Benthic, aes(fExp, Richness)) +
  geom_boxplot(fill = "lightblue", outlier.shape = NA) +
  geom_point(alpha = 0.5) +
  theme_bw()
```

We can see an issue here. There is a much greater spread in level 10 that 11 and fewer values in 11 too. The `table()` function can help here.

```{r}
table(Benthic$fExp, Benthic$Richness)
```

Here we see the likely culprit. The rows on top are the unique values for species richness and the first column is the factor fExp. The number in the cells (or rows) are the number of cases of counts of each species richness level. You can see a lot of zeros in the level (11). Most of the counts are species richness levels of 6 or below. So in effect the counts for level 11 are heavily zero inflated. This is what is causing the problem.

There might also be interactions between 'fExp' and other variables in the model notable 'NAP'. We can visualise this with a scatterplot.

```{r}
ggplot(Benthic, aes(x = NAP, y = Richness, colour = fExp)) + # colour plots lines for each level of the fExp factor.
  geom_point(alpha = 0.6, size = 2) +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x', linewidth = 1.1) +
  labs(
    x = "NAP (continuous variable)",
    y = "Species richness",
    colour = "Exposure level"
  ) +
  theme_classic()
```

We can see the lines converge indicating an interaction between the variables. Species richness declines along the NAP variable are much greater at exposure level 10 than 11.

This is a complex fix because glmmTMB's more esoteric options for different error structure, i.e. Negative Binomial (`family =nbinom2`), Conway-Maxwell-Poisson (`family = compois`), Zeroinflated and Hurdle models operate on the response variable, not subsets of it. The only route we have is to use the `dispformula()` function which allows us to model the dispersion parameter directly. The rub is that it doesn't work with a straight Poisson model because these do not have a dispersion parameter; variance is meant to be equal to the mean. This is why Poisson models can be over or under dispersed (Table 9.1).

**Table 9.1: Causes of overdispersion**

| Case                | Condition   | Explanation                        | Possible causes                                                                           |
|---------------------|-------------|------------------------------------|-------------------------------------------------------------------------------------------|
| **Equidispersion**  | Var(Y) = μ  | Ideal Poisson behaviour            | Random, independent counts                                                                |
| **Overdispersion**  | Var(Y) \> μ | More variable than Poisson expects | Unmodelled heterogeneity, clustering, zero-inflation, temporal/spatial dependence         |
| **Underdispersion** | Var(Y) \< μ | Less variable than Poisson expects | Constrained counts, regular spacing, or strong limiting processes (e.g. extreme toxicity) |

We need to use one of the other families to effect a fix. You already know that the dispersion test was not significant, but the parameter was 1.26, tending towards overdispersion. Our suspicion is that our fExp factor is the cause of that. Look at the table above. We have unmodelled heterogeneity so use Conway-Maxwell-Poisson (`family = compois`) as it also flexible enough to deal with over- and under-dispersion. We combine this with a component that models the dispersion term directly. Our exploration of 'fExp' leads us to believe it might interact with 'NAP'. This makes sense as both are indirectly related beach geomorphology. We add an interaction term into the model - `dispformula = ~NAP * fExp`. This is akin to running an ANCOVA directly on 'fExp' factor allowing different slopes for each factor level. Our new model looks like this.

```{r}
compois_M <- glmmTMB(Richness ~ fExp + humus + NAP  + humus: NAP + (1 | fbeach), data = Benthic, family = compois, dispformula = ~NAP * fExp)
```

Well go through most of the validation steps but only show the residual spreads for 'fExp'. First we simulate the residuals and run the standard plots.

```{r}
# Create the DHARMa simulation object
simulation_output <- simulateResiduals(fittedModel = compois_M)
# This one plot checks for multiple issues at once. Notice is the same as the base R function call
plot(simulation_output, rank=FALSE) # we have turned off the ranking here so we see the true patterns of the residuals
```

Our dispersion test is still not significant but it would be interesting to know what the parameter is; and it is a little smaller at 1.1624.

```{r}
testDispersion(simulation_output)
```

Test for outliers. None found.

```{r}
testOutliers(simulation_output)
```

Now we just need to confirm that the `dispformula = ~NAP * fExp` has actually sorted out the heterogeneity in the 'fExp' variable (Fig. ). **If you have time add in the other covariates by using the code provided above**. There are no problems so we can go forward and interpret the model and create some supporting pictures.

```{r}
plotResiduals(simulation_output, form = Benthic$fExp, rank=FALSE)
```

**Fig: The residual spreads for the fExp factor**

##### Interpreting the summary()

The model summary looks like this.

```{r}
summary(compois_M)
```

Looks complex, so we'll break it down chunk by chunk.

##### Random Effects

**fbeach (Intercept):** There is a limited amount of variation in baseline Richness between beaches (Std. Dev. = 0.11) which confirms the mixed-effects structure is useful.

##### Fixed Effects

The conditional model predicts the effects on 'Richness' showing what drives the average species richness.

-   **fExp11:** Exposure level has a strong, significant effect (Estimate = -0.797). Exponentiating the coefficient to understand its multiplicative effect (exp(-0.797) ≈ 0.45). This means that, holding NAP and humus constant, the predicted species Richness at exposure level "11" is only 45% of the richness at the baseline exposure level. Or more simply, the higher exposure level (fExp11) is associated with a **55% decrease** in the expected number of species.

-   **NAP:** This effect is more complicated because of the significant interaction with humus (humus:NAP term, p = 0.0211). This means the effect of NAP is **not constant**; it changes depending on the amount of humus ( or vice versa).

    The total effect (slope) of NAP is captured by the equation: Slope_of_NAP = -0.628 + (3.172 \* humus).

    -   **When humus is low (e.g., humus = 0)**

        -   **Effect:** The humus:NAP term becomes zero, and the effect of NAP is just its main effect coefficient, -0.628. After exponentiating (exp(-0.628) ≈ 0.53), we see that a one-unit increase in NAP multiplies the expected Richness by 0.53. So for samples with very low humus, a one-unit increase in NAP is associated with a **47% decrease** in species richness.

    -   **When humus is higher (e.g., humus = 0.2)**

        -   **Effect:** The NAP slope becomes -0.628 + (3.172 \* 0.2) = -0.628 + 0.634 = +0.006. The effect is now slightly positive and very close to zero. NAP has a strong negative impact on species richness, but only in environments with low humus. As humus levels increase, this negative effect is buffered and disappears, suggesting that high humus may provide a mitigating effect against the pressures associated with NAP.

##### Generating pictures to support analysis

Now we have our story, we need some pictures to support it. As we have done in previous workshops we need to generate predictions from the models and plot some figures with these, such as partials (see Week 7). We have generate these manually in the past by holding the means for competing variables at zero and then calculating the partial slopes. It is more complicated with mixed models so we will use the **ggeffects** package to do this as it can handle models from GLMs and also mixed models such as GLMMs. It also involves a lot less code.

The beauty of this package is that the points of plotted on the count not log scale for you! Because **ggeffects** uses **ggplot2** as the plotting tool we can layer and modify the plot extensively. The 95% CIs and lines are provided as standard by **ggeffects** but not the data points, we need to add those. And as **ggeffects** works with **ggplot2** we cannot use `par(mfrow = c(1,2))` to generate a panel plot but we can use the **patchwork** package to arrange a panel plot. We'll start with the single factor partials and the basic plot.

```{r}
# Uses library(ggeffects)
# single effects
# Effect of NAP
p1 <- plot(ggeffects::ggpredict(compois_M, terms = "NAP")) 

# Effect of fExp
p2 <- plot(ggeffects::ggpredict(compois_M, terms = "fExp"))

# Effects of humus
p3 <- plot(ggeffects::ggpredict(compois_M, terms = "humus"))

# arrange using patchwork. Check the help file for examples of how to do this.
p1 + p2 / p3   
```

```{r}
# interactive effects
p4 <- plot(ggeffects::ggpredict(compois_M, terms = c("NAP", "fExp")))
p5 <- plot(ggeffects::ggpredict(compois_M, terms = c("NAP", "humus")))

p4 / p5
```

We can jazz the plots up by saving the predictions from `ggpredict()` to an object then layering up in the **ggplot2** as we normally would. We will use p4 for this. We already have the object.

```{r}
# create predictions instead of plot them directly. Preds is dataframe not a graphics object
pred <- ggpredict(compois_M, terms = c("NAP", "fExp"))

# We need to rename the groups in the preds df otherwise we'll get two legends
pred$fExp <- pred$group

# The key thing here is that we use two dataframes, the raw data (Benthic) for the points and the predictions (preds) for the regression lines and CIs
ggplot() +
  geom_point(data = Benthic, aes(x = NAP, y = Richness, color = fExp), # raw data 
             alpha = 0.5, size = 2) +
  geom_line(data = pred, aes(x = x, y = predicted, col = fExp), linewidth = 1) + # predictions
  geom_ribbon(data = pred, aes(x = x, ymin = conf.low, ymax = conf.high, fill = fExp), # predictions
              alpha = 0.2) + # add some labels etc
  labs(x = "NAP", y = "Species richness", color = "Exposure", fill = "Exposure") +
  theme_bw() # add a clean theme

```

### Take a deep breath and bookmark these blogs

I appreciate this is complex, but it is very similar to the ANCOVA examples we covered earlier in the module, in Week 6, with the key difference being that we place the grouping (factor) variable in the **random** rather than **fixed** structure of the model. Luckily, this stuff is well covered on the web. Check out the code driven example of how to use these models using the made up example of [dragons on mountain tops](https://ourcodingclub.github.io/tutorials/mixed-models/). This site is a superb resource, please explore.

I am aware here that we are looking at random effects as a means of getting rid of nuisance heterogeneity. This is not always a useful perspective as McGill and others discuss in his [blog](https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/). I encourage you to read it.

### A Quick Checklist

-   If you have only one observation level per group → fixed effect.

-   Where you have numerous levels per group → random effects might be needed.

-   If you want inference about the group itself then go for fixed. If you want information about variation among levels within groups then random is the approach.

-   How many levels do we need need in a random effect? → According to many certainly 3 or more levels, but realistically `~` 10 or more - see the blog discussions above.

## Follow-up work

-   Read the blogs!
-   Carry out the Dragons tutorial in the blog! Add in some more advanced visualisation for you EDA - for example, some faceting using ggplot.

## Next Week

Next week I will introduce you to the datasets you'll be using for your assessment for Part A of this module. There is a choice of four. The data are simulated to conform to particular hydro-ecological scenarios.

## References
