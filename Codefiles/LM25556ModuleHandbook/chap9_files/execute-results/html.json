{
  "hash": "fcb2a08344eb7a9a85fb60ecef253eaf",
  "result": {
    "engine": "knitr",
    "markdown": "# Dealing with data heterogeneity using mixed models\n\nLast week we covered the application of **Generalised Additive Models (GAMs)** to data that did not fit a key assumption of GLMs, i.e. linearity or the $x$ `~` $y$ relationship. This week, for our final session, we focus how we might deal with situations where we have structural dependencies in our data that do no meet the key assumption for linear, GLM and GAM models: **independence of sample points**. Deciding whether or not to use **mixed models** not as straightforward as it seems because it depends on the study design, objectives and data structure.\n\nBut let's start by thinking of them as an extension to ANOVA/ANCOVA and a means of dealing with heterogeneity in our data. We'll explore how heterogeneity arises in the data. If you are unlucky it arises because you have messed up your sample design. Hopefully, this is not the case, rather, it is a response to dependence issues that are factored into your design. Figure 9.1. shows two simple scenarios where the application of mixed modelling (either ANOVA or regression models) is a necessity. In panel (A), we have a block experiment that mirrors our fertilizer example from week 4 with one important difference. We have 3 subsamples from within each treatment within each block. The subsamples are nested with the treatments and not independent from each other. We ought to accommodate this in our regression.\n\n[TASK: Take a few mins chatting with your neighbour thinking about why this is the case. **HINT: we know that co-located samples are much more likely to be similar to each other i.e. *within* each treatment and each treatment block than *between* treatments and blocks** (this characteristic is know as **spatial autocorrelation** \\[\\~ 5 min\\]]{style=\"color:red;\"}.\n\nPanel (B), shows the situation where we have repeated measurement at each site over three time periods. Let's say we are counting bird species at each site in Spring, Summer and Autumn and we take one sample in each season. We need to account for this because repeated measurements from the individual sites are not independent. The samples across the seasons, *within* each site will be more similar to each other seasonal samples *between* the sites (a characteristic know as **temporal autocorrelation**). Please note there are numerous ways of dealing with autocorrelation, such as Geographically Weighted Regression (GRWs) or Spatial Regression Models (SRMs) for spatial issues and Time Series analysis for temporal dependencies. There are is an example in the book Appendix for time series work.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/mixed_models_examples.png){width=1006}\n:::\n:::\n\n\n**Fig. 9.1: Mixed model example sample designs where some form of mixed model (either mixed model ANOVA or mixed regression models) is necessary**\n\n\n## Today's Session\n\nToday's work is all about sorting out these potential analytical problems. And in doing so, hopefully supporting you in your project analyses over summer!\n\n### Learning Outcomes\n\nBy the end of this session you will be able to:\n\n-   Explore data to look for dependencies and potential autocorrelative structures\\\\\n\n-   Use straightforward linear mixed effects models to resolve the potential issues\n\n-   Anything more complex and we will need to discuss options and approaches!\n\n### Load libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# List of packages\npackages <- c(\"tidyverse\", \"ggfortify\", \"performance\", \"car\", \"skimr\", \"gridExtra\", \"broom\", \n\"ggeffects\",\"MASS\",\"MuMIn\", \"lme4\",\"glmmTMB\", \"DHARMa\")\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nRegistered S3 method overwritten by 'MuMIn':\n  method        from \n  nobs.multinom broom\n\nLoading required package: Matrix\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nThis is DHARMa 0.4.7. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n [7] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"     \"graphics\" \n[13] \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[2]]\n [1] \"ggfortify\" \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"    \n [7] \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"    \n [6] \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[11] \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"  \n[16] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[4]]\n [1] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"  \n [6] \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"      \n[11] \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"      \n[16] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[21] \"base\"       \n\n[[5]]\n [1] \"skimr\"       \"car\"         \"carData\"     \"performance\" \"ggfortify\"  \n [6] \"lubridate\"   \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"      \n[11] \"readr\"       \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"  \n[16] \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"   \n[21] \"methods\"     \"base\"       \n\n[[6]]\n [1] \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"     \"performance\"\n [6] \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"     \"dplyr\"      \n[11] \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"      \"ggplot2\"    \n[16] \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[21] \"datasets\"    \"methods\"     \"base\"       \n\n[[7]]\n [1] \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"    \n [6] \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"    \n[11] \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[16] \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"  \n[21] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[8]]\n [1] \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"        \n [6] \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"    \n[11] \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"      \n[16] \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"   \n[21] \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[9]]\n [1] \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"      \n [6] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"  \n[11] \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"      \n[16] \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"      \n[21] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[26] \"base\"       \n\n[[10]]\n [1] \"MuMIn\"       \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"  \n [6] \"skimr\"       \"car\"         \"carData\"     \"performance\" \"ggfortify\"  \n[11] \"lubridate\"   \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"      \n[16] \"readr\"       \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"  \n[21] \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"   \n[26] \"methods\"     \"base\"       \n\n[[11]]\n [1] \"lme4\"        \"Matrix\"      \"MuMIn\"       \"MASS\"        \"ggeffects\"  \n [6] \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"    \n[11] \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"    \n[16] \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[21] \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"  \n[26] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[12]]\n [1] \"glmmTMB\"     \"lme4\"        \"Matrix\"      \"MuMIn\"       \"MASS\"       \n [6] \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"        \n[11] \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"    \n[16] \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"      \n[21] \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"   \n[26] \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[13]]\n [1] \"DHARMa\"      \"glmmTMB\"     \"lme4\"        \"Matrix\"      \"MuMIn\"      \n [6] \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"      \n[11] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"  \n[16] \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"      \n[21] \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"      \n[26] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[31] \"base\"       \n```\n\n\n:::\n:::\n\n\nThe new packages this week are **lme4**, **glmmTMB** and **DHARMa:**\n\n-   **lme4** - The lme4 package provides functions to fit and analyze linear mixed models, generalized linear mixed models and nonlinear mixed models [@Bates2015].\n-   **glmmTMB** - An extremely powerful and complex multi-level modelling package for Generalised Linear Mixed Models (GLMMs) [@Brooks2017b]. Permits the application of zero-inflated mixed models.\n-   **DHARMa** - Provides functions that provide simulated residual diagnostics for mixed models along with numerous tests for zero-inflation, over dispersion and so [@Hartig2024].\n\n## Essential Reading\n\nRead these three introductory papers on mixed modelling, if you haven't already done so (after the class would make sense!):\n\n-   Start with chapter 8, 'Mixed Models', in @Zuur2007.\n\n-   Then @Harrison2018a.\n\n-   Finally, the classic introduction by @Bolker2009.\n\n## What are mixed models?\n\nYou are all familiar with applying regression models to data using **fixed effects**, the $x$ (or explanatory) components in the models. **Fixed effects** are effects of predictors that are assumed to be the same across all units (e.g. treatment, temperature, altitude). A **mixed model** is a regression model that includes both:\n\n-   **Fixed effects** and\n-   **Random effects**. **Random effects** allow certain parameters to vary across groups / clusters / units (e.g. intercepts, slopes for individual subjects, plots, regions). They model correlation or non-independence in the data.\n\nThey are useful when data are hierarchically structured (e.g. measurements nested within subjects, spatial clusters, repeated measurements over time) or when you have grouped / clustered observations.\n\n### The key Idea: Variance Partitioning\n\n-   Fixed effects explain systematic, population-wide variation.\n\n-   Random effects explain the clustering/nesting, and the\n\n-   Residuals capture leftover within-group variation.\n\nSo the model partitions variance into:\n\n-   Between-groups (random effects)\n\n-   Within-groups (residuals).\n\n### Visualising a mixed model\n\nYou will remember from numerous sessions that a regression generates two coefficients, an intercept and a slope (for all the explanatory variables). Mixed models use the random structure in the model to capture variability in two ways. You can use them to generate:\n\n-   a **random intercept model** - where we allow the intercepts of our explanatory variables to vary, or\n\n-   a **random intercept and slope model** - where both the intercept and slopes can vary (Fig. 9.2).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chap9_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n**Fig. 9.2: Examples of a random intercept (left panel) and a random intercept and random slope model (right panel).The *black line* = fixed effect (population regression line). The *coloured lines* represent the various groups**\n\nThe left panel in Figure 9.2 shows the situation where the slopes of the various group measurements are the same but where the intercepts differ. Look at where the lines sit on the Y axis. The panel on the right shows the situation where both the intercepts and the slopes vary. You have seen this before in week 4 where you examined the 'RIKX.csv' dataset. We will revisit those data today.\n\n### A little bit of maths\n\nWe can represent these models in same mathematical way as we did with LMs and GLMs. The extra element is the addition of the grouping variable (this will be a factor class variable):\n\n**General Form (Linear Mixed Model, LMM)**\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 x_{ij} + u_{0j} + u_{1j}x_{ij} + \\varepsilon_{ij}\n$$ **Where:**\n\n-   $y_{ij}$: outcome for observation *i* in group *j*\\\n-   $\\beta_0, \\beta_1$: fixed effects (population-level intercept and slope)\\\n-   $u_{0j}, u_{1j}$: random effects (group-level deviations, e.g. each subject has its own intercept/slope)\\\n-   $\\varepsilon_{ij}$: residual error (within-group variability)\n\n**General Form (Generalized Linear Mixed Model, GLMM):**\n\nThe model for the expected outcome $\\mu_{ij} = E[y_{ij}]$ is:\n\n$$\ng(\\mu_{ij}) = \\beta_0 + \\beta_1 x_{ij} + u_{0j} + u_{1j}x_{ij}\n$$\n\n**Where:**\n\n-   $y_{ij}$: the observed response (e.g., a count of events).\n-   $g(\\cdot)$: link function (e.g., logit for binary data, log for counts).\n-   $\\beta_0, \\beta_1$: fixed effects (population-level intercept and slope).\n-   $u_{0j}, u_{1j}$: random effects (group-level deviations).\n\n## Using mixed models\n\n### Mixed linear models\n\nWe'll start with a mixed linear model (the equation for it is above) and use the RIKZ.csv data set. You should be familiar with it because we explored it in Week 3 of the module. \n\n### EDA to explore for heterogeneity in data sets\n\nThe obivous way to approach the problem is to facet your plots and isolate the regression lines for each element of your grouping factor. What we are looking for are different slopes and different intercepts or both. If we are interested modelling this directly in terms of interactions then we model as we did in our ANCOVA example. But when you have many levels in the factor you are using a lot of degrees of freedom. \n\n\n\n\n### Take a deep breath and bookmark these blogs\n\nI appreciate this is a little complex, but it is very similar to the ANCOVA examples we covered earlier in the module, in week 6, with the key difference being that we place the grouping (factor) variable in the **random** rather than **fixed** structure of the model. Luckily, this stuff is well covered on the web. Check out the code driven example of how to use these models using the made up example of [dragons on mountain tops](https://ourcodingclub.github.io/tutorials/mixed-models/). This site is a superb resource, please explore. \n\nI am aware here that we are looking at random effects as a means of getting rid of nuisance heterogeneity. This is not always a useful perspective as McGill and others discuss in this [blog:](https://dynamicecology.wordpress.com/2015/11/04/is-it-a-fixed-or-random-effect/). I encourage you to read it. \n\n### A Quick Checklist\n\n-   One observation per group? → fixed effect.\n\n-   Many observations per group? → random effects might be needed.\n\n-   Do you want inference about the group itself (fixed) or about variation among groups (random)?\n\n-   How many levels do we need need in a random effect? → According to many certainly 3 or more levels, but realistically `~` 10 or more -  see the blog discussions below.\n\n**Remember: if you only have one replicate / measurement you cannot calculate anything!!**\n\n## Follow-up work\n\n- Read the blogs!\n\n\n## Next Week\n\nNext week I will introduce you to the datasets you'll be using for your assessment for Part A of this module. There is a choice of four. The data are simulated to conform to particular hydro-ecological scenarios.\n\n## References\n",
    "supporting": [
      "chap9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}