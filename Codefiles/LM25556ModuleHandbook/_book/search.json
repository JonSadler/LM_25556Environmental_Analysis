[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#this-is-the-module-handbook-for-the-uob-module-lm25556-environmental-analysis-and-modelling-in-r.",
    "href": "index.html#this-is-the-module-handbook-for-the-uob-module-lm25556-environmental-analysis-and-modelling-in-r.",
    "title": "Welcome",
    "section": "This is the module handbook for the UoB module: LM25556: Environmental analysis and modelling in R.",
    "text": "This is the module handbook for the UoB module: LM25556: Environmental analysis and modelling in R.\n\nFigure Credits::\n\nTop left. Penguin violin plot. From: R Graph Gallery using the PalmerPenguins package data. Plot created by Tuo Wang and code is here: https://r-graph-gallery.com/web-violinplot-with-ggstatsplot.html\nTop right. An original figure of the famous ‘bullhead/loach chironomid predation’ dataset. See Doncaster and Davey (2007) Analysis of Variance and Covariance: How to Choose and Construct Models for the Life Sciences. CUP, Cambridge.\nBottom left. The Birch trees on the Worcester-Birmingham Canal. Photograph: Jon Sadler.\nBottom right. A flock of turnstones (Bardsey Isle, North Wales). Photograph: Jon Sadler.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "The class materials\nPlease note I have used google translate to improve the access to these material for colleagues from China (the model is Chinese simplified). It may, or may not, faithfully translate the English to Mandarin. I have absolutely no way or knowing either way! Perhaps our native Mandarin speakers can confirm whether this is a helpful intervention or not! 请注意，我使用了谷歌翻译来方便中国同事访问这些资料（模型是简体中文）。它可能会，也可能不会，准确地将英文翻译成普通话。我完全不知道答案！或许我们的普通话母语人士可以确认一下，这是否是一种有效的干预措施！\nThe class materials are written as a Quarto book which is renders to a sequence of link html files (and a PDF document). You need to click on any of the ‘*.html’ file to use it as a book - this will open up the list of chapters. 课程资料以 Quarto 格式编写，并渲染为一系列 HTML 链接文件（以及一个 PDF 文档）。您需要点击“index.html”文件才能将其用作书籍——这将打开章节列表。\nThe module is structured into 10 weeks of content, with each week introducing you to different aspects of using R to analyse environmental datasets. Chapter 10 or week 10 will introduce you to the datasets for Part A of the module assessment, which is worth 50% of the module mark.\nAs the year progresses I will add bonus material to appendices for different analytical tasks, such as GIS mapping, timeseries analyses, binomial GLMs and perhaps some new and emerging multivariate approaches, such as Generalised Linear Latent Variable Models (GLLVMs).\n隨著時間的推移，我將在附錄中添加針對不同分析任務的獎勵資料，例如 GIS 映射、時間序列分析、二項式 GLM 以及一些新興的多元方法，例如廣義線性潛變數模型 (GLLVM)。",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-this-document",
    "href": "preface.html#how-to-use-this-document",
    "title": "Preface",
    "section": "How to use this document",
    "text": "How to use this document\nThe instructions are self-evident. The instructions and code are interlinked, please:\n\nWork through the workshop materials and click on the code link icon which are located on the top right of each link. This will copy the code from the chunk to your computer clipboard.\nWork through the workshop materials and click on each code link which will copy the code from the chunk to your computer clipboard.\nPaste the code into the relevant weekly codefile;\nRemember to use the # symbol to add comments to your code. Please use comments extensively.\nEach chapter has its own References section. Please undertake the guided readings as indicated, especially if it is reading for the next workshop. This will support your learning immeasurably!\n\n说明一目了然。说明和代码相互关联，请注意：\n点击左侧列表浏览章节资料。打开章节后，该章节的章节内容会显示在右侧的下拉列表中。\n浏览研讨会资料，然后点击每个代码链接，代码就会从该区块复制到您的计算机剪贴板。\n将代码粘贴到相关的每周代码文件中；\n请记住使用 # 符号为代码添加注释。请广泛使用注释。\n每章都有各自的参考文献部分。请按照指示阅读指导性阅读材料，尤其是下次研讨会的阅读材料。这将极大地支持您的学习！",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chap1.html",
    "href": "chap1.html",
    "title": "1  An introduction to R and RStudio",
    "section": "",
    "text": "1.1 Outline\nThe aim of this first class is to get introduce you to R and how it works within RStudio.\nIn it we will:\n本模块接下来的几个研讨会将以这些数据为案例研究。数据以及研讨会（html 文档）以 zip 文件的形式存储在 CANVAS 网站上：\nhttps://canvas.bham.ac.uk/courses/74919/pages/code-and-data-files-download-links-and-instructions\n请将它们下载到您的硬盘、共享网络驱动器、OneDrive 或其他合适的位置。双击 zip 文件并解压数据。\n本课程旨在向您介绍 R 语言及其在 RStudio 中的工作原理。\n我们将：\nIt is crucial that you download the the datafiles in to a directory that you will return to very week. As described below, I store data and code files separately in ‘project directories’.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#outline",
    "href": "chap1.html#outline",
    "title": "1  An introduction to R and RStudio",
    "section": "",
    "text": "Work with the Console window to carry out some basic R tasks to familarise with you the software and coding in general.\nLoad the data into R using RStudio.\nView the raw data in R amnd modify it.\nSubset data in R.\n\n\n\n\n\n\n\n使用控制台窗口执行一些基本的 R 任务，以帮助您熟悉该软件和编程。\n使用 RStudio 将数据加载到 R 中。\n在 R 中查看原始数据并进行修改。\n在 R 中对数据进行子集化。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#about-r-and-rstudio",
    "href": "chap1.html#about-r-and-rstudio",
    "title": "1  An introduction to R and RStudio",
    "section": "1.2 About R and RStudio",
    "text": "1.2 About R and RStudio\nIn this course, we will use R, the industry-standard programming language for environmental and geographic data analysis. While tools like Excel or SPSS rely on clicking through menus, R operates on typed commands. This command-based approach is fundamentally more powerful because it forces you to create a script. This script acts as a permanent, shareable record of your entire workflow, ensuring your analysis is transparent. More importantly, it allows you to repeat complex tasks perfectly, guaranteeing that you—and others—can reproduce the same results every time!\n在本课程中，我们将使用 R，它是用于环境和地理数据分析的行业标准编程语言。Excel 或 SPSS 等工具依赖于点击菜单，而 R 则依靠键入的命令进行操作。这种基于命令的方法本质上更加强大，因为它强制您创建脚本。该脚本可作为整个工作流程的永久可共享记录，确保您的分析透明化。更重要的是，它允许您完美地重复复杂的任务，确保您和其他人每次都能重现相同的结果！\nIt was designed in 1990s for statistics and the generation of publication quality graphics, which can be easily modified and tweaked by making slight changes to the script. Command-line computing like this can be off-putting at first as it is easy to make mistakes that aren’t always obvious to detect. But it is worth sticking with because:\n\nIt is an industry standard with a large user base.\nIt’s free and cross-platform so runs on Windows, Mac-OS and Unix.\nIt is highly customisable with a large number of libraries (‘or packages’) supporting multilevel and longitudinal regression, and mapping, spatial statistics, spatial regression and geostatistics to name a few methods.\n\n它设计于 20 世纪 90 年代，用于统计和生成出版级质量的图表，只需对脚本进行细微修改即可轻松进行修改和调整。像这样的命令行计算一开始可能会让人感到不适应，因为很容易犯一些不易察觉的错误。但它值得坚持使用，因为：\n\n它是拥有庞大用户群的行业标准。\n它是免费且跨平台的，可在 Windows、Mac OS 和 Unix 上运行。\n它高度可定制，拥有大量支持多级和纵向回归、制图、空间统计、空间回归和地理统计等方法的库（或软件包）。\n\nR can be downloaded from https://www.r-project.org/. While it is possible to conduct analysis on R directly, we will run it via Rstudio which provides a user-friendly graphical user interface. After downloading R, Rstudio can be obtained for free from https://www.rstudio.com/.\nThe software is available on our university computer clusters and accessible via the AppsAnywhere application. There is a shortcut to this on the desktop of all PCs. Click on RStudio and the R will already be there.\nYou can run this on your personal laptop and bring it to class. If it is not on your computer already please review the pre-session module on the CANVAS website for guidance on installation and so on.\nR 语言可以从 https://www.r-project.org/ 下载。虽然可以直接在 R 上进行分析，但我们将通过 Rstudio 运行它，因为它提供了用户友好的图形用户界面。下载 R 语言后，您可以从 https://www.rstudio.com/ 免费获取 Rstudio。\n该软件已在我们大学的计算机集群上安装，可通过 AppsAnywhere 应用程序访问。所有电脑的桌面上都有该软件的快捷方式。点击 RStudio，R 语言就会自动出现。\n您可以在个人笔记本电脑上运行该软件并将其带到课堂上。如果您的电脑上还没有 R 语言，请查看 CANVAS 网站上的课前模块，获取安装指南等。\nTo open R click on the start menu and open RStudio. You should see a screen resembling the image below (Fig. 2.1). The screen is separated into 4 windows:\n\nThe scripting window (top left). This is where you open and edit your scripts (or code files).\nThe console window (bottom left). This is where R lists the commands you have entered and the results of those commands (or at least some of them).\nThe environment windows - or if you like R’s brain. This window shows the objects held in memory by the system (e.g. datafiles, vectors and so on), as well a history of things you’ve done, the connections you have open and lastly some tutorial resources.\nThe output/input window. This shows many things but the most important are the packages you have in the system, the files you are viewing (i.e. where you working directory is), the plots/graphics you might create and help options.\n\n要打开 R，请点击开始菜单，然后打开 RStudio。您将看到类似下图（图 2）的屏幕。该屏幕分为 4 个窗口： - 脚本窗口（左上角）。您可以在此处打开和编辑脚本（或代码文件）。 - 控制台窗口（左下角）。R 在此处列出您输入的命令以及这些命令（或至少部分命令）的执行结果。 - 环境窗口 - 如果您喜欢 R 的“大脑”，也可以称之为环境窗口。此窗口显示系统内存中保存的对象（例如数据文件、向量等），以及您执行操作的历史记录、您打开的连接以及一些教程资源。 - 输出/输入窗口。此窗口显示许多内容，但最重要的是系统中的软件包、您正在查看的文件（即您的工作目录）、您可能创建的图表/图形以及帮助选项。\n\n\n\n\n\nRStudio screen windows\n\n\n\n\nWe are going to start by setting the working directory. This is so R knows where to open and save files to. It is recommended that you set the working directory to an appropriate space in your computers work space. To start with, make it the directory where you have stored the unzipped data.\n首先，我们需要设置工作目录。这是为了让 R 知道在哪里打开和保存文件。建议将工作目录设置为计算机工作区中合适的位置。首先，将其设置为存储解压数据的目录。\nTo set the working directory, go to the Session option from the top bar of RStudio, click on Set Working Directory then Choose Directory and use the search window to click through to your directory and select Open.\n要设置工作目录，请转到 RStudio 顶部栏中的 会话 选项，单击 设置工作目录，然后单击 选择目录，并使用搜索窗口单击到您的目录并选择 打开。\nAlternatively, you can type in the address of the working directory manually using the setwd() function as demonstrated below. This requires you to type in the full address of where your data are stored. Pay attention to the double quotes around the path address. The code for this is:\nWhere, setwd() is the function call, and “~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Codefiles” is the path.\n或者，您可以使用 setwd() 函数手动输入工作目录的地址，如下所示。这需要您输入数据存储的完整地址。请注意路径地址周围的双引号。代码如下：\n其中，setwd() 是函数调用， “~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Codefiles” 是路径。\nYou will need to add in your path. 您需要在路径中添加.\nIf you want to know where your working directory currently is then use the getwd() function. R will return the disk address where it is looking. Or click on the Files tab on the bottom right window.\n如果你想知道当前工作目录的位置，可以使用 getwd() 函数。R 会返回当前正在查找的磁盘地址。或者，点击右下角窗口的 Files 选项卡。\n\n#get working directory (WD)\ngetwd()\n\n[1] \"/Users/jonsadler/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Codefiles/LM25556ModuleHandbook\"\n\n\nIf you want to see what files are in your working directly to copy a filename then either click on the Files tab on the bottom right window or use the list.files() function. Remember this list is from my computer. Your file list will be different. You can also navigate around using the Files tab on the output window on the bottom right of your screen.\n如果您想直接复制文件名并查看工作区中有哪些文件，请点击右下方窗口的“文件”选项卡，或使用“list.files()”函数。请记住，此列表来自我的电脑。您的文件列表可能会有所不同。\n\n#List the files in the WD\nlist.files()\n\n [1] \"_book\"                       \"_quarto.yml\"                \n [3] \"Book_cover.R\"                \"canal.jpg\"                  \n [5] \"chap1.qmd\"                   \"chap1.rmarkdown\"            \n [7] \"chap10.qmd\"                  \"chap2.qmd\"                  \n [9] \"chap3.qmd\"                   \"chap4.qmd\"                  \n[11] \"chap5_files\"                 \"chap5.qmd\"                  \n[13] \"chap6.qmd\"                   \"chap7.qmd\"                  \n[15] \"chap8.qmd\"                   \"chap9.qmd\"                  \n[17] \"cover.png\"                   \"fish.png\"                   \n[19] \"ggplot_interactionPlot.R\"    \"images\"                     \n[21] \"index.html\"                  \"index.qmd\"                  \n[23] \"LM25556ModuleHandbook.Rproj\" \"my-cover.png\"               \n[25] \"Penguins.png\"                \"plover.jpg\"                 \n[27] \"preface\"                     \"preface.html\"               \n[29] \"preface.qmd\"                 \"references.bib\"             \n[31] \"references.qmd\"              \"site_libs\"                  \n[33] \"turnstones.jpg\"             \n\n\n\n1.2.1 Loading some packages into the system\nThe base installation of R has a lot of packages loaded as default. They sit in the toolbox available for instant use. But some functions are only available in different packages that are not in the base R installation. View each package as a set of tools that do things that aren’t in the base toolbox. This module will use the tidyverse package for many basic tasks; it includes tools for data manipulation/wrangling, creating graphics, dealing with dates and lots of other things. It was created by Hadley Wickham and you can read about it here:\nhttps://cran.r-project.org/web/packages/tidyverse/vignettes/paper.html\nand here:\nhttps://doi.org/10.18637/jss.v059.i10\nHere is the documentation/manual related to the package:\nhttps://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf\nR 的基础安装默认加载了许多软件包。它们位于工具箱中，可立即使用。但有些函数仅在基础 R 安装中未包含的其他软件包中可用。您可以将每个软件包视为一组工具，用于执行基础工具箱中未包含的操作。本模块将使用 tidyverse 软件包执行许多基本任务；它包含用于数据操作/整理、创建图形、处理日期以及许多其他功能的工具。该软件包由 Hadley Wickham 创建，您可以在此处阅读相关信息：\nhttps://cran.r-project.org/web/packages/tidyverse/vignettes/paper.html\n以及此处：\nhttps://doi.org/10.18637/jss.v059.i10\n以下是与该软件包相关的文档/手册：\nhttps://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf\nTo use packages you must first install them. You can do this by clicking on packages on the bottom right window, selecting install and searching for the package and installing it. Or if know know the name of the package, you can hard wire the installing using the function install.packages():\n要使用软件包，您必须先安装它们。您可以通过点击右下角窗口的“packages”按钮，选择“install”，然后搜索并安装软件包来完成此操作。或者，如果您知道软件包的名称，也可以使用函数“install.packages()”来强制安装：\n\n#Install packages function\n# install.packages(\"tidyverse\", dependencies = TRUE) # Note the use of quotes\n# NOTE: to allow this to render I have made the function call a comment. To make it run copy the code and delete the '#' from before the 'install.packages' command.\n# 注意：为了方便渲染，我已将函数调用添加注释。要使其运行，请复制代码并删除“install.packages”命令前的“#”。\n\nThe ‘dependencies=TRUE’ argument ensures that any dependent package is also installed with your target package. Remember the packages are tools in a toolbox; to use them after you have installed them you need to take them tool out of the box. The function that does this is called library().\n‘dependencies=TRUE’ 参数确保所有依赖包都会与目标包一起安装。请记住，这些包就像工具箱中的工具；安装后要使用它们，您需要将它们从工具箱中取出。执行此操作的函数称为 library()。\n\n#Load the library\nlibrary(tidyverse) # this loads multiple packages/libraries\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nR will generally manages the conflicts between packages that have functions with the same names but you can tell R use a function from a particular package using the package name, then two colons, and finally the function name. For example:\ndplyr::filter().\nWhere, dplyr is the package, :: links the package to the function filter().\nR 通常会处理包含同名函数的包之间的冲突，但您可以使用包名称、两个冒号以及函数名称来指示 R 使用特定包中的函数。例如：\ndplyr::filter()。\n其中，dplyr 是包，:: 将包链接到函数 filter()。\n\n\n1.2.2 Your first code / script file\nWe are going to create a new code file so you can progress through the workshop. The code commands are listed in the html file outlining the workshop activities and highlighted in square boxes. To make a new file click on the green cross at the top left of the window -&gt; select ‘Script file’ from the dropdown list. A new file called untitled will be added to the tabs. Simply copy the text from the boxes and drop it into the scripting window. Use this area as your workspace rather than typing directly into the console window (bottom left). Change the text below to fit your filename and add today’s date.\n我们将创建一个新的代码文件，以便您完成研讨会。代码命令列在概述研讨会活动的 HTML 文件中，并以方框突出显示。要创建新文件，请点击窗口左上角的绿色十字 -&gt; 从下拉列表中选择“脚本文件”。一个名为无标题的新文件将添加到选项卡中。只需从方框中复制文本并将其拖放到脚本窗口中即可。将此区域用作您的工作区，而不是直接在控制台窗口（左下角）中输入。请将下面的文本更改为适合您的文件名，并添加今天的日期。\n\n# Filename: add in your filename. I suggest something that includes 'week_1'\n# Date: Add a date so you can track changes\n\nThe # symbol tells R that is is a comment so it is not run as a line of code. We use comments to aid script interpretation for users and to remind ourselves why our former self coded what we did. It is amazing what we forget when moving on from a piece of coding.\nTo run a line of code hold control Ctrl and enter on your keyboard (command and enter on a mac). You can also highlight blocks of code and do the same. You can also select the line (or block of code) you wish to run and click ‘Run’ at the top of the scripting window.\nNOTE: The code is sequential so start at the top and work down. If you miss bits of code it will error and you will get confused and likely frustrated.\n# 符号告诉 R，这是一个注释，因此它不会作为一行代码运行。我们使用注释来帮助用户解释脚本，并作为我们的备忘录。令人惊讶的是，当我们从一段代码中继续前进时，我们竟然会忘记很多东西。\n要运行一行代码，请按住键盘上的 control Ctrl 和 Enter 键（在 Mac 上，请按住 **command 和 Enter 键）。您也可以高亮显示代码块并执行相同操作。您也可以选择要运行的行（或代码块），然后点击脚本窗口顶部的“运行”。\n注意：代码是连续的，因此请从顶部开始向下运行。如果您错过任何代码片段，就会出错，您会感到困惑，甚至可能感到沮丧。\n\n\n1.2.3 Creating a sensible directory system\nAs I noted above, I use a hierachical system for my analyses, broken into projects (Fig. 2.2). I add the raw data in a data directory within the project, along with another one called codefiles. I’d encourage you to do the same.\n\n\n\n\n\nAn example of an hierarchical file structure\n\n\n\n\n\n\n1.2.4 Coding Basics\nR has a steep learning curve, but the benefits of using it are well worth the effort. Take your time and think what the code is doing as you use it. As Wickham et al. (2023) note “Frustration is natural when you start programming in R because it is such a stickler for punctuation, and even one character out of place can cause it to complain. But while you should expect to be a little frustrated, take comfort in that this experience is typical and temporary: it happens to everyone, and the only way to get over it is to keep trying”.\nR 语言的学习曲线陡峭，但使用它所带来的好处绝对值得你付出努力。在使用过程中，请慢慢思考代码的功能。正如 Wickham 等人 (2023) 指出的那样：“刚开始使用 R 编程时，感到沮丧是很自然的，因为它对标点符号非常严格，即使一个字符出错也会导致它报错。虽然你可能会感到有些沮丧，但请放心，这种经历很常见，而且是暂时的：每个人都会遇到这种情况，克服它的唯一方法就是继续尝试”。\nThe best way to learn R is to take the basic code provided in the workshops and experiment with changing parameters - such as point type in a graph, line thickness or point colour - You cannot break it! Everything is fixable. Make lots of notes as you go along and if you are getting really frustrated take a break! In many ways this is like learning a language; a little practice more frequently is a good strategy.\nR operates by taking instructions, running functions and adding the outcomes of those functions into objects. The objects are generated by using the assignment operator &lt;-.\nThis ‘pipes’ the outcome of a piece of code into an object.\nFor example, try the following (copy the text from the box below and paste it into your code file, below the filename and data comment lines):\n学习 R 的最佳方法是学习研讨会上提供的基础代码，并尝试改变参数——例如图表中的点类型、线条粗细或点颜色——你无法破坏它！一切都是可以修复的。边学边做笔记，如果感到很沮丧，就休息一下！这在很多方面就像学习一门语言；多练习一些是个好策略。\nR 的运行方式是接收指令、运行函数，并将这些函数的结果添加到对象中。这些对象是使用赋值运算符生成的 &lt;-.\n这会将一段代码的结果“通过管道”传输到一个对象中。\n例如，尝试以下操作（从下面的框中复制文本并将其粘贴到代码文件中，位于文件名和数据注释行下方）：\n\nTutorname &lt;- \"Jon\" # note the quotes. These tell R it is a string.\n\nWhen you run the code it takes the string Jon and adds it to an object called Tutorname. This object is then stored in the Environment window (top right) (Fig. 2.3).\n运行代码时，它会获取字符串 Jon 并将其添加到名为 Tutorname 的对象中。该对象随后存储在 Environment 窗口（右上角）中（图 3）。\n\n\n\n\n\nThe environment window function\n\n\n\n\nIf you just type “Jon” and run that code, it will repeat the name in console window (bottom left window).\n如果您只是输入“Jon”并运行该代码，它将在控制台窗口（左下角窗口）中重复该名称。\n\n\"Jon\"  \n\n[1] \"Jon\"\n\n\nAll R statements where you create objects, assignment statements, have the same form:\nobject_name &lt;- value When reading that code, say “object name gets value” in your head. You will make lots of assignments, and typing &lt;- repetitively is a pain to type. You can save time with RStudio’s keyboard shortcut: Press Alt and the - (the minus sign) in sequence. This is option plus - (the minus sign) on a Mac.\nLet’s explore some more… R will do lots of things. It will act like a huge calculator running mathematical calculations:\n所有用于创建对象的 R 语句（赋值语句）都具有相同的形式：\nobject_name &lt;- value 阅读该代码时，请在脑海中默念“对象名称获取值”。您将进行大量赋值，而重复输入 &lt;- 会非常麻烦。您可以使用 RStudio 的键盘快捷键来节省时间：依次按下 Alt 和 -（减号）。在 Mac 上，这相当于 option 加 -（减号）。\n让我们进一步探索…… R 可以做很多事情。它就像一个巨大的计算器，运行着数学计算：\n\n1/1000*25 # see the response in the console window (bottom left)\n\n[1] 0.025\n\n\nor\n\ntan(pi*100) # or\n\n[1] 1.964387e-15\n\n10+20+30 # or \n\n[1] 60\n\n\nAs we have seen above you can add the outcomes of these calculations to an object. Keep and eye on the Environment window as you run the code. This is useful if you want to use the outcome many times.\n正如我们上面所见，您可以将这些计算的结果添加到对象中。运行代码时，请密切关注环境窗口。如果您想多次使用该结果，这将非常有用。\n\nx &lt;- 8 * 9\n\nNotice it doesn’t show the content of the object in the console window but it does store as an object in the computer memory or R’s Environment (see top right window).\nIf you want to know whats in the object then type the object name and run the line of code.\n请注意，它不会在控制台窗口中显示对象的内容，但它会作为对象存储在计算机内存或 R 的环境中（参见右上角的窗口）。\n如果您想知道对象中的内容，请输入对象名称并运行该代码行。\n\nx\n\n[1] 72\n\n\nAnd it gives you the answer in the console (bottom left window).\nYou can combine multiple elements into a vector with the c() function (literally means ‘concatenate’ or ‘add together’):\n它会在控制台（左下角窗口）中显示答案。\n您可以使用 c() 函数（字面意思是“连接”或“相加”）将多个元素组合成一个向量：\n\nprimes &lt;- c(2, 3, 5, 7, 11, 13) # adds primes from zero to 13 into a vector\n\nAnd basic arithmetic on vectors is applied to every element of of the vector. So here each prime in the sequence is multiplied by 2 and the and returned to the same object.\n向量的基本运算应用于向量的每个元素。因此，这里将序列中的每个素数乘以 2，然后返回同一个对象。\n\nprimes * 2\n\n[1]  4  6 10 14 22 26\n\n\nNOTE R overwrites the previous object with no warnings as that is what you told it to do!: If want to retain the first object then give the new one a different name!\n注意 R 会毫无警告地覆盖前一个对象，因为这是你告诉它要做的！：如果要保留第一个对象，那么给新对象起一个不同的名称！\n\n\n1.2.5 Getting Data into R via the Console\nMost of the following derives from Beginner’s Guide to R (Zuur, Elena N. Ieno, and Meesters 2009). First Steps - typing in a small dataset.\n以下内容大部分来自 Zuur 等人的《2009 R 初学者指南》。第一步 - 输入一个小的数据集。\n\na &lt;- 59\nb &lt;- 55\nc &lt;- 55.5\nd &lt;- 55\n\nSee what happens if you type the object names: 看看输入对象名称会发生什么：\n\nd\n\n[1] 55\n\na\n\n[1] 59\n\n\nNow use some basic maths operators to play around with them. Try a * b etc\nTo see what you’ve done type any of the letters. You could have called these variables anything but it is wise to use names that are memorable and relate to what the variable represents. Let’s assume you have been measuring bird wing length (in mm). So:\n现在使用一些基本的数学运算符来操作它们。试试 a * b 等等。\n要查看你输入的内容，请输入任意字母。你可以给这些变量起任何名字，但最好使用容易记住且与变量含义相关的名称。假设你一直在测量鸟翼的长度（以毫米为单位）。所以：\n\nWing1 &lt;- 59 # Note naming conventions in R. It's case sensitive, no spaces, forward slashes \nWing2 &lt;- 55 # hyphens; the names cannot start with a numeric either! \nWing3 &lt;- 55.5 # It will accept dots (periods), underscores etc \nWing4 &lt;- 55 # Capitalising variable names is good practice as there maybe functions with similar names! \nWing5 &lt;- 52.5\n\nBase R is powerful and has numerous functions allocated to it. Let’s try a few basic ones with these data.\nR 基础功能强大，并配备了众多函数。让我们用这些数据尝试一些基本函数。\n\nsqrt(Wing1) # Square root of variable \n\n[1] 7.681146\n\nWing1 * Wing3 \n\n[1] 3274.5\n\nWing1 + Wing2 + Wing3 \n\n[1] 169.5\n\nWing1 * Wing5 / 5\n\n[1] 619.5\n\n\nAlthough R is performing these calculations and returning the answer via the console, it is not storing them. The console output is visible in the html file (the document you are reading) below the code snippet. I used a quarto file to create the html output in this module book, but I want you all to work with base R codefiles for the time being. To store these you need to define new variables (or objects).\n虽然 R 会执行这些计算并通过控制台返回结果，但它不会存储它们。控制台输出可以在代码片段下方的 HTML 文件（您正在阅读的文档）中查看。我使用 Quarto 文件创建了 HTML 输出，但我希望大家暂时使用基础 R 代码文件。要存储这些输出，您需要定义新的变量（或对象）。\n\nSQ.Wing1 &lt;- sqrt(Wing1) # Calculation product is now stored as another variable SQ.Wing1\n\nThis doesn’t display the outcome in the console window. It has added as a variable to your environment (see top right panel of your screen). To see it you need to type the new object name.\n这不会在控制台窗口中显示结果。它已作为变量添加到您的环境中（请参阅屏幕右上角的面板）。要查看它，您需要输入新的对象名称。\n\nSQ.Wing1\n\n[1] 7.681146\n\n\nWe can hold objects in any form. You used the concatenate c() function above to calculate a run of prime numbers from a vector. We can create a vector of our wing lengths.\n我们可以保存任何形式的对象。你之前使用了 concatenate c() 函数从一个向量计算出一连串素数。我们可以创建一个包含机翼长度的向量。\n\nWingcrd &lt;- c(59, 55, 53.5, 55, 52.5, 57.5, 53, 55) \n\nNote spaces improve readability and you need () brackets not [ ] or {}; these are reserved for other things. You can create vectors with strings using quotes. This typically how factor variables might be stored in a dataframe:\n注意：空格可以提高可读性，并且需要使用 () 括号，而不是 [ ] 或 {}；这些是保留用于其他用途的。您可以使用引号创建包含字符串的向量。因子变量通常存储在数据框中，如下所示：\n\nmy_string &lt;- c(\"str1\", \"str2\") # It needs quotation marks\nmy_string # returns the vector\n\n[1] \"str1\" \"str2\"\n\n\nOnce you have data in a vector you can find individual values and ranges of values by using the [ ] square brackets. Very useful for subsetting data for analyses! We’ll return to this many times, so do not worry if it isn’t making sense right now.\n将数据放入向量后，您可以使用[ ]方括号查找单个值及其值域。 这对于分析子集数据非常有用！我们以后会多次讨论这一点，所以如果现在看不懂也不用担心。\n\nWingcrd[1] # Finds the first value \n\n[1] 59\n\nWingcrd[1 : 5] # Returns values 1-5 \n\n[1] 59.0 55.0 53.5 55.0 52.5\n\nWingcrd[-2] # Returns all values excluding value 2\n\n[1] 59.0 53.5 55.0 52.5 57.5 53.0 55.0\n\n\nNotice the formatting here.\nNow we have multiple variables we can use base R functions to provide statistical descriptors of them (sum, mean, max, min, median, var, sd and so on).\n注意这里的格式。\n现在我们有多个变量，我们可以使用基础 R 函数来提供它们的统计描述符（总和、平均值、最大值、最小值、中位数、方差、标准差等等）。\n\nsum.wings &lt;- sum(Wingcrd)# We have allocated the sum of the values to the variable sum.wings\nsum.wings\n\n[1] 440.5\n\n\nEXERCISE: Spend a few minutes to try a few more out:sum, mean, max, min, median, var, sd (5 minutes or so).\nNow let’s add the other variables in same way to create the other variables a table. These are other measurements from birds, specifically, their foot length (tarsus), head length including their bill (head) and their weight (wt).\n练习：花几分钟时间尝试更多变量：总和、平均值、最大值、最小值、中位数、方差、标准差（大约 5 分钟）。\n现在，让我们以相同的方式添加其他变量，创建一个表格。这些是鸟类的其他测量数据，具体来说，包括它们的足长（跗骨）、包括喙在内的头长（头部）以及体重（wt）。\n\nTarsus &lt;- c(22.3, 19.7, 20.8, 20.3, 20.8, 21.5, 20.6, 21.5) \nHead &lt;- c(31.2, 30.4, 30.6, 30.3, 30.3, 30.8, 32.5, NA) # see the NA\nWt &lt;- c(9.5, 13.8, 14.8, 15.2, 15.5, 15.6, 15.6, 15.7)\n\nNote one variable has a missing value; these must be listed with an NA; empty cells are a problem! Take heed though! Missing values cause issues for some functions:\n注意，一个变量有缺失值；这些变量必须用NA列出；空单元格会有问题！不过要注意！缺失值会导致某些函数出现问题：\n\nsum(Head) # Told you!\n\n[1] NA\n\n\nThe system errored because it was not expecting a missing value or NA. To get around that you need to instruct the function that NAs exist.\n系统出错了，因为它没有预料到缺失值或 NA。为了解决这个问题，你需要指示函数存在 NA。\n\nsum(Head, na.rm = TRUE) # Note check help files for functions. Some use 'na.action'\n\n[1] 216.1\n\n\nLet’s pose a question. Can you combine these variables Wingcrd, Tarsus, Head, Wt into one file? Maybe c() will help….?\n让我们提出一个问题。你能把 Wingcrd、Tarsus、Head 和 Wt 这些变量合并到一个文件中吗？也许 c() 能帮上忙……？\n\nBirdData &lt;- c(Wingcrd, Tarsus, Head, Wt)  \n\nLet’s have a look:\n让我们来看看：\n\nBirdData\n\n [1] 59.0 55.0 53.5 55.0 52.5 57.5 53.0 55.0 22.3 19.7 20.8 20.3 20.8 21.5 20.6\n[16] 21.5 31.2 30.4 30.6 30.3 30.3 30.8 32.5   NA  9.5 13.8 14.8 15.2 15.5 15.6\n[31] 15.6 15.7\n\n\nHmmm big mess!!!! BirdData is a single vector of length 32 (4 x 8). It is organised this way because it cannot distinguish the 8 values in each variables. To sort this out we need to create another variable with 32 values to use an index. We could do this the hard way and use the repeat and sequencing tools to sort it then align it but I am a fan of easier fixes.\nWe can combine it directly in a table, by binding these values into a data file of sorts using the cbind() or column bind function.\n嗯，乱糟糟的！BirdData 是一个长度为 32（4 x 8）的向量。它之所以这样组织，是因为它无法区分每个变量中的 8 个值。为了解决这个问题，我们需要创建另一个包含 32 个值的变量来使用索引。我们也可以采用更复杂的方法，使用 repeat 和 sequence 工具对其进行排序，然后再进行对齐，但我更喜欢更简单的方法。\n我们可以直接在表中合并它们，方法是使用 cbind() 或列绑定函数将这些值绑定到一个数据文件中。\n\nBirdData2 &lt;- cbind(Wingcrd, Tarsus, Head, Wt) \nBirdData2 # See product of your labour. A data file!\n\n     Wingcrd Tarsus Head   Wt\n[1,]    59.0   22.3 31.2  9.5\n[2,]    55.0   19.7 30.4 13.8\n[3,]    53.5   20.8 30.6 14.8\n[4,]    55.0   20.3 30.3 15.2\n[5,]    52.5   20.8 30.3 15.5\n[6,]    57.5   21.5 30.8 15.6\n[7,]    53.0   20.6 32.5 15.6\n[8,]    55.0   21.5   NA 15.7\n\n\nIf you click on the “BirdData2” object in the Environment tab above right window it will open as a tab in this window. Looks familiar doesn’t it?!\nNotice this is one of the few occasions where the function call has an uppercase letter it is ‘View’ not ‘view’. This is because it was uniquely created for RStudio. This highly unusual all R functions start with lowercase letters (e.g. mean, sd).\nNo suppose you want to access elements of BirdData2 to check on data accuracy. We can find elements as we did above by using the square brackets. Pay attention to the format of the subset search here [row,column]. You need to tell R to look in the right place. The rows are always the first element in the bracket followed by a comma and the column. R is expecting a number in this bracket and the numbers are effectively coordinates. So [1,1] will return the value that is located in the first row and first column of the table.\n如果您在右侧窗口上方的“环境”选项卡中点击“BirdData2”对象，它将在此窗口中以选项卡的形式打开。看起来很熟悉，是吧？！\n请注意，这是函数调用中少数几个包含大写字母的情况之一，它是“View”而不是“view”。这是因为它是专门为 RStudio 创建的。所有 R 函数都以小写字母开头（例如 mean、sd），这种情况非常罕见。\n假设您想访问 BirdData2 的元素来检查数据准确性。我们可以像上面一样使用方括号查找元素。请注意此处子集搜索的格式为 [row,column]。您需要告诉 R 在正确的位置查找。行始终是括号中的第一个元素，后跟逗号和列。R 期望括号中是一个数字，而这些数字实际上是坐标。因此 [1,1] 将返回位于表格第一行第一列的值。\n\nBirdData2[,1] # lists ALL the data from the first column. There is no value for the row, and\n\n[1] 59.0 55.0 53.5 55.0 52.5 57.5 53.0 55.0\n\nBirdData2[1,] # lists ALL the data from the first row.\n\nWingcrd  Tarsus    Head      Wt \n   59.0    22.3    31.2     9.5 \n\nBirdData2[1,1] # Reports the item in the first row and column \n\nWingcrd \n     59 \n\nBirdData2[,2 : 4] # Reports the data in columns 2-4 or \"Tarsus\", \"Head\" and \"Wt\" \n\n     Tarsus Head   Wt\n[1,]   22.3 31.2  9.5\n[2,]   19.7 30.4 13.8\n[3,]   20.8 30.6 14.8\n[4,]   20.3 30.3 15.2\n[5,]   20.8 30.3 15.5\n[6,]   21.5 30.8 15.6\n[7,]   20.6 32.5 15.6\n[8,]   21.5   NA 15.7\n\nBirdData2[,-1] # Returns all columns minus column one \"Wingcrd\n\n     Tarsus Head   Wt\n[1,]   22.3 31.2  9.5\n[2,]   19.7 30.4 13.8\n[3,]   20.8 30.6 14.8\n[4,]   20.3 30.3 15.2\n[5,]   20.8 30.3 15.5\n[6,]   21.5 30.8 15.6\n[7,]   20.6 32.5 15.6\n[8,]   21.5   NA 15.7\n\nBirdData2[, c(1, 2, 4)] # Returns all the data from columns 1, 2 and 4 (i.e. non sequential)\n\n     Wingcrd Tarsus   Wt\n[1,]    59.0   22.3  9.5\n[2,]    55.0   19.7 13.8\n[3,]    53.5   20.8 14.8\n[4,]    55.0   20.3 15.2\n[5,]    52.5   20.8 15.5\n[6,]    57.5   21.5 15.6\n[7,]    53.0   20.6 15.6\n[8,]    55.0   21.5 15.7\n\n\nThat’s a lot of work for a simple data file we could (and will later) load in from a text file or excel. Why do it? Because you’ve learned some very important functions c(), and cbind() are three of my favourites (but check rbind). What we did was create a simple table of data that looked like a spreadsheet. But it is actually an array or matrix, lacking the column headers you typically find in a spreadsheet.\n对于一个简单的数据文件来说，这可是个大工程，我们可以（以后也会）从文本文件或 Excel 中加载它。为什么要这么做？因为你已经学习了一些非常重要的函数，其中“c”和“cbind”是我最喜欢的三个函数（不过，也请检查一下“rbind”）。我们所做的就是创建一个简单的数据表，看起来像电子表格。但它实际上是一个数组或矩阵，缺少电子表格中常见的列标题。\n\nclass(BirdData2)\n\n[1] \"matrix\" \"array\" \n\n\nTo use it you need to use column and row numbers. Climate scientists frequently use arrays for their larger dataset because the require less space and can be more readily used for mathematical calculations, but sometimes is easier for our data to be stored in an spreadsheet object known as a dataframe.\n要使用它，您需要使用列号和行号。气候科学家经常使用数组来存储较大的数据集，因为数组占用的空间更少，而且更容易用于数学计算。但有时，将数据存储在称为数据框的电子表格对象中会更方便。\n\n\n1.2.6 Creating and using dataframes\nThey have special properties and are the general means of housing data in R (after import). So we can recreate the data file above using the data.frame function (we’ll shorten the names as well).\n它们具有特殊属性，是 R 中（导入后）存储数据的通用方法。因此，我们可以使用 data.frame 函数重新创建上面的数据文件（我们也会缩短其名称）。\n\nBirdData3 &lt;- data.frame(WC = Wingcrd, TS = Tarsus,\n  HD = Head, W = Wt) \nBirdData3\n\n    WC   TS   HD    W\n1 59.0 22.3 31.2  9.5\n2 55.0 19.7 30.4 13.8\n3 53.5 20.8 30.6 14.8\n4 55.0 20.3 30.3 15.2\n5 52.5 20.8 30.3 15.5\n6 57.5 21.5 30.8 15.6\n7 53.0 20.6 32.5 15.6\n8 55.0 21.5   NA 15.7\n\n\nLook the environment tab again it’s added a blue arrow symbol and it tells you how many observations (=rows) and variables (=columns) you have in the dataframe. Now click on the blue arrow. This shows your column data and the column name. Click on the name and it will open the file up in View.\nSome key things you need to know about dataframes.\n\nThey hold data of different types unlike matrices and vectors more easily;\nNo real limit on size (for most purposes) but bigger does = slower;\nThey allow you to draw out data with all the available filters and searches (see above);\nYou can use the dollar sign operator assigns column to the data frame.\n\n再次查看“环境”选项卡，它添加了一个蓝色箭头符号，它指示数据框中有多少个观察值（=行）和变量（=列）。现在点击蓝色箭头。这将显示您的列数据和列名称。点击列名称，它将在视图中打开文件。\n关于数据框，您需要了解一些关键事项。\n\n与矩阵和向量不同，它们更容易保存不同类型的数据；\n对大小没有实际限制（在大多数情况下），但数据框越大，速度越慢；\n它们允许您使用所有可用的过滤器和搜索功能（参见上文）提取数据；\n您可以使用美元符号运算符将列分配给数据框。\n\n\nBirdData3\n\n    WC   TS   HD    W\n1 59.0 22.3 31.2  9.5\n2 55.0 19.7 30.4 13.8\n3 53.5 20.8 30.6 14.8\n4 55.0 20.3 30.3 15.2\n5 52.5 20.8 30.3 15.5\n6 57.5 21.5 30.8 15.6\n7 53.0 20.6 32.5 15.6\n8 55.0 21.5   NA 15.7\n\n\nReturns the full dataframe in its entirety, whereas:\n返回完整的数据框，而：\n\nBirdData3$WC \n\n[1] 59.0 55.0 53.5 55.0 52.5 57.5 53.0 55.0\n\n\nReturns just that one column. 仅返回该列。\nOnce you have your data in a dataframe, you can also add in additional columns and also rename either the column headings and the row headings using the colnames() and rownames() functions. Have a look at this using the ?? help from the console window / pane (if you are in RStudio) 将数据放入数据框后，您还可以添加其他列，并使用 colnames() 和 rownames(). 函数重命名列标题和行标题。如果您使用的是 RStudio，请使用控制台窗口/窗格中的 ?? 帮助查看此信息。\n\n\n1.2.7 A few more key functions\nThese will be useful as you progress:\nread.table() imports data from a tab separated text file (see below) names() lists the column names in the datafile. str() shows the data structure of the datafile. dim() gives the file dimensions (rows v columns). head() displays the first six rows. tail() displays the last six rows. View() displays 1000 rows of the datafile (like a spreadsheet) as you recall from above.\nread.table() 从制表符分隔的文本文件导入数据（见下文） names() 列出数据文件中的列名 str() 显示数据文件的数据结构。 dim() 给出完整的维度（行 v 列） head() 显示前六行 tail() 显示后六行 View() 显示数据文件的 1000 行（像电子表格一样），正如您从上面回忆的那样。\n\n\n1.2.8 Loading in data from a file\nWe’ll start by importing the deer.txt using read.table. Remember unless you have set your data subdirectory as your working directory you need to show R where the data is located. My standard practice is to open my codefiles from the directory by double-clinking on it. This will open RStudio and then working directory will the the code file directory. I then ‘hardwire’ the read calls using a path in the function:\n我们首先使用 read.table 导入 deer.txt 文件。记住，除非你已将数据子目录设置为工作目录，否则你需要向 R 展示数据所在的位置。我的标准做法是双击该目录打开我的代码文件。这将打开 RStudio，然后工作目录将指向代码文件目录。然后，我使用函数中的路径“硬连接” read 调用：\n\nDeer &lt;- read.table(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/Deer.txt\", header=TRUE)\n\nThis is easier than you’d imagine. Look at the bottom right window. This shows files, plots, etc. Click on ‘Files’. This will show you where R is currently ‘looking’. You can click on the subdirectories listed in the directory structure till you land on the ‘data’ subdirectory. Then simply click on the blue cog and select copy ‘folder path to clipboard’ (Fig. 2.4). Then paste this into your read function call (see above). I use a mac, so your windows path will look different.\n这比你想象的要简单。查看右下角的窗口。这里显示了文件、图表等。点击“文件”。这将显示 R 当前正在“查看”的位置。你可以点击目录结构中列出的子目录，直到找到“数据”子目录。然后点击蓝色齿轮，选择“将文件夹路径复制到剪贴板”（图 3）。然后将其粘贴到你的 read 函数调用中（见上文）。我用的是 Mac，所以你的 Windows 路径看起来会有所不同。\n\n\n\n\n\nCopying the path information\n\n\n\n\nNow we have the data loaded let’s explore some of those commands we mentioned above.\n现在我们已经加载了数据，让我们来探索一下上面提到的一些命令。\n\ndim(Deer)\n\n[1] 1182    9\n\n\nWe have 1182 row and 9 columns in our data.\n我们的数据有 1182 行和 9 列。\n\nhead(Deer) \n\n  Farm Month Year Sex clas1_4 LCT   KFI Ecervi Tb\n1   AL    10    0   1       4 191 20.45   0.00  0\n2   AL    10    0   1       4 180 16.40   0.00  0\n3   AL    10    0   1       3 192 15.90   2.38  0\n4   AL    10    0   1       4 196 17.30   0.00  0\n5   AL    10    0   1       4 204    NA   0.00 NA\n6   AL    10    0   1       4 190 16.30   0.00  0\n\n\nShows our first 6 rows. Have a guess what ‘tail’ shows! If we want to understand the type of data we have in the file we can use base R’s str() function or tidyverse’s glimpse(). I prefer the latter but it’s your decision.\n显示前 6 行。猜猜“tail”显示的是什么！如果我们想了解文件中的数据类型，可以使用基础 R 的 str() 函数或 tidyverse 的 glimpse()。我更喜欢后者，但最终决定权在你。\n\nstr(Deer)\n\n'data.frame':   1182 obs. of  9 variables:\n $ Farm   : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ Month  : int  10 10 10 10 10 10 10 10 10 10 ...\n $ Year   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Sex    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ clas1_4: int  4 4 3 4 4 4 4 4 4 4 ...\n $ LCT    : num  191 180 192 196 204 190 196 200 197 208 ...\n $ KFI    : num  20.4 16.4 15.9 17.3 NA ...\n $ Ecervi : num  0 0 2.38 0 0 0 1.21 0 0.8 0 ...\n $ Tb     : int  0 0 0 0 NA 0 NA 1 0 0 ...\n\n\n\nglimpse(Deer)\n\nRows: 1,182\nColumns: 9\n$ Farm    &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"A…\n$ Month   &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 12…\n$ Year    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 99, 99, 99, 99, 9…\n$ Sex     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,…\n$ clas1_4 &lt;int&gt; 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 4,…\n$ LCT     &lt;dbl&gt; 191, 180, 192, 196, 204, 190, 196, 200, 197, 208, 216, 199, 17…\n$ KFI     &lt;dbl&gt; 20.45, 16.40, 15.90, 17.30, NA, 16.30, 22.20, 14.70, 13.50, 15…\n$ Ecervi  &lt;dbl&gt; 0.0000, 0.0000, 2.3800, 0.0000, 0.0000, 0.0000, 1.2100, 0.0000…\n$ Tb      &lt;int&gt; 0, 0, 0, 0, NA, 0, NA, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, NA, NA…\n\n\nIf we want to how many observations we have per farm, then we can use the table() function. This is brilliant for looking at balance of say an ANOVA (Analysis of Variance) design or even an interaction in a regression call.\n如果我们想知道每个农场有多少个观测值，可以使用 table() 函数。这对于查看方差分析 (ANOVA) 设计的平衡性，甚至回归分析中的交互作用都非常有效。\n\ntable(Deer$Farm)\n\n\n   AL    AU    BA    BE    CB   CRC    HB   LCV    LN   MAN    MB    MO    NC \n   15    37    98    19    93    16    35     2    34    76    41   278    32 \n   NV    PA    PN    QM    RF    RO R\\xd1   SAL   SAU    SE    TI    TN  VISO \n   35    11    45    75    34    44    25     1     3    26    21    31    15 \n   VY \n   40 \n\n\nWe can see that a few farms have got very few observations (i.e. it’s unbalanced) - we’d need to be careful in how we analyse it.\n我们可以看到，一些农场的观测数据非常少（即不平衡），我们需要谨慎分析。\nIf we were interested in seeing whether gender interacts with year, then:\n如果我们想知道性别是否与年份有相互作用，那么：\n\ntable(Deer$Sex, Deer$Year) \n\n   \n      0   1   2   3   4   5  99\n  1 115  85 154  75  78  34  21\n  2  76  40 197 123  60  35   0\n\n\nRows: 1 = Males, 2 = Female Columns: 0 = 2000, 1 = 2001, 2 = 2002, 3 = 2003, 4 = 2004, 5 = 2005 and 99 = 1999\nIn 1999 no females were sampled, so you cannot use an interaction effect for Year:Sex in your models. This will make sense later in the course!\nNow let’s spend a few minutes using some of the new knowledge you have gained.\n行：1 = 男性，2 = 女性；列：0 = 2000，1 = 2001，2 = 2002，3 = 2003，4 = 2004，5 = 2005，99 = 1999\n1999 年没有女性样本，因此你不能在模型中使用“年份：性别”的交互作用效应。这在课程的后续部分会讲解！\n现在，让我们花几分钟时间运用你学到的一些新知识。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#class-exercises",
    "href": "chap1.html#class-exercises",
    "title": "1  An introduction to R and RStudio",
    "section": "1.3 Class Exercises",
    "text": "1.3 Class Exercises\n\n1.3.1 Exercise 1 - using the c function to create vectors\n\n\n\nTable 1: Deer Measurement Data\n\n\nFarm\nMonth\nYear\nSex\nLengthClass\nLengthCT\nEcervi\nTb\n\n\n\n\nMO\n11\n00\n1\n1\n75.0\n0\n0\n\n\nMO\n7\n00\n2\n1\n85.0\n0\n0\n\n\nMO\n7\n01\n2\n1\n91.6\n0\n1\n\n\nMO\nNA\nNA\n2\n1\n95.0\nNA\nNA\n\n\nLN\n9\n03\n1\n1\nNA\n0\n0\n\n\nSE\n9\n03\n2\n1\n105.1\n0\n0\n\n\nQM\n11\n02\n2\n1\n106.0\n0\n0\n\n\n\n\n\nThese data are a small part of a data file we imported earlier ( = deer.txt for full data).\nTb is the occurrence of Tuberculosis in cattle on some farms; The variables Farm, Month, Year, Sex, LengthClass are self-explanatory Ecervi is the occurrence of a cattle parasite.\n\nUsing the c function, create a variable that contains the length variable that contains the lengths of seven animals;\nAlso create a variable that contains the Tb values;\nWhat’s the average length, sd of the length of the cattle on the farms?\n\nTb 表示某些农场牛的结核病发病率；变量 Farm、Month、Year、Sex 和 LengthClass 的含义不言自明；Ecervi 表示牛寄生虫的发病率。\n\n使用 c 函数创建一个变量，其中包含长度变量，该变量包含七头牛的长度；\n再创建一个变量，其中包含 Tb 值；\n农场牛的平均长度（标准差）是多少？\n\n\n\n1.3.2 Exercise 2:\nUsing BirdData2 find the following by using the [ ] brackets to subset data.\n\ndata found in the cell situated in the last row and last column.\nrows 3-4 in column called “Head”.\nAll the data from columns 2 and 3 (you need to use 2: 3 to link the columns).\nAll the weights and write them to a variable Y.\nAll the columns excluding the first and third “Wingcrd” and “Wt”; use the ‘c’ function to do this. Write them to object called ‘X’.\n\n使用 BirdData2，通过使用 [ ] 括号对数据进行子集处理，找到以下结果：\n\n在最后一行最后一列的单元格中找到的数据。\n“Head”列的第 3-4 行。\n第 2 列和第 3 列的所有数据（需要使用 2:3 连接各列）。\n计算所有权重并将其写入变量 Y。\n除第一列和第三列“Wingcrd”和“Wt”之外的所有列；使用“c”函数执行此操作。将它们写入名为“X”的对象。\n\n\n\n1.3.3 Exercise 3: Creating and using data frames\n\nUse the data on deer parasites and Tb (EXERCISE 1 above) to create a data frame\nSquare root transform the length variable\nAdd it to the data frame as a new variable\n\nHINT: To do this you need to use c() to create all the variables first!!!\n\n使用鹿寄生虫和结核分枝杆菌的数据（上述练习 1）创建一个数据框\n对长度变量进行平方根变换\n将其作为新变量添加到数据框中\n\n提示：要执行此操作，您需要先使用 c() 创建所有变量！",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#post-class-work",
    "href": "chap1.html#post-class-work",
    "title": "1  An introduction to R and RStudio",
    "section": "1.4 Post class work",
    "text": "1.4 Post class work\n\nComplete the class exercises,if you haven’t already done so.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#final-reflection-on-surviving-the-module",
    "href": "chap1.html#final-reflection-on-surviving-the-module",
    "title": "1  An introduction to R and RStudio",
    "section": "1.5 Final reflection on surviving the module!",
    "text": "1.5 Final reflection on surviving the module!\nEffort is everything here. Programming is time intensive and involves a lot of problem solving. To to spend a little time learning R each day. The pay off will be high in the long run. Here are a couple of excellent resources to stay abreast of things that are happening:\n在这里，努力至关重要。编程非常耗时，而且需要解决很多问题。每天花一点时间学习 R 语言，从长远来看，回报将非常丰厚。以下是一些优秀的资源，可以帮助您随时了解最新动态：\n\ntidyverse blog\nR Weekly",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#next-week",
    "href": "chap1.html#next-week",
    "title": "1  An introduction to R and RStudio",
    "section": "1.6 Next Week",
    "text": "1.6 Next Week\nWe’ll be delving deeper into data imports, data manipulation and the murky world of data tidying and wrangling.\n\nHave a quick look at Chapters 3, 4, 5, 7, 19 and 20 in Wickham, Cetinkaya-Rundel, and Grolemund (2023).\nRead Chapter 3 of the Beckerman, Childs, and Petchey (2017) book. We have online access to this book.\n\n我们将深入探讨数据导入、数据操作以及数据整理和整理的复杂世界。请快速浏览@Wickham2023的第3、4、5、7、19和20章。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap1.html#references",
    "href": "chap1.html#references",
    "title": "1  An introduction to R and RStudio",
    "section": "1.7 References",
    "text": "1.7 References\n\n\n\n\nBeckerman, Andrew, Dylan Childs, and Owen Petchey. 2017. Getting Started with R: An Introduction for Biologists. 2nd. ed. Oxford: Oxford University Press.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd Ed. O’Reilly.\n\n\nZuur, Alain F., Elena N. Ieno, and Erik Meesters. 2009. A Beginner’s Guide to R (Use R!). New York: Springer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "chap2.html",
    "href": "chap2.html",
    "title": "2  Importing files and manipulating data",
    "section": "",
    "text": "2.1 Outline\nLast week we covered the basics of inputting and accessing data in vectors and eventually dataframes. We concluded by importing data (tab delineated) into R as dataframe. This week the aim is to familarise you with R data file imports using various file formats:\nAnd, focus on the functions in base R and the tidyverse package that we can use to evaluate data structures, summarise, select and aggregate data and so on. Buckle up, it’s a bit dry but it is essential material!\n上周我们讲解了在向量以及数据框中输入和访问数据的基础知识。最后，我们讲解了如何将数据（制表符分隔）以数据框的形式导入 R。本周的目标是让你熟悉使用各种文件格式导入 R 数据文件：\n制表符分隔的 txt 文件\n逗号分隔的 csv 文件（我们通常使用的格式）\n以及 Excel 电子表格格式，例如 xls 和 xlsx。\n接下来，我们将重点介绍 R 基础库和 tidyverse 包中的函数，这些函数可以用来评估数据结构、汇总、选择和聚合数据等等。系好安全带，虽然内容有点枯燥，但绝对是必学内容！",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#outline",
    "href": "chap2.html#outline",
    "title": "2  Importing files and manipulating data",
    "section": "",
    "text": "Tab delineated txt files\nComma separated csv files (this usual format we select)\nAnd excel spreadsheet format, e.g. xls and xlsx.\n\n\n\n\n\n\n\n\n2.1.1 Learning Outcomes\nBy the end of this session you will be able to:\n\nLoad data into R stored in most of the heavily used formats (we are excluding those commonly used in remote sensing, such as shapefiles, geopackages and rasters). But should you need to access these for your projects then drop me an email.\nUse common functions to modify, select, subset, summarise and aggregate data. These are the data wrangling skills covered in Wickham et al. (2019) and chapters 3 and 4 of his book (Wickham, Cetinkaya-Rundel, and Grolemund 2023) Data Science, which is available online here: https://r4ds.hadley.nz/data-transform.htmlhttps://r4ds.hadley.nz/data-transform.html.\n\n在本课程结束时，您将能够：\n将以大多数常用格式存储的数据加载到 R 中（我们不包括遥感领域常用的格式，例如 Shapefile、地理包和栅格数据）。如果您需要访问这些格式的数据，请给我发送电子邮件。\n使用常用函数修改、选择、子集化、汇总和聚合数据。这些是 Wickham et al. (2019) 及其著作 (Wickham, Cetinkaya-Rundel, and Grolemund 2023)《数据科学》第 3 章和第 4 章中涵盖的数据整理技巧，该书可在此处在线获取：https://r4ds.hadley.nz/data-transform.htmlhttps://r4ds.hadley.nz/data-transform.html。\n\n\n2.1.2 Load the packages\nWe’ll need tidyverse and two additional packages today. We use a little bit of code that checks our installation for packages we have previously installed and if it finds one missing it will install it for us. Don’t worry about the code right now, we’ll introduce to the function lapply() as the module progresses.\n今天我们需要 tidyverse 和另外两个软件包。我们使用一小段代码来检查安装中是否存在之前安装过的软件包，如果发现缺少某个软件包，就会自动安装。现在先不用担心代码，随着模块的进展，我们会逐步介绍 lapply 函数。\n\n# List of packages\npackages &lt;- c(\"skimr\",\"readxl\",\"tidyverse\")\n\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n[[1]]\n[1] \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[7] \"methods\"   \"base\"     \n\n[[2]]\n[1] \"readxl\"    \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n [7] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"readxl\"    \"skimr\"    \n[13] \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"  \n[19] \"base\"     \n\n\nNOTE: You can load the libraries using the pacman package, which will install any that are not in your current R setup. i.e. simulate the code above. The function we use to do this is p_load(). We are not using this package because it does not work well with the UoB apps anywhere simulator. But you could do this on your own computer. It’s cleaner and easier.\nOur new packages for the data are:\n\nskimr - this is a package designed to generate on the fly summaries of data in dataframes (Waring et al. 2022).\nreadxl - although not formerly part of the tidyverse suite this is installed along side it. It is a package for reading data into R from the various variants of Microsoft’s excel spreadsheet (Wickham and Bryan 2023).\n\n注意：您可以使用 pacman 包加载库，它会安装您当前 R 设置中不存在的任何库。例如，模拟上面的代码。我们使用的函数是 p_load()。我们没有使用此包，因为它与 UoB Apps Anywhere 模拟器配合不佳。但您可以在自己的电脑上执行此操作，这样更简洁、更轻松。\n我们新的数据包包括：\nskimr - 这是一个用于动态生成数据框中数据摘要的包 (Waring et al. 2022)。\nreadxl - 虽然它以前不是 tidyverse 套件的一部分，但它与 tidyverse 套件一起安装。它是一个用于从各种 Microsoft Excel 电子表格版本中将数据读入 R 的包 (Wickham and Bryan 2023)。\n\n\n2.1.3 Create your codefile\nAs last call it something sensible, like Week_2_LM25556. You don’t need the .R suffix, that will be added for you when you save it.\n最后，给它起个更合理的名字，比如 Week_2_LM25556。不需要 .R 后缀，保存时会自动添加。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#getting-data-into-r-via-file-import",
    "href": "chap2.html#getting-data-into-r-via-file-import",
    "title": "2  Importing files and manipulating data",
    "section": "2.2 Getting Data into R via file import",
    "text": "2.2 Getting Data into R via file import\nRemember to either set your working directory to the …data directory or better still hardwire to the file by adding the path to the read command. I opt for the latter because as long as you don’t change the directory structure, when you revisit the project every data load will work seamlessly.\nWe will focus on the two main text file types initially:\n\ntab delineated (txt);\ncomma delineated (cvs).\n\nThese are ASCI data so are compact and small. Note though, there are not compressed. We need other commands for compressed files.\nThen we’ll look at importing excel formats.\n请记住，要么将工作目录设置为 …data 目录，要么最好通过将路径添加到读取命令来将其固定到文件。我选择后者，因为只要您不更改目录结构，当您重新访问项目时，每次数据加载都能无缝进行。\n我们将首先关注两种主要的文本文件类型：\n制表符分隔 (txt)；\n逗号分隔 (cvs)。\n这些是 ASCI 数据，因此紧凑且小巧。但请注意，它们没有压缩。我们需要其他用于压缩文件的命令。\n然后我们将讨论导入 Excel 格式。\n\n2.2.1 Tab text files\nWe’ll start by using the read.table function for “tab delineated” data. The key elements of this are : the read.table() function (from base R), the text in the parentheses which is the path and datafile name. You will need the suffix! And the last element header = TRUE tells R that the column names are in the first row of the table. This format is the same for all of core import functions.\n我们首先使用 read.table 函数来处理“制表符分隔”数据。其关键元素包括：read.table() 函数（来自基础 R），括号中的文本（即路径和数据文件名）。您需要后缀！最后一个元素 header = TRUE 告诉 R 列名位于表的第一行。所有核心导入函数都采用相同的格式。\n\nSquid &lt;- read.table(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/squid.txt\", header = TRUE)\n\n\n\n2.2.2 csv files\nTo load in a csv file we write a similar bit of code. We’ll select bird_stress.csv as we will be using it later on:\n\nSleep &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/MammalSleep.csv\")\n\nRows: 83 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, genus, vore, order, conservation\ndbl (6): sleep_total, sleep_rem, sleep_cycle, awake, brainwt, bodywt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice we have just changed read call to read_csv. This function is in the tidyverse package and does not require us to specify whether the file has a header or not. It also provides you with some additional information on the file. NOTE: base R has its own version called read.csv. But that command will need the header = TRUE argument. Read chapter 7 of Wickham, Cetinkaya-Rundel, and Grolemund (2023) for a more detailed outline of this command and its use: https://r4ds.hadley.nz/data-import.html.\n注意，我们刚刚将 read 调用改为 read_csv。此函数位于 tidyverse 包中，不需要我们指定文件是否包含文件头。它还会提供一些关于文件的其他信息。注意：R 语言有自己的版本 read.csv。但该命令需要 header = TRUE 参数。阅读 Wickham, Cetinkaya-Rundel, and Grolemund (2023) 的第 7 章，了解有关此命令及其用法的更详细概述：https://r4ds.hadley.nz/data-import.html。\n\n\n2.2.3 Excel files\nThe last format we’ll focus on is data sat in xls or xlsx spreadsheets. This is also covered in depth (along with Google Sheets imports) in Wickham, Cetinkaya-Rundel, and Grolemund (2023), chapter 20: https://r4ds.hadley.nz/spreadsheets.html. The functions we use here are in the readxl package:\n\nread_xls() reads Excel files with xls format.\nread_xlsx() read Excel files with xlsx format.\nread_excel() can read files with both formats. It guesses the file type based on the input file.\n\nAs you are aware both xls and xlsx files permit user to create multiple sheets within the spreadsheet. As a result you need to specify the sheet of data you want. The default will read in sheet 1 (the first sheet listed in the spreasheet). Let’s have a go with two spreadsheets: fish.xlsx and birdsurvey.xls.\n我们最后要关注的格式是 xls 或 xlsx 电子表格中的数据。Wickham, Cetinkaya-Rundel, and Grolemund (2023) 的第 20 章也对此进行了深入介绍（包括 Google 表格的导入）：https://r4ds.hadley.nz/spreadsheets.html。我们这里使用的函数位于 readxl 包中：\nread_xls() 读取 xls 格式的 Excel 文件。\nread_xlsx() 读取 xlsx 格式的 Excel 文件。\nread_excel() 可以读取两种格式的文件。它会根据输入文件猜测文件类型。\n正如您所知，xls 和 xlsx 文件都允许用户在电子表格中创建多个工作表。因此，您需要指定所需的数据工作表。默认会读取工作表 1（电子表格中列出的第一个工作表）。让我们尝试使用两个电子表格：fish.xlsx 和 birdsurvey.xls。\n\n# read both in \n# birdsurvey.xls first\nbird &lt;- read_xls(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/birdsurvey.xls\")\nfish &lt;- read_xlsx(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/fish.xlsx\")\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n\n\nThe first import worked well as there is only one sheet in the file. Second one didn’t. The ‘New names:’ response is because there were empty columns in the sheet. If you view it by double clicking the dataframe in the environment window (top right), you’ll see it has pulled in the sheet with all the metadata. You actually need sheet 2. We can amend the code to do that. Notice that the sheet argument sits outside the quotes.\n第一次导入工作顺利，因为文件中只有一张工作表。第二次导入则不然。“新名称：”的响应是因为工作表中有空列。如果您双击环境窗口（右上角）中的数据框来查看，您会看到它已导入包含所有元数据的工作表。您实际上需要的是工作表 2。我们可以修改代码来实现这一点。请注意，工作表参数位于引号之外。\n\n# sheet = 2 is the datasheet number within the spreadsheet [they are number 1-n left to right!]\nfish &lt;- read_xlsx(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/fish.xlsx\", sheet=2)\n\n\n\n2.2.4 Things to watch out for\nWhen importing an Excel, CVS, or any other spreadsheet for that matter, make sure that your data are formatted correctly with variables in columns. Use sensible column names, and don’t use these characters are R doesn’t like them: £, $, %, ^, ,, ., &lt;&gt;, ?, , |, [], {} etc. Make sure missing values have a ‘NA’ and they are not left blank. Having said that, read_csv and the other excel import functions are very flexible and will load the files with a multitude of issues, but it will recode things en route and you will have to do some additional tidying afterwards.\n导入 Excel、CVS 或任何其他电子表格时，请确保数据格式正确，列中包含变量。使用合理的列名，不要使用 R 不支持的字符：£, $, %, ^, ,, ., &lt;&gt;, ?, , |, [], {} 等。确保缺失值标有“NA”，且不留空。话虽如此，read_csv 和其他 Excel 导入函数非常灵活，虽然加载文件时可能会出现很多问题，但它会在导入过程中重新编码，之后您还需要进行一些额外的整理工作。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#examining-whats-in-the-dataframe",
    "href": "chap2.html#examining-whats-in-the-dataframe",
    "title": "2  Importing files and manipulating data",
    "section": "2.3 Examining what’s in the dataframe",
    "text": "2.3 Examining what’s in the dataframe\nYou should now have three separate dataframes loaded in your Environment; see the above right window.\nThere are a range of options here from base R, tidyverse and skimr to look at these data to evaluate some basic parameters. We’ll look at them all. First up though, you can just click on a dataframe in the Environment window (top left) and then RStudio will use it’s bespoke View function to open the file in your code window. It opens in a spreadsheet format and you can scroll up and down and left and right.\n现在，您的“环境”窗口中应该已经加载了三个独立的数据框；请参见右上方窗口。\n这里有一系列选项，包括基础 R、tidyverse 和 skimr，可用于查看这些数据并评估一些基本参数。我们将逐一介绍它们。首先，您只需在“环境”窗口（左上角）中点击一个数据框，RStudio 就会使用其定制的“查看”功能在您的代码窗口中打开该文件。它会以电子表格格式打开，您可以上下左右滚动查看。\n\n2.3.1 base R\nLet’s focus on the Squid data to start with and use some base R functions to evaluate it. You were introduced to these last week. Run the code the review the outcomes.\n我们先来关注一下 Squid 数据，并使用一些基本的 R 函数来评估它。我们上周已经介绍过这些函数了。运行代码并查看结果。\n\nnames(Squid)        # Lists the column (or variable names)\n\n[1] \"Sample\"   \"Year\"     \"Month\"    \"Location\" \"Sex\"      \"GSI\"     \n\nstr(Squid)          # Indicates the structure of the data frame (i.e. variable types etc)\n\n'data.frame':   2644 obs. of  6 variables:\n $ Sample  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Year    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Month   : int  1 1 1 1 1 1 1 1 1 2 ...\n $ Location: int  1 3 1 1 1 1 1 3 3 1 ...\n $ Sex     : int  2 2 2 2 2 2 2 2 2 2 ...\n $ GSI     : num  10.44 9.83 9.74 9.31 8.99 ...\n\nhead(Squid)         # Lists the first 6 rows of data\n\n  Sample Year Month Location Sex     GSI\n1      1    1     1        1   2 10.4432\n2      2    1     1        3   2  9.8331\n3      3    1     1        1   2  9.7356\n4      4    1     1        1   2  9.3107\n5      5    1     1        1   2  8.9926\n6      6    1     1        1   2  8.7707\n\ntail(Squid)         # No prizes for guessing what that does....\n\n     Sample Year Month Location Sex    GSI\n2639   2639    4    12        1   2 0.0257\n2640   2640    4    10        1   1 0.0200\n2641   2641    4    10        1   2 0.0191\n2642   2642    4    12        1   1 0.0165\n2643   2643    4    10        1   1 0.0132\n2644   2644    4    12        1   1 0.0070\n\ndim(Squid)          # Lists the dimensions of the data frame (in this case 2644 cases or rows and 6 variables or columns)\n\n[1] 2644    6\n\n\nOut of all of the above, str() is probably the most useful because it allows you to view the ranges and classes of data in the dataframe. This is important because the data class has implications for the analyses we can do.\n以上所有方法中，str() 可能是最有用的，因为它允许你查看数据框中数据的范围和类别。这很重要，因为数据类别会影响我们接下来的分析。\n\n\n2.3.2 tidyverse\ntidyverse has a version of this called glimpse. It is essentially a souped up str() with some evident benefits:\n\n\n\n\n\n\n\n\n\nFeature\nstr() (Base R)\nglimpse() (Tidyverse)\nWinner\n\n\n\n\nLayout\nHorizontal, wraps messily\nVertical, aligned, one-per-line\nglimpse()\n\n\nReadability\nCan be difficult to scan\nExcellent, easy to scan\nglimpse()\n\n\nShows Dimensions\nIn a sentence at the start\nClearly at the top (Rows: …, Columns: …)\nglimpse()\n\n\nType Display\nnum, chr, Factor\n&lt;dbl&gt;, &lt;chr&gt;, &lt;fct&gt;\n(Matter of preference, but glimpse() is more consistent with Tidyverse)\n\n\nPipe-friendly\nNo, str(data)\nYes, data %&gt;% glimpse()\nglimpse()\n\n\nAvailability\nAlways available in base R\nRequires dplyr or tibble to be loaded\nstr()\n\n\n\n\n\n2.3.3 skimr\nIf you want more summary information without troubling yourself with some addtional work then skimr is a good choice. This packages generates a host of useful summary information: including summary quartiles (p25, p50 (this is the median), p75), mean, max (p100), min (p0), the number of missing data points (or NAs - super useful). It even draws a raggy little histogram so you can ‘eyeball’ the distribution of each variable.\n如果您想获得更多摘要信息，又不想费力做其他工作，skimr 是个不错的选择。这个软件包可以生成大量有用的摘要信息：包括摘要四分位数（p25、p50（中位数）、p75）、平均值、最大值（p100）、最小值（p0）、缺失数据点数（或 NA - 非常有用）。它甚至还会绘制一个粗糙的小直方图，方便您“目测”每个变量的分布情况。\n\nskim(Squid)\n\n\nData summary\n\n\nName\nSquid\n\n\nNumber of rows\n2644\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSample\n0\n1\n1322.50\n763.40\n1.00\n661.75\n1322.50\n1983.25\n2644.0\n▇▇▇▇▇\n\n\nYear\n0\n1\n2.36\n0.96\n1.00\n2.00\n2.00\n3.00\n4.0\n▅▇▁▆▃\n\n\nMonth\n0\n1\n6.01\n3.34\n1.00\n3.00\n6.00\n9.00\n12.0\n▇▇▅▅▆\n\n\nLocation\n0\n1\n1.45\n0.84\n1.00\n1.00\n1.00\n1.00\n4.0\n▇▁▁▂▁\n\n\nSex\n0\n1\n1.47\n0.50\n1.00\n1.00\n1.00\n2.00\n2.0\n▇▁▁▁▇\n\n\nGSI\n0\n1\n2.19\n2.66\n0.01\n0.17\n1.17\n2.78\n14.6\n▇▁▁▁▁\n\n\n\n\n\nHave a look at the other files with glimpse() and skim().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#transforming-data---summarising-aggregating-filtering-mutating",
    "href": "chap2.html#transforming-data---summarising-aggregating-filtering-mutating",
    "title": "2  Importing files and manipulating data",
    "section": "2.4 Transforming data - Summarising, aggregating filtering, mutating",
    "text": "2.4 Transforming data - Summarising, aggregating filtering, mutating\nThe next two sections will introduce you to some important functions: - tapply(), unique() and table(), in base R; - aggregate(), summarise(), mutate(), in dplyr, and; - pivot_longer(), pivot_wider() and separate(), in tidyr.\nWe’ll use the Veg data. This dataframe holds information on vegetation transects from a multi-year study in the US. Further information is available in Zuur, Elena N. Ieno, and Meesters (2009).\n接下来的两节将介绍一些重要的函数：- 基础 R 中的 tapply()、unique() 和 table()；- dplyr7 中的aggregate()、summarise() 和 mutate()；以及 - tidyr 中的gather()、spread() 和separate()。\n我们将使用 Veg 数据。该数据框包含来自美国一项多年研究的植被横断面信息。更多信息请访问 Zuur, Elena N. Ieno, and Meesters (2009)。\n\nVeg &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/Vegetation.csv\")\n\nRows: 58 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): TransectName\ndbl (23): Samples, Transect, Time, R, ROCK, LITTER, ML, BARESOIL, FallPrec, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe tapply() function creates summaries of variables (means, sd and so). Let’s say you’re interested in finding how the mean species richness (R in the data file) differs across transects 8 transects - a lot of effort. The unique() function shows you how many unique levels a factor has:\ntapply() 函数会创建变量（均值、标准差等）的汇总。假设您想了解 8 条样带中物种丰富度平均值（数据文件中的 R）的差异，这需要付出很多努力。unique() 函数会显示某个因子有多少个唯一级别：\n\nunique(Veg$Transect)\n\n[1] 1 2 3 4 5 6 7 8\n\n\nFortunately, one function will do that for you:\n\ntapply(Veg$R, Veg$Transect, mean)       \n\n        1         2         3         4         5         6         7         8 \n 7.571429  6.142857 10.375000  9.250000 12.375000 11.500000 10.500000 11.833333 \n\n\nThis is useful when you are looking to use functions on subsets of a variable conditional on another variable (generally a factor - which does the grouping). You can use any arithmetic function sd, median, var (variance) and so on.\n\nMe &lt;- tapply(Veg$R, Veg$Transect, mean)    # Calculates the mean\nSd &lt;- tapply(Veg$R, Veg$Transect, sd)      # Calculates the sd\nLe &lt;- tapply(Veg$R, Veg$Transect, length)  # Calculates the number of observations (length)\nDescriptors &lt;- cbind(Me, Sd, Le)\nDescriptors\n\n         Me        Sd Le\n1  7.571429 1.3972763  7\n2  6.142857 0.8997354  7\n3 10.375000 3.5831949  8\n4  9.250000 2.3145502  8\n5 12.375000 2.1339099  8\n6 11.500000 2.2677868  8\n7 10.500000 3.1464265  6\n8 11.833333 2.7141604  6\n\n\n\n2.4.1 dplyr - some key functions\nThis task is much easier using another package dplyr() (from Wickham’s tidyverse package). The code is simpler and package super powerful. Key functions in this package are:\n\nselect() - allows you to select columns in a dataframe for further manipulation;\nslice() - allows you to select rows in a dataframe;\nfilter() - subsetting or removing observations based on some condition;\ngroup_by() - creates “sub-tables” or groups based on the unique values in one or more columns.;\nsummarise() - provides summary statistics for a group of observations (e.g. mean, SD etc);\narrange() - orders dataframes by observations (it’s a sorting function);\njoin - allows dataframes to be joined using unique attribute IDs;\nmutate() - create new columns on the basis of calculations and transforms of other columns.\n\n使用另一个包 dplyr()（来自 Wickham 的 tidyverse 包）可以更轻松地完成此任务。该包的代码更简洁，功能强大。此包中的关键函数包括：\nselect() - 允许您选择数据框中的列以进行进一步操作；\nslice() - 允许您选择数据框中的行；\nfilter() - 根据某些条件对观测值进行子集设置或移除；\ngroup_by() - 根据一列或多列中的唯一值创建“子表”或组；\nsummarise() - 为一组观测值提供汇总统计信息（例如平均值、标准差等）；\narrange() - 按观测值对数据框进行排序（这是一个排序函数）；\njoin - 允许使用唯一属性 ID 连接数据框；\nmutate() - 根据其他列的计算和变换创建新列。\nYou will need to read a little to support your learning here. Have a look at chapters 3, 4 in Wickham, Cetinkaya-Rundel, and Grolemund (2023) and try some of his exercises. It all helps to fix the functions in your mind. Remember you are learning a language. The function calls are effectively verbs (they do things) and they have a context for usage and varying modes of use, i.e. a suite of function arguments. Practice is essential. Let’s look at each function in turn.\n你需要阅读一些内容来支持你的学习。可以看看@Wickham2023的第三章和第四章，并尝试一些他的练习。这些都有助于巩固你对函数的理解。记住，你正在学习一门语言。函数调用实际上是动词（它们会执行操作），它们有使用上下文和不同的使用模式，也就是一组函数参数。练习至关重要。让我们依次看看每个函数。\n\n2.4.1.1 select\nSelect() offers a way of subsetting dfs by selecting the variables or columns you want for further analysis. Take our Veg data. If we glimpse()it we can see that it has lots of columns (or variables).\nSelect() 提供了一种通过选择需要进一步分析的变量或列来对 dfs 进行子集化的方法。以 Veg 数据为例。如果我们使用 glimpse() 函数，就会发现它包含很多列（或变量）。\n\nglimpse(Veg)\n\nRows: 58\nColumns: 24\n$ TransectName &lt;chr&gt; \"A_22_58\", \"A_22_62\", \"A_22_67\", \"A_22_74\", \"A_22_81\", \"A…\n$ Samples      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ Transect     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, …\n$ Time         &lt;dbl&gt; 1958, 1962, 1967, 1974, 1981, 1994, 2002, 1958, 1962, 196…\n$ R            &lt;dbl&gt; 8, 6, 8, 8, 10, 7, 6, 5, 8, 6, 6, 6, 6, 6, 7, 10, 8, 18, …\n$ ROCK         &lt;dbl&gt; 27.0, 26.0, 30.0, 18.0, 23.0, 26.0, 39.0, 25.0, 24.0, 21.…\n$ LITTER       &lt;dbl&gt; 30.0, 20.0, 24.0, 35.0, 22.0, 26.0, 19.0, 26.0, 24.0, 16.…\n$ ML           &lt;dbl&gt; 0, 0, 0, 0, 4, 0, 4, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, …\n$ BARESOIL     &lt;dbl&gt; 26.0, 28.0, 30.0, 16.0, 9.0, 23.0, 19.0, 33.0, 29.0, 41.0…\n$ FallPrec     &lt;dbl&gt; 30.22, 99.56, 43.43, 54.86, 24.38, 10.16, 34.29, 30.22, 9…\n$ SprPrec      &lt;dbl&gt; 75.43, 56.13, 65.02, 58.67, 87.63, 57.15, 31.49, 75.43, 5…\n$ SumPrec      &lt;dbl&gt; 125.47, 94.99, 112.26, 70.35, 81.78, 95.25, 62.99, 125.47…\n$ WinPrec      &lt;dbl&gt; 39.62, 107.44, 76.70, 90.67, 45.97, 60.70, 43.68, 39.62, …\n$ FallTmax     &lt;dbl&gt; 16.96, 14.56, 18.44, 17.16, 18.49, 17.39, 19.29, 16.98, 1…\n$ SprTmax      &lt;dbl&gt; 15.77, 15.21, 12.76, 14.00, 14.33, 16.91, 13.86, 15.79, 1…\n$ SumTmax      &lt;dbl&gt; 25.17, 24.85, 25.51, 26.67, 26.02, 26.78, 26.27, 25.19, 2…\n$ WinTmax      &lt;dbl&gt; 3.47, 1.16, 3.09, 2.46, 5.72, 3.64, 2.54, 3.49, 1.18, 3.1…\n$ FallTmin     &lt;dbl&gt; 0.49, -0.18, 1.23, 1.43, 1.09, 0.28, 1.86, 0.51, -0.16, 1…\n$ SprTmin      &lt;dbl&gt; 0.36, 0.18, -1.86, -0.53, 0.75, 0.64, -1.37, 0.38, 0.19, …\n$ SumTmin      &lt;dbl&gt; 6.97, 6.40, 7.12, 7.20, 6.90, 6.94, 6.96, 6.99, 6.41, 7.1…\n$ WinTmin      &lt;dbl&gt; -8.54, -10.76, -8.50, -8.28, -7.56, -9.21, -9.61, -8.52, …\n$ PCTSAND      &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 20, 20, 20, 20, 20, 20, 20, 1…\n$ PCTSILT      &lt;dbl&gt; 30, 30, 30, 30, 30, 30, 30, 34, 34, 34, 34, 34, 34, 34, 4…\n$ PCTOrgC      &lt;dbl&gt; 0.03459, 0.03459, 0.03459, 0.03459, 0.03459, 0.03459, 0.0…\n\n\nIt has 24 to be exact. We might only require a few of these to do an analysis. Let’s suppose we want to know how climatic factors, specifically precipitation, affect species richness (this the column called ‘R’ in the data). We can create a new df called ‘Veg1’ by selection the variables we need from Veg.\n确切地说，它有 24 个。我们可能只需要其中几个就可以进行分析。假设我们想知道气候因素，特别是降水量，如何影响物种丰富度（即数据中称为“R”的列）。我们可以从 Veg 中选择我们需要的变量，创建一个名为“Veg1”的新 df。\n\nVeg1 &lt;- Veg %&gt;% select(\"R\", \"FallPrec\",\"SprPrec\",\"SumPrec\",\"WinPrec\") # Here we are selection by columun names\n\nNotice we now have a new df called Veg1 in our environment and only had 5 columns. Obviously, typing lots of names if you want all the climate variables is a bit of a faff. No problem, you can select them using column numbers as we found out with our subsetting work in Week 1. We keep overwriting the dfs with the same name so we are no filling up the Environment with lots of redundant dfs.\n注意，我们现在在环境中添加了一个名为 Veg1 的新 df，它只有 5 列。显然，如果要包含所有气候变量，输入大量名称会比较麻烦。没问题，你可以使用列号来选择它们，就像我们在第一周的子集工作中发现的那样。我们不断覆盖同名的 df，这样就不会在环境中填充大量冗余的 df。\n\nVeg1 &lt;- Veg %&gt;% select(5, 10:21) # this selects column 5 (R) and then all the climate variables\n\nWe don’t need the [row,column] format because select() only allows you to select columns (or variables) from the df. To select rows, you need to use slice().\n我们不需要 [row,column] 格式，因为 select() 只允许从 df 中选择列（或变量）。要选择行，需要使用 slice()。\n\n\n2.4.1.2 slice\nIt is not often that we select particular rows from datasets and if needed to we’d likely go for some kind of filter that is conditional on values in particular columns (see below), or might want order the rows in a particular column in descending order to sample the top 100. But you might want the first 5, or last 10, or even a random sample of 20. This use additional variants of the slice, namely the functions, slice_tail() and slice_sample().\n我们并不经常从数据集中选择特定的行，如果需要，我们可能会采用某种以特定列中的值为条件的过滤器（见下文），或者可能希望按降序排列特定列中的行以抽取前 100 个。但您可能需要前 5 个，或后 10 个，甚至是随机抽取 20 个。这使用了切片的其他变体，即函数 slice_tail() 和 slice_sample()。\n\nVeg %&gt;% slice(1:5)           # first 5 rows\n\n# A tibble: 5 × 24\n  TransectName Samples Transect  Time     R  ROCK LITTER    ML BARESOIL FallPrec\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A_22_58            1        1  1958     8    27     30     0       26     30.2\n2 A_22_62            2        1  1962     6    26     20     0       28     99.6\n3 A_22_67            3        1  1967     8    30     24     0       30     43.4\n4 A_22_74            4        1  1974     8    18     35     0       16     54.9\n5 A_22_81            5        1  1981    10    23     22     4        9     24.4\n# ℹ 14 more variables: SprPrec &lt;dbl&gt;, SumPrec &lt;dbl&gt;, WinPrec &lt;dbl&gt;,\n#   FallTmax &lt;dbl&gt;, SprTmax &lt;dbl&gt;, SumTmax &lt;dbl&gt;, WinTmax &lt;dbl&gt;,\n#   FallTmin &lt;dbl&gt;, SprTmin &lt;dbl&gt;, SumTmin &lt;dbl&gt;, WinTmin &lt;dbl&gt;, PCTSAND &lt;dbl&gt;,\n#   PCTSILT &lt;dbl&gt;, PCTOrgC &lt;dbl&gt;\n\nVeg %&gt;% slice_tail(n = 10)   # last 3 rows\n\n# A tibble: 10 × 24\n   TransectName Samples Transect  Time     R  ROCK LITTER    ML BARESOIL\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 K_11_74           49        7  1974    14    11   23       0        0\n 2 K_11_81           50        7  1981     9     6   40       0        0\n 3 K_11_94           51        7  1994     5     0   14.5     0       35\n 4 K_11_02           52        7  2002    12    15   21       7        7\n 5 L_11_62           53        8  1962     9    25   24       2       20\n 6 L_11_67           54        8  1967    10    23   15       4       20\n 7 L_11_74           55        8  1974    16    14   15       0       14\n 8 L_11_81           56        8  1981    12    11   18       0        4\n 9 L_11_94           57        8  1994    10     8   29       0       27\n10 L_11_02           58        8  2002    14    13   26       0       13\n# ℹ 15 more variables: FallPrec &lt;dbl&gt;, SprPrec &lt;dbl&gt;, SumPrec &lt;dbl&gt;,\n#   WinPrec &lt;dbl&gt;, FallTmax &lt;dbl&gt;, SprTmax &lt;dbl&gt;, SumTmax &lt;dbl&gt;, WinTmax &lt;dbl&gt;,\n#   FallTmin &lt;dbl&gt;, SprTmin &lt;dbl&gt;, SumTmin &lt;dbl&gt;, WinTmin &lt;dbl&gt;, PCTSAND &lt;dbl&gt;,\n#   PCTSILT &lt;dbl&gt;, PCTOrgC &lt;dbl&gt;\n\nVeg %&gt;% slice_sample(n = 20) # 20 random rows\n\n# A tibble: 20 × 24\n   TransectName Samples Transect  Time     R  ROCK LITTER    ML BARESOIL\n   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 I_12_58           39        6  1958     9     5   32       0       30\n 2 K_11_94           51        7  1994     5     0   14.5     0       35\n 3 I_12_81           43        6  1981    10     3   23       0        9\n 4 G_12_02           38        5  2002    12     8   29       7        5\n 5 L_11_62           53        8  1962     9    25   24       2       20\n 6 C_22_67           10        2  1967     6    21   16       1       41\n 7 C_22_74           11        2  1974     6    18   25       0       33\n 8 A_22_02            7        1  2002     6    39   19       4       19\n 9 C_22_81           12        2  1981     6    19   28       0       14\n10 A_22_94            6        1  1994     7    26   26       0       23\n11 F_11_74           26        4  1974    12    41   12       0        7\n12 F_11_58           23        4  1958     8    53   10       0       20\n13 A_22_62            2        1  1962     6    26   20       0       28\n14 G_12_67           33        5  1967     9     4   51       0       13\n15 A_22_58            1        1  1958     8    27   30       0       26\n16 K_11_74           49        7  1974    14    11   23       0        0\n17 I_12_67           41        6  1967    14     5   29       3       12\n18 D_12_58           15        3  1958     7    56   17       0       16\n19 L_11_94           57        8  1994    10     8   29       0       27\n20 L_11_67           54        8  1967    10    23   15       4       20\n# ℹ 15 more variables: FallPrec &lt;dbl&gt;, SprPrec &lt;dbl&gt;, SumPrec &lt;dbl&gt;,\n#   WinPrec &lt;dbl&gt;, FallTmax &lt;dbl&gt;, SprTmax &lt;dbl&gt;, SumTmax &lt;dbl&gt;, WinTmax &lt;dbl&gt;,\n#   FallTmin &lt;dbl&gt;, SprTmin &lt;dbl&gt;, SumTmin &lt;dbl&gt;, WinTmin &lt;dbl&gt;, PCTSAND &lt;dbl&gt;,\n#   PCTSILT &lt;dbl&gt;, PCTOrgC &lt;dbl&gt;\n\n\n\n\n2.4.1.3 filter\nFiltering allows you to subset data. You’ve already done this the hard way already last session. dplyr() makes it easier and quicker. It works on rows using some sort of search criterion.\n过滤功能允许您对数据进行子集化。您在上节课中已经用很困难的方法完成了这项操作。dplyr() 可以使其更简单、更快捷。它使用某种搜索条件对行进行操作。\n\nVeg %&gt;% filter(Transect == 1:2) # displays all the records in transects 1 and 2\n\n# A tibble: 8 × 24\n  TransectName Samples Transect  Time     R  ROCK LITTER    ML BARESOIL FallPrec\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A_22_58            1        1  1958     8    27     30     0       26     30.2\n2 A_22_67            3        1  1967     8    30     24     0       30     43.4\n3 A_22_81            5        1  1981    10    23     22     4        9     24.4\n4 A_22_02            7        1  2002     6    39     19     4       19     34.3\n5 C_22_58            8        2  1958     5    25     26     0       33     30.2\n6 C_22_67           10        2  1967     6    21     16     1       41     43.4\n7 C_22_81           12        2  1981     6    19     28     0       14     24.4\n8 C_22_02           14        2  2002     6    26     18     2       42     34.3\n# ℹ 14 more variables: SprPrec &lt;dbl&gt;, SumPrec &lt;dbl&gt;, WinPrec &lt;dbl&gt;,\n#   FallTmax &lt;dbl&gt;, SprTmax &lt;dbl&gt;, SumTmax &lt;dbl&gt;, WinTmax &lt;dbl&gt;,\n#   FallTmin &lt;dbl&gt;, SprTmin &lt;dbl&gt;, SumTmin &lt;dbl&gt;, WinTmin &lt;dbl&gt;, PCTSAND &lt;dbl&gt;,\n#   PCTSILT &lt;dbl&gt;, PCTOrgC &lt;dbl&gt;\n\n\n\n\n2.4.1.4 summarise\nHere we are just summarising the descriptive statistics for one column/variable (called FallPrec) in the Veg df. We have the mean, standard deviation, variance and length (or ’n”, the number of data points).\n这里我们只是总结了 Veg df 中一列/变量（称为 FallPrec）的描述性统计数据。我们有平均值、标准差、方差和长度（或“n”，即数据点的数量）。\n\nVeg %&gt;% summarise(mean=mean(FallPrec), sd=sd(FallPrec), var=var(FallPrec), length=length(FallPrec))\n\n# A tibble: 1 × 4\n   mean    sd   var length\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n1  55.9  38.5 1485.     58\n\n\n\n\n2.4.1.5 mutate\nMutate and create a new dataframe (sleep1) with a new column showing the proportion of rapid eye movement (REM) sleep for each species (rem_proportion) and the body weight transformed into grams (bodywt_grams).\n变异并创建一个新的数据框（sleep1），其中有一个新列显示每个物种的快速眼动（REM）睡眠比例（rem_proportion）和转换为克的体重（bodywt_grams）。\n\nsleep1 &lt;- Sleep %&gt;% \n  mutate(rem_proportion = sleep_rem / sleep_total, \n         bodywt_grams = bodywt * 1000) \n\nLook at your new dataframe with the additional ‘new’ column.\n\nsleep1\n\n# A tibble: 83 × 13\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Cheet… Acin… carni Carn… lc                  12.1      NA        NA      11.9\n 2 Owl m… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n 3 Mount… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n 4 Great… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n 5 Cow    Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n 6 Three… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n 7 North… Call… carni Carn… vu                   8.7       1.4       0.383  15.3\n 8 Vespe… Calo… &lt;NA&gt;  Rode… &lt;NA&gt;                 7        NA        NA      17  \n 9 Dog    Canis carni Carn… domesticated        10.1       2.9       0.333  13.9\n10 Roe d… Capr… herbi Arti… lc                   3        NA        NA      21  \n# ℹ 73 more rows\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;,\n#   bodywt_grams &lt;dbl&gt;\n\n\n\n\n2.4.1.6 join\nThe join functions allow you to combine dataframes using a column with rows of unique identifiers (or keys) that is common to both files. You can use one primary key or two or more if the key requires more than one variable; a compound key.\ndplyr has six join functions: left_join(), inner_join(), right_join(), full_join(), semi_join(), and anti_join(). They all take a pair of data frames (x and y) and return one data frame. The order of the rows and columns in the output is primarily determined by x. We’ll focus on the most commonly used one: left_join() and illustrate using it with the fish data we imported earlier in the session. If we look at that dataframe…\n连接函数允许您使用包含两个文件共有的唯一标识符（或键）行的列来组合数据框。您可以使用一个主键，如果键需要多个变量，则可以使用两个或多个主键；复合键。\ndplyr 有六个连接函数：left_join()、inner_join()、right_join()、full_join()、semi_join() 和 anti_join()。它们都接受一对数据框（x 和 y）并返回一个数据框。输出中行和列的顺序主要由 x 决定。我们将重点介绍最常用的函数：left_join()，并演示如何将其与我们之前在课程中导入的鱼类数据结合使用。如果我们看一下该数据框……\n\nglimpse(fish)\n\nRows: 30\nColumns: 30\n$ Site_no   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ CHA       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 3, 2, 1, 1, 0, 0, …\n$ TRU       &lt;dbl&gt; 3, 5, 5, 4, 2, 3, 5, 0, 0, 1, 3, 5, 5, 5, 4, 3, 2, 1, 0, 0, …\n$ VAI       &lt;dbl&gt; 0, 4, 5, 5, 3, 4, 4, 0, 1, 4, 4, 4, 5, 5, 4, 3, 4, 3, 3, 1, …\n$ LOC       &lt;dbl&gt; 0, 3, 5, 5, 2, 5, 5, 0, 3, 4, 1, 4, 2, 4, 5, 5, 4, 3, 5, 2, …\n$ OMB       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 2, 0, 1, 1, 0, 0, …\n$ BLA       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 2, 1, 1, 0, …\n$ HOT       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, …\n$ TOX       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 3, 2, …\n$ VAN       &lt;dbl&gt; 0, 0, 0, 0, 5, 1, 1, 0, 0, 2, 0, 0, 0, 0, 3, 5, 3, 2, 2, 2, …\n$ CHE       &lt;dbl&gt; 0, 0, 0, 1, 2, 2, 1, 0, 5, 2, 1, 1, 0, 1, 3, 2, 2, 3, 1, 3, …\n$ BAR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 3, 2, 4, …\n$ SPI       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 3, 2, 3, …\n$ GOU       &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 1, 2, 4, 4, …\n$ BRO       &lt;dbl&gt; 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 2, …\n$ PER       &lt;dbl&gt; 0, 0, 0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 1, 2, …\n$ BOU       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, …\n$ PSO       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, …\n$ ROT       &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ CAR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, …\n$ TAN       &lt;dbl&gt; 0, 0, 0, 1, 0, 2, 0, 4, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 4, …\n$ BCO       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ PCH       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ GRE       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, …\n$ GAR       &lt;dbl&gt; 0, 0, 0, 0, 0, NA, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 2, 2, 5, 5,…\n$ BBO       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ ABL       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 5, …\n$ ANG       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, …\n$ fish_abun &lt;dbl&gt; 3, 12, 16, 21, 20, 20, 16, 8, 14, 14, 11, 18, 19, 28, 33, 40…\n$ fish_SR   &lt;dbl&gt; 1, 3, 4, 8, 8, 9, 5, 4, 5, 6, 6, 6, 6, 10, 11, 17, 22, 23, 2…\n\n\n…we see it has 30 rows and 30 columns. The variable listed is Site_no. The is the unique identifier or key. If you looked at the fish.xlsx spreadsheet in Excel you would have noticed other sheets, one of them is called environmmental variables. It is sheet 3 in the spreadsheet. The Site_no variable is also in that sheet. First need to load the sheet into R and then combine them using left_join().\n我们看到它有 30 行 30 列。列出的变量是 Site_no。它是唯一标识符或键。如果您查看了 Excel 中的 fish.xlsx 电子表格，您会注意到其他工作表，其中一个名为“环境变量”。它是电子表格中的工作表 3。Site_no 变量也在该工作表中。首先需要将工作表加载到 R 中，然后使用 left_join() 将它们合并。\n\n#load in the file\nenv &lt;- read_xlsx(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/fish.xlsx\", sheet=3)\n# now join the fish and env using the common key, Site_no.\nfish_env &lt;- fish %&gt;% \n  left_join(env, join_by(Site_no))\n\nLook at the combined dataframe. What you have done is combine the two dataframes adding the environmental data from each site to the fish species data in to a new dataframe called fish_env.\n看看合并后的数据框。你所做的就是将两个数据框合并起来，将每个站点的环境数据和鱼类物种数据添加到一个名为 fish_env 的新数据框中。\n\nstr(fish_env)\n\ntibble [30 × 43] (S3: tbl_df/tbl/data.frame)\n $ Site_no  : num [1:30] 1 2 3 4 5 6 7 8 9 10 ...\n $ CHA      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ TRU      : num [1:30] 3 5 5 4 2 3 5 0 0 1 ...\n $ VAI      : num [1:30] 0 4 5 5 3 4 4 0 1 4 ...\n $ LOC      : num [1:30] 0 3 5 5 2 5 5 0 3 4 ...\n $ OMB      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ BLA      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ HOT      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ TOX      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ VAN      : num [1:30] 0 0 0 0 5 1 1 0 0 2 ...\n $ CHE      : num [1:30] 0 0 0 1 2 2 1 0 5 2 ...\n $ BAR      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ SPI      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ GOU      : num [1:30] 0 0 0 1 1 1 0 0 0 1 ...\n $ BRO      : num [1:30] 0 0 1 2 0 1 0 0 0 0 ...\n $ PER      : num [1:30] 0 0 0 2 3 1 0 0 0 0 ...\n $ BOU      : num [1:30] 0 0 0 0 0 0 0 2 0 0 ...\n $ PSO      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ ROT      : num [1:30] 0 0 0 0 2 0 0 1 0 0 ...\n $ CAR      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ TAN      : num [1:30] 0 0 0 1 0 2 0 4 1 0 ...\n $ BCO      : num [1:30] 0 0 0 0 0 0 0 1 0 0 ...\n $ PCH      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ GRE      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ GAR      : num [1:30] 0 0 0 0 0 NA 0 0 4 0 ...\n $ BBO      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ ABL      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ ANG      : num [1:30] 0 0 0 0 0 0 0 0 0 0 ...\n $ fish_abun: num [1:30] 3 12 16 21 20 20 16 8 14 14 ...\n $ fish_SR  : num [1:30] 1 3 4 8 8 9 5 4 5 6 ...\n $ das      : num [1:30] 0.3 2.2 10.2 18.5 21.5 32.4 36.8 49.1 70.5 99 ...\n $ alt      : num [1:30] 934 932 914 854 849 846 841 792 752 617 ...\n $ pen      : num [1:30] 48 3 3.7 3.2 2.3 3.2 6.6 2.5 1.2 9.9 ...\n $ deb      : num [1:30] 0.84 1 1.8 2.53 2.64 2.86 4 1.3 4.8 10 ...\n $ pH       : num [1:30] 7.9 8 8.3 8 8.1 7.9 8.1 8.1 8 7.7 ...\n $ dur      : num [1:30] 45 40 52 72 84 60 88 94 90 82 ...\n $ pho      : num [1:30] 0.01 0.02 0.05 0.1 0.38 0.2 0.07 0.2 0.3 0.06 ...\n $ nit      : num [1:30] 0.2 0.2 0.22 0.21 0.52 0.15 0.15 0.41 0.82 0.75 ...\n $ amm      : num [1:30] 0 0.1 0.05 0 0.2 0 0 0.12 0.12 0.01 ...\n $ oxy      : num [1:30] 12.2 10.3 10.5 11 8 10.2 11.1 7 7.2 10 ...\n $ dbo      : num [1:30] 2.7 1.9 3.5 1.3 6.2 5.3 2.2 8.1 5.2 4.3 ...\n $ x        : num [1:30] 88 94 102 100 106 112 114 110 136 168 ...\n $ y        : num [1:30] 7 14 18 28 39 51 61 76 100 112 ...\n\n\n\n\n2.4.1.7 Combining functions\nUsing combinations of these commands you can pull out, summarise and analyses subsets of data from large data files very quickly and effectively. Here are a couple of examples.\nLet’s replicate the tapply() function we did earlier to generate the mean, sd, and var (variance) in the species richness variable (R) in the Veg df. We are using the %&gt;% piping operate to sequence the functions. Veg is our df, we group_by() numbers in the Transect columns then summarise the mean, sd and var for each transect.\n结合使用这些命令，您可以快速有效地从大型数据文件中提取、汇总和分析数据子集。以下是几个示例。\n让我们复制之前执行的 tapply() 函数，以生成 Veg 数据表中物种丰富度变量 (R) 的平均值、标准差和方差 (var)。我们使用 %&gt;% 管道运算符对函数进行排序。Veg 是我们的 df，我们对 Transect 列中的数字进行 group_by()，然后汇总每个横断面的平均值、标准差和方差。\n\nVeg %&gt;% group_by(Transect) %&gt;% summarise(mean=mean(R), sd=sd(R), var=var(R))\n\n# A tibble: 8 × 4\n  Transect  mean    sd    var\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1        1  7.57 1.40   1.95 \n2        2  6.14 0.900  0.810\n3        3 10.4  3.58  12.8  \n4        4  9.25 2.31   5.36 \n5        5 12.4  2.13   4.55 \n6        6 11.5  2.27   5.14 \n7        7 10.5  3.15   9.9  \n8        8 11.8  2.71   7.37 \n\n\nAnother example might be arranging variables by some value and selection to top 20 for further analysis. Do this you’d use arrange() in combination with slice(). We arrange by bodywt in descending heaviest to lightest (using the ‘-’ character), and then sample the top 10 species. Alternative slice functions exist in dplyr (see below).\n另一个例子可能是按某个值对变量进行排序，然后选取前 20 个进行进一步分析。为此，您需要结合使用 accordion() 和 slice()。我们按体重从重到轻的降序排列（使用“-”符号），然后对前 10 个物种进行采样。dplyr 中提供了其他切片函数（见下文）。\n\nSleep %&gt;% arrange(-bodywt) %&gt;% slice(1:10) # rows 1 to 10. OR:\n\n# A tibble: 10 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Afric… Loxo… herbi Prob… vu                   3.3      NA        NA      20.7\n 2 Asian… Elep… herbi Prob… en                   3.9      NA        NA      20.1\n 3 Giraf… Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n 4 Pilot… Glob… carni Ceta… cd                   2.7       0.1      NA      21.4\n 5 Cow    Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n 6 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 7 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n 8 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 9 Bottl… Turs… carni Ceta… &lt;NA&gt;                 5.2      NA        NA      18.8\n10 Tiger  Pant… carni Carn… en                  15.8      NA        NA       8.2\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\nSleep %&gt;% arrange(-bodywt) %&gt;% slice_head(n=10) # uses a different dplyr function slice_head()\n\n# A tibble: 10 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Afric… Loxo… herbi Prob… vu                   3.3      NA        NA      20.7\n 2 Asian… Elep… herbi Prob… en                   3.9      NA        NA      20.1\n 3 Giraf… Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n 4 Pilot… Glob… carni Ceta… cd                   2.7       0.1      NA      21.4\n 5 Cow    Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n 6 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 7 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n 8 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 9 Bottl… Turs… carni Ceta… &lt;NA&gt;                 5.2      NA        NA      18.8\n10 Tiger  Pant… carni Carn… en                  15.8      NA        NA       8.2\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\n\n\n\n2.4.2 What is tidy data?\nTidy data follows three core attributes (Wickham 2014):\n\nEach variable → has its own column;\nEach observation → has its own row;\nEach value → has its own cell.\n\n整洁数据遵循三个核心属性 (Wickham 2014)：\n每个变量 → 都有自己的列；\n每个观测值 → 都有自己的行；\n每个值 → 都有自己的单元格。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#manipulating-and-tidying-data-using-tidyr",
    "href": "chap2.html#manipulating-and-tidying-data-using-tidyr",
    "title": "2  Importing files and manipulating data",
    "section": "2.5 Manipulating and tidying data using tidyr",
    "text": "2.5 Manipulating and tidying data using tidyr\nKey functions in this library are:\n\npivot_longer() – a way to reshape data from wide to long, keeping column names as key variables;\npivot_wider() – a way to reshape data from long to wide, creating new columns from key variables;\nseparate() – split one column into multiple columns based on a delimiter or character position;\nunite() – combine multiple columns into one by pasting their values together with a separator.\n\nWe’ll generate some data that simulates messy data and then transform it to tidy data.\n该库中的关键函数包括：\npivot_longer() – 将数据从宽列转换为长列，并将列名保留为键变量；\npivot_wider() – 将数据从长列转换为宽列，并通过键变量创建新列；\nseparate() – 根据分隔符或字符位置将一列拆分为多列；\nunite() – 将多列的值用分隔符粘贴在一起，合并为一列。\n我们将生成一些模拟杂乱数据的数据，然后将其转换为整洁数据。\n\n# Create some messy data (wide-style, species split into columns)\nSite   &lt;- c(\"Site1\", \"Site2\", \"Site3\")\nSample &lt;- c(\"S1\", \"S2\", \"S3\")\nPlot   &lt;- c(\"P1\", \"P2\", \"P3\")\nSp1    &lt;- c(123, 533, NA)\nSp2    &lt;- c(156, 45, 45)\nSp3    &lt;- c(243, 4, 345)\n\nmessy_df &lt;- data.frame(Site, Sample, Plot, Sp1, Sp2, Sp3)\nmessy_df\n\n   Site Sample Plot Sp1 Sp2 Sp3\n1 Site1     S1   P1 123 156 243\n2 Site2     S2   P2 533  45   4\n3 Site3     S3   P3  NA  45 345\n\n\nYou can see here that the species are column names i.e. the data is in a wide format, which has its uses, but a lot of plotting packages (e.g. ggplot2) and analytical functions (e.g. aov, or lm) in R need data organised differently. ie. in tidy (or long) format. We achieve using pivot_longer().\n您可以看到，物种是列名，即数据采用宽格式，这有其用途，但 R 中的许多绘图包（例如 ggplot2）和分析函数（例如 aov 或 lm）需要以不同的方式组织数据，即采用整洁（或长）格式。我们使用pivot_longer()来实现。\n\n# Tidy (long) data\ntidy_df &lt;- messy_df %&gt;%\n  pivot_longer(cols = starts_with(\"Sp\"),\n               names_to = \"Species\", # new column \n               values_to = \"Count\")  # another new column\ntidy_df\n\n# A tibble: 9 × 5\n  Site  Sample Plot  Species Count\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Site1 S1     P1    Sp1       123\n2 Site1 S1     P1    Sp2       156\n3 Site1 S1     P1    Sp3       243\n4 Site2 S2     P2    Sp1       533\n5 Site2 S2     P2    Sp2        45\n6 Site2 S2     P2    Sp3         4\n7 Site3 S3     P3    Sp1        NA\n8 Site3 S3     P3    Sp2        45\n9 Site3 S3     P3    Sp3       345\n\n\nYou see here we have moved the species counts from the cells in each former species variables them a new column called ‘Count’ and put the former species names (sp1,sp2, and sp3 i.e. the column names) into rows within a new column (or variable) called 'Species'. If you want to transform it back to wide you can usepivot_wider().\n可以看到，我们将物种数量从每个先前物种变量的单元格中移到了一个名为“Count”的新列中，并将先前的物种名称（sp1、sp2 和 sp3，即列名）放入名为“Species”的新列（或变量）中的行中。如果要将其转换回宽表，可以使用pivot_wider()。\n\n2.5.0.1 An example of tidying a more complex dataset\nWe will use a partial datafile on stream temperatures to illustrate this process. Load in the datafile.\n\nstreamT&lt;-read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/streamT.csv\") \n\nRows: 244 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (29): Distance, Depth, 15/06/2015 16:35, 15/06/2015 16:37, 15/06/2015 16...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nColumn 1 is distance downstream in a river (in metres), Column 2 is depth in the stream bed (again in m) and Columns 3:30 are dates and times of temperatures readings with T measured in (C).\nHave a look at these data - identify the issues.\n第 1 列表示河流下游距离（以米为单位），第 2 列表示河床深度（同样以米为单位），第 3 列 30 列表示温度读数的日期和时间，单位为摄氏度 (C)。\n查看这些数据 - 找出问题所在。\n\nglimpse(streamT) \n\nRows: 244\nColumns: 29\n$ Distance           &lt;dbl&gt; 42.325, 42.579, 42.833, 43.087, 43.341, 43.595, 43.…\n$ Depth              &lt;dbl&gt; 0.000000000, 0.004044586, 0.008089172, 0.012133758,…\n$ `15/06/2015 16:35` &lt;dbl&gt; 15.504, 15.380, 15.514, 15.387, 15.365, 15.385, 15.…\n$ `15/06/2015 16:37` &lt;dbl&gt; 15.817, 15.577, 15.632, 15.505, 15.442, 15.367, 15.…\n$ `15/06/2015 16:39` &lt;dbl&gt; 15.785, 15.498, 15.485, 15.507, 15.451, 15.435, 15.…\n$ `15/06/2015 16:41` &lt;dbl&gt; 15.758, 15.582, 15.542, 15.387, 15.329, 15.510, 15.…\n$ `15/06/2015 16:43` &lt;dbl&gt; 15.671, 15.404, 15.429, 15.451, 15.458, 15.434, 15.…\n$ `15/06/2015 16:45` &lt;dbl&gt; 15.569, 15.530, 15.469, 15.417, 15.514, 15.436, 15.…\n$ `15/06/2015 16:47` &lt;dbl&gt; 15.583, 15.425, 15.358, 15.410, 15.441, 15.470, 15.…\n$ `15/06/2015 16:49` &lt;dbl&gt; 15.482, 15.349, 15.345, 15.419, 15.490, 15.531, 15.…\n$ `15/06/2015 16:52` &lt;dbl&gt; 15.556, 15.520, 15.484, 15.399, 15.331, 15.419, 15.…\n$ `15/06/2015 16:54` &lt;dbl&gt; 15.749, 15.528, 15.428, 15.455, 15.398, 15.431, 15.…\n$ `15/06/2015 16:56` &lt;dbl&gt; 15.727, 15.504, 15.552, 15.416, 15.417, 15.472, 15.…\n$ `15/06/2015 16:58` &lt;dbl&gt; 15.783, 15.599, 15.592, 15.572, 15.531, 15.466, 15.…\n$ `15/06/2015 17:00` &lt;dbl&gt; 15.540, 15.412, 15.428, 15.420, 15.390, 15.347, 15.…\n$ `15/06/2015 17:02` &lt;dbl&gt; 15.793, 15.482, 15.504, 15.563, 15.453, 15.466, 15.…\n$ `15/06/2015 17:04` &lt;dbl&gt; 15.830, 15.455, 15.510, 15.603, 15.468, 15.463, 15.…\n$ `15/06/2015 17:06` &lt;dbl&gt; 15.893, 15.598, 15.685, 15.672, 15.518, 15.558, 15.…\n$ `15/06/2015 17:08` &lt;dbl&gt; 15.696, 15.426, 15.483, 15.480, 15.446, 15.394, 15.…\n$ `15/06/2015 17:10` &lt;dbl&gt; 15.908, 15.522, 15.407, 15.473, 15.491, 15.574, 15.…\n$ `15/06/2015 17:12` &lt;dbl&gt; 15.862, 15.616, 15.521, 15.556, 15.480, 15.435, 15.…\n$ `15/06/2015 17:14` &lt;dbl&gt; 15.904, 15.489, 15.547, 15.535, 15.444, 15.549, 15.…\n$ `15/06/2015 17:16` &lt;dbl&gt; 15.852, 15.521, 15.537, 15.498, 15.441, 15.469, 15.…\n$ `15/06/2015 17:18` &lt;dbl&gt; 15.789, 15.577, 15.549, 15.584, 15.499, 15.442, 15.…\n$ `15/06/2015 17:20` &lt;dbl&gt; 15.643, 15.360, 15.364, 15.311, 15.423, 15.481, 15.…\n$ `15/06/2015 17:23` &lt;dbl&gt; 15.730, 15.558, 15.611, 15.494, 15.526, 15.420, 15.…\n$ `15/06/2015 17:25` &lt;dbl&gt; 15.719, 15.623, 15.507, 15.599, 15.615, 15.640, 15.…\n$ `15/06/2015 17:27` &lt;dbl&gt; 15.639, 15.393, 15.377, 15.449, 15.395, 15.458, 15.…\n$ `15/06/2015 17:29` &lt;dbl&gt; 15.699, 15.526, 15.570, 15.641, 15.622, 15.522, 15.…\n\n\nSeveral of the columns are names are dates not numeric variables Stream temperature is hidden under the date columns making it difficult to analyse. We are going to reformat the file so date is in one column with the relevant temperature reading per distance and depth in another two columns. To do this we use pivot_longer().\n一些列是名称，而不是数字变量。溪流温度隐藏在日期列下，这使其难以分析。我们将重新格式化文件，使日期位于一列，而每个距离和深度对应的温度读数位于另外两列。为此，我们使用pivot_longer()。\n\nstreamTTidy &lt;- streamT %&gt;%  # selects our df\n  pivot_longer(cols = 3:29, # finds the columns to pivot (i.e. they are all dates)\n               names_to = \"DateT\", # writes the column names into the rows\n               values_to = \"Temp\") # pulls the temperature data from the cells and puts in the rows linked to the date when the measurement was taken.\n\nThe code pools the dates from columns 3:29 and places the former column headers into a DateT column and the temperatures into a Temp column. The columns we don’t mention, Depth and Distance of left untouched.\nThe remaining issue is that the DateT column has two variables date and time. We can resolve this using the separate() function.\n代码将第 3:29 列的日期合并到一起，并将之前的列标题放入 DateT 列，将温度放入 Temp 列。我们未提及的 Depth 和 Distance 列保持不变。\n剩下的问题是 DateT 列包含两个变量 date 和 time。我们可以使用 separate() 函数解决这个问题。\n\nStreamTTidier &lt;- streamTTidy %&gt;%\n  separate(DateT, into = c(\"Date\", \"Time\"), sep = \"\\\\ \") %&gt;%\n  separate(Date, into = c(\"Day\", \"Month\", \"Year\"), sep = \"\\\\/\") %&gt;%\n  separate(Time, into = c(\"Hour\", \"Min\"), sep = \"\\\\:\")\n\nLine 1 of this code takes the DateT column and pulls out Date from time using the separator \\\\ . This is a space. Line 2 separates Date into Day, Month and Year using the separate \\\\/ Line 3 separates Time into Hours and Minutes using the separator \\\\: Check help file for more information on separators. But in short R is expecting the character that separates the fields to be given immediately after the \\\\ You now have a tidy datafile to start analysis temperature change by distance downstream, depth into the stream bed and across time each day and or seasonally.\nWe could have done it one larger code element using the piping operators to gather then separate.\n这段代码的第 1 行获取 DateT 列，并使用分隔符  从时间中提取日期。 是一个空格。第 2 行使用分隔符 / 将日期分隔为日、月和年。第 3 行使用分隔符 : 将时间分隔为小时和分钟。有关分隔符的更多信息，请参阅帮助文件。简而言之，R 期望在  之后立即提供分隔字段的字符。现在，您有了一个整洁的数据文件，可以开始分析温度随下游距离、河床深度以及每天或每个季节随时间的变化。\n我们可以使用管道运算符将其合并成一个更大的代码元素，然后进行收集和分离。\n\nStreamTTidier2 &lt;- streamT %&gt;% gather(DateT, Temp, 3:29) %&gt;% \n  separate(DateT, into = c(\"Date\", \"Time\"), sep = \"\\\\ \") %&gt;%\n  separate(Date, into = c(\"Day\", \"Month\", \"Year\"), sep = \"\\\\/\") %&gt;%\n  separate(Time, into = c(\"Hour\", \"Min\"), sep = \"\\\\:\")\n\n\n\n2.5.1 Working with dates and times\nAll we did above with our dates/times was chop into smaller elements to separate out the months, days and times. We could do this because it was stored as a character. But R can store dates and times as a unique data class. Dates are stored in R as an numeric, which is effectively the mumber of days since 1st January 1070. Excel does the same. Times are location specific, depending on your time zone.\nBoth can be manipulated using the lubridate package - examples are available in the code in the Appendix. lubridate has a run of functions to manipulate dates and times directly. DOY() (Day of Year) and DOM() (Day of Month) functions. It is also covered well in chapter 17 of Wickham, Cetinkaya-Rundel, and Grolemund (2023). You will need to wrestle with these functions if you have to analyse time series data on say, river flows, or fish mortality data on days with extreme heat events, for example.\n上面我们对日期/时间所做的就是将其分割成更小的元素，以区分月份、日期和时间。我们可以这样做，因为它是以字符形式存储的。但 R 可以将日期和时间存储为一个独特的数据类。日期在 R 中存储为数值型数据，实际上是自 1070 年 1 月 1 日以来的天数。Excel 也是如此。时间是特定于位置的，取决于您的时区。\n两者都可以使用 lubridate 包进行操作——示例可在附录的代码中找到。lubridate 包含一系列可以直接操作日期和时间的函数。DOY()（年份）和 DOM()（月份）函数。Wickham, Cetinkaya-Rundel, and Grolemund (2023) 的第 17 章也对此进行了详细介绍。如果您需要分析时间序列数据，例如河流流量或极端高温事件发生时鱼类死亡率数据，则需要掌握这些函数。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#class-exercises",
    "href": "chap2.html#class-exercises",
    "title": "2  Importing files and manipulating data",
    "section": "2.6 Class Exercises",
    "text": "2.6 Class Exercises\n\n2.6.1 CLASS EXERCISE 1: Using base R\nUse the deep sea research data (ISIT.txt).\n\nLoad it (it is tab delineated), examine its structure with your favoured functions. The file contains bioluminscence data on organisms from various depths and locations in the North Sea.\nExtract the data from station 1. How many observations are there from this station?\nWhat are minimum, maximum, median and mean sampled depth of stations 2 and 3.\nIdentify stations with fewer observations than 20. Create a data frame omitting them.\nExtract the data for 2002 and sort it by increasing depth values.\nShow the data that were measured at depths greater than 2000m in April (all years).\n\n使用深海研究数据 (ISIT.txt)。\n加载它（以制表符分隔），并使用您常用的函数检查其结构。该文件包含北海不同深度和位置的生物发光数据。\n从站点 1 提取数据。该站点有多少个观测点？\n站点 2 和 3 的最小、最大、中值和平均采样深度是多少。\n找出观测点少于 20 个的站点。创建一个数据框，将其忽略。\n提取 2002 年的数据，并按深度值递增排序。\n显示 4 月份（所有年份）在深度超过 2000 米处测得的数据。\n\n\n2.6.2 CLASS EXERCISE 2: Using tidyverse functions\nRepeat each of the above using tidyverse functions.\n使用 tidyverse 函数重复上述每个操作。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#next-week",
    "href": "chap2.html#next-week",
    "title": "2  Importing files and manipulating data",
    "section": "2.7 Next Week",
    "text": "2.7 Next Week\nWe will focus our attention on Exploratory Data Analysis (EDA) and graphical tools we can use to display data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#follow-up-work",
    "href": "chap2.html#follow-up-work",
    "title": "2  Importing files and manipulating data",
    "section": "2.8 Follow-up work",
    "text": "2.8 Follow-up work\nThree things to do:\n\nPlease attempt to read at least some of the material listed above but certainly this one (Wickham 2014)! and the blog below.\nRead Brian McGill’s blog on the 10 commandments for good data data management.. There is a lot of code that you might could also work through. I have downloaded the datasets for you if you wish to try. These are the Raw Data files. They follow a constellation schema per Commandment #3 (if you have read the blog). Some info on the data:\n\nraptor_survey.csv/df is the fact table\nraptor_temps.csv/temps is a 2nd environmental fact table\nraptor_sites.csv/siteinfo is a dimension table (site) for both fact tables\n\n\nYou know the commands to read them in.\n有两件事要做：\n请尝试至少阅读上面列出的部分材料，但一定要阅读这篇(Wickham 2014)！以及下面的博客。\n阅读 Brian McGill 关于良好数据管理十诫的博客。其中有很多代码你或许也能理解。如果你想尝试，我已经为你下载了数据集。这些是原始数据文件。它们遵循第三诫的星座图（如果你读过这篇博客的话）。以下是一些关于数据的信息：\nraptor_survey.csv/df 是事实表\nraptor_temps.csv/temps 是第二个环境事实表\nraptor_sites.csv/siteinfo 是两个事实表的维度表（站点）。\n\nRead chapter 4 in (Beckerman, Childs, and Petchey 2017).\n\n你已经知道读取它们的命令了。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap2.html#references",
    "href": "chap2.html#references",
    "title": "2  Importing files and manipulating data",
    "section": "2.9 References",
    "text": "2.9 References\n\n\n\n\nBeckerman, Andrew, Dylan Childs, and Owen Petchey. 2017. Getting Started with R: An Introduction for Biologists. 2nd. ed. Oxford: Oxford University Press.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. “Skimr: Compact and Flexible Summaries of Data.”\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (September): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. “Readxl: Read Excel Files: R Package Version 1.4.3.”\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd Ed. O’Reilly.\n\n\nZuur, Alain F., Elena N. Ieno, and Erik Meesters. 2009. A Beginner’s Guide to R (Use R!). New York: Springer.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Importing files and manipulating data</span>"
    ]
  },
  {
    "objectID": "chap3.html",
    "href": "chap3.html",
    "title": "3  Data exploration strategies using R",
    "section": "",
    "text": "3.0.1 Learning Outcomes\nThis week we are going to make a start on data visualisation and exploration. To do this we will use graphical techniques that visualize and transform to explore data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. Zuur, Ieno, and Elphick (2010) provides an excellent outline of EDA strategies. Please read the paper!\nData exploration is an important and integral part of data analysis. The process is iterative and evaluative and may take several cycles until you in a position to finalise your analytical route (Fig. 3.1). If you read around this you will find that people suggest it is appropriate to spend ~ 50% of your analytical time engaged in exploring your datasets to identify (sometimes hidden) issues.\n本周我们将开始数据可视化和探索。为此，我们将使用可视化和转换的图形技术来系统地探索数据，统计学家称之为探索性数据分析，简称EDA。\n数据探索是数据分析的重要组成部分。这个过程是迭代和评估性的，可能需要几个周期才能最终确定分析路线（图3.1）。如果你仔细阅读这篇文章，你会发现人们建议将约50%的分析时间用于探索数据集以识别（有时是隐藏的）问题是合适的。\nLearning how to do this properly is long process, so today we will start with the basics which we will extend and reinforce in subsequent workshops. The aim is to use the ggplot2 package to do this work and we will use a range of different types of graphs to illustrate how to represent data structure and the relationship between columns of data in our dataframes. We will introduce you to elements of base R graphics as the workshops progress.\n这是一个漫长的过程，所以我们将从基础知识入手，并在接下来的工作坊中不断扩展和巩固。我们的目标是使用 ggplot2 包来完成这项工作，并使用各种不同类型的图表来说明如何表示数据结构以及数据框中各列之间的关系。随着工作坊的进展，我们将向您介绍 R 图形的基本元素。\nStatisticians (and all researchers by association if they use statistics) use visualization and transformation to explore their data in a systematic way. EDA is an iterative cycle where you:\n统计学家（以及所有从事统计学研究的研究人员）会运用可视化和数据转换技术，系统地探索数据。统计学家称之为探索性数据分析，简称EDA。EDA是一个迭代循环，在这个过程中，您可以：\nSession Aim 会议目标 To understand the importance of data visualisation for data analysis. 了解数据可视化对于数据分析的重要性。\nAt the end of this workshop your will be able to use the ggplot2 package for data visualisation with a range of types of graphics to examine your data in terms of:\nMaxim of the day “Don’t start data analysis without exploring the data (graphically) first”\n在本研讨会结束时，您将能够使用ggplot2数据可视化软件包，并结合多种类型的图形来分析您的数据，具体如下：\n每日箴言“在开始数据分析之前，务必先（以图形方式）探索数据”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#thinking-about-visualisation",
    "href": "chap3.html#thinking-about-visualisation",
    "title": "3  Data exploration strategies using R",
    "section": "3.1 Thinking about visualisation",
    "text": "3.1 Thinking about visualisation\nBefore we proceed it is worth thinking about why we are producing the graphics and what makes a plot work well. Obviously, we use the packages to show the data, its structure and dependencies within and between the data items. In terms of thinking about how to develop the graphics / plots we need to:\n\nUse graphic tools that help in the comparison of differences in the data that we believe are important;\nEnsure our plots represent the magnitudes and data dependencies honestly and accurately;\nDraw graphical elements clearly, minimizing clutter and redundant information;\nMake the graphics easy to interpret via appropriate colour choices, scales, insets and so on.\n\n在继续之前，我们有必要思考一下我们为什么要制作这些图表，以及如何才能让图表效果更好。显然，我们使用这些包来展示数据、数据结构以及数据项内部和之间的依赖关系。在思考如何开发图表/图表方面，我们需要.\n\n使用图形工具来帮助比较我们认为重要的数据差​​异；\n确保我们的图表真实准确地反映量级和数据依赖关系；\n清晰地绘制图形元素，最大限度地减少混乱和冗余信息；\n通过适当的颜色、比例、插图等，使图表易于理解。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#load-packages",
    "href": "chap3.html#load-packages",
    "title": "3  Data exploration strategies using R",
    "section": "3.2 Load packages",
    "text": "3.2 Load packages\n\n# List of packages\npackages &lt;- c(\"skimr\",\"readxl\",\"tidyverse\",\"janitor\")\n\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n[[1]]\n[1] \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[7] \"methods\"   \"base\"     \n\n[[2]]\n[1] \"readxl\"    \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n [7] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"readxl\"    \"skimr\"    \n[13] \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"  \n[19] \"base\"     \n\n[[4]]\n [1] \"janitor\"   \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"    \n [7] \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"readxl\"   \n[13] \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[19] \"methods\"   \"base\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#load-data",
    "href": "chap3.html#load-data",
    "title": "3  Data exploration strategies using R",
    "section": "3.3 Load data",
    "text": "3.3 Load data\nWe are going to use a run of different data sources to visualise today so we’ll load each one at the point we need it.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#an-introduction-to-ggplot2-from-the-tidyverse-package",
    "href": "chap3.html#an-introduction-to-ggplot2-from-the-tidyverse-package",
    "title": "3  Data exploration strategies using R",
    "section": "3.4 An introduction to ggplot2 (from the tidyverse package)",
    "text": "3.4 An introduction to ggplot2 (from the tidyverse package)\nWe are going to use ggplot2 for all our plotting in this module unless it is more difficult to do so (i.e. GIS type functions). As I mentioned R was created to generate excellent graphic so the functions in base R to this are extensive. My decision to use ggplot2 is base on its clear graphical structure or as Wickham et al. might say, its grammar of graphics. There is a whole book dedicated to this H. Wickham et al. (2009). The third updated edition is freely available here should you wish to read it: https://ggplot2-book.org/.\nIt works by layering up components as shown below in Figure 3.2. This is a screenshot from Hadley Wickham, Cetinkaya-Rundel, and Grolemund (2023) i.e. the book list in the URL above. The text describes the elements better than I could. 除非比较困难（例如使用 GIS 类型的函数），否则我们将在本模块中使用 ggplot2 进行所有绘图。正如我所提到的，R 语言旨在生成出色的图形，因此 R 语言中拥有丰富的函数。我决定使用 ggplot2 是基于其清晰的图形结构，或者用 Wickham 等人的话来说，它的图形语法。有一本书专门介绍 ggplot2：\nWickham, H., Gentleman, R., Hornick, K. & Parmigiani, G. (2009) ggplot2：用于数据分析的优雅图形。Springer 出版社，纽约。\n如果您想阅读，可以在此处免费获取第三版更新版本：https://ggplot2-book.org/\n它的工作原理是将组件分层，如下图 2 所示。这是 Wickham 等人 (2024) 著作的屏幕截图，即上方 URL 中的书籍列表。文本对这些元素的描述比我能理解的要好。\nVisually, in terms of the code structure, your command will looks like this (Fig. 3.3): 从代码结构上看，您的命令将如下所示（图 3.2）：\n\n\n\n\n\n\n\n\nFigure 3.1: Fig. 3.2: ggplot layering of elements\n\n\n\n\n\n\n\n\n\n\nFig. 3.4: The code syntax for a ggplot call)\n\n\n\n\nReading from left to write. All ggplots begin with the function call (in green), and we pull in the data with the data= argument. Next you layer the geom function. This is the argument that selects the type of graphics (e.g. boxplot, histogram etc), and finally you map the aesthetics, aes() (i.e. the X and Y elements). There are other layer arguments for adding titles, captions, varying axes etc. We’ll keep it simple to start with and expand the range of arguments as you become more experienced.\n从左到右依次阅读。所有 ggplot 都以函数调用（绿色）开始，然后使用 data= 参数导入数据。接下来，对 geom_function 进行分层。该参数用于选择图形类型（例如箱线图、直方图等），最后绘制图形的视觉效果（即 X 和 Y 元素）。还有其他分层参数可用于添加标题、说明、不同的轴等。我们将从简单的开始，随着您经验的积累，逐步扩展参数的范围。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#your-first-plot",
    "href": "chap3.html#your-first-plot",
    "title": "3  Data exploration strategies using R",
    "section": "3.5 Your first plot",
    "text": "3.5 Your first plot\nWe are going to create this plot. It is plot designed to show the variability in discharge metrics (mean daily flow, yearly discharge range) and river catchment size. What patterns does the plot show? Spend a few minutes looking at it.\n我们将创建此图。该图旨在显示流量指标（平均日流量、年流量变化范围）和河流集水区大小的变化。该图显示了哪些模式？请花几分钟时间查看。\n\n\n\n\n\nFigure 3.4: River discharge variability in relation to catchment size\n\n\n\n\nWe’ll start by creating a plot layer by layer so you can visualise what’s happening. We will make a complex graphic but you don’t need to understand all the details to start with.\n我们将从逐层创建图表开始，以便您直观地了解正在发生的事情。我们会制作一个复杂的图形，但您无需了解所有细节即可开始。\nWe’ll use some hydrological data. You need to load it. This is my directory structure, your paths will be different.\n我们将使用一些水文数据。您需要加载它。这是我的目录结构，您的路径可能会有所不同。\n\n# Load data\ndata &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/Riv_hydrol.csv\")\n\nRows: 23 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Code, River, Ym\ndbl (9): Rivcode, Carea, Mean, Med, Max, SD, Ran, min, Xm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s look at it’s structure. It is river flow (discharge) data from rivers with different catchment sizes. We use the skim function to do this.\n我们来看看它的结构。它是来自不同流域大小的河流的流量（排放）数据。我们使用 skim 函数来执行此操作。\n\nskim(data)\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n23\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nCode\n0\n1\n5\n10\n0\n23\n0\n\n\nRiver\n0\n1\n3\n8\n0\n23\n0\n\n\nYm\n0\n1\n4\n6\n0\n23\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nRivcode\n0\n1\n15.70\n8.81\n1.00\n9.00\n17.00\n22.50\n29.00\n▇▃▅▇▇\n\n\nCarea\n0\n1\n362.73\n360.14\n54.90\n100.45\n207.00\n504.00\n1282.10\n▇▂▁▁▁\n\n\nMean\n0\n1\n10.01\n12.47\n1.29\n2.21\n4.09\n11.31\n47.32\n▇▁▂▁▁\n\n\nMed\n0\n1\n5.64\n7.68\n0.55\n1.32\n2.83\n5.80\n32.97\n▇▁▁▁▁\n\n\nMax\n0\n1\n99.10\n133.19\n8.27\n19.37\n51.53\n127.70\n589.80\n▇▂▁▁▁\n\n\nSD\n0\n1\n12.55\n15.58\n1.14\n2.53\n5.41\n15.68\n57.09\n▇▁▂▁▁\n\n\nRan\n0\n1\n98.06\n132.31\n8.09\n18.95\n50.48\n127.00\n586.51\n▇▂▁▁▁\n\n\nmin\n0\n1\n1.03\n1.16\n0.05\n0.42\n0.58\n1.09\n5.00\n▇▂▁▁▁\n\n\nXm\n0\n1\n45584.87\n88158.95\n2016.00\n2890.00\n4071.00\n31229.00\n271115.00\n▇▁▁▁▁\n\n\n\n\n\nHere is the first layer of the plot (Fig. 3.5). We have just added the data to ggplot so it just prints a canvas to receive it!\n这是绘图的第一层（图 3.5）。我们刚刚将数据添加到 ggplot 中，因此它只需打印一个画布来接收数据即可！\n\n# create plot blank canvas\nggplot(data = data) # when run it we get a grey box in plot window (bottom left)\n\n\n\n\n\n\n\n\nFig. 3.5: The ggplot canvas.\nThen we need some axes. These are mapped with the aesthetics argument, shortened to aes. x is the yearly range in discharges meansure in cumecs. y is the mean of daily discharge across the year (Fig. 3.6).\n\nggplot(\n  data = data,\n  mapping = aes(x = Ran, y = Mean)\n)\n\n\n\n\n\n\n\n\nFig. 3.6: The axes are now in place. Notice it just returns the column name as a label. We can, of course, modify the axes labels.\nNext we need to add the data to the plot. We doe this by specifying the geom. Because the data are continuous and we are looking to compare one column against another, we need a scatterplot of some form. We use the geom_point() function to plot the dots (Fig. 3.7). We can change the type of dot, dot colour, dot size and fill type to improve the visual aesthetic in due course.\n接下来，我们需要将数据添加到图中。我们通过指定“geom”来实现这一点。由于数据是连续的，并且我们希望比较各列之间的差异，因此我们需要绘制某种形式的散点图。我们使用“geom_point()”函数来绘制点（图 3.7）。我们可以更改点的类型、颜色、大小和填充类型，以提升视觉美感。\n\nggplot(data, aes(x = Ran, y = Mean)) +\n  geom_point(\n  )\n\n\n\n\n\n\n\n\nFig. 3.7: Adding some data points to the plot.\nYou will recall out figure has effectively three dimensions, mean flow, the range of flow and we want to show catchment size influences. We do this by adding that data column to the aes function by specifying the size argument (Fig. 3.8).\n你会记得，我们的图实际上有三个维度：平均流量、流量范围，以及我们想要展示的流域规模的影响。我们通过指定“size”参数，将该数据列添加到“aes”函数中来实现这一点（图 3.8）。\n\nggplot(data, aes(x = Ran, y = Mean, size = Carea)) + \n  geom_point(\n  )\n\n\n\n\n\n\n\n\nFig. 3.8: Adding catchment size to the plot.\nNotice the points are now scaled to represent the catchment size for each river. ggplot adds a legend to show the scale by default. We can modify this too. We now have our basic plot. We could stop here if we are just looking to visualise things but if we plan to use the figure in a report or paper we might want to make it look a little more professional. We’ll start by adding some titles, enhanced labels and a caption with some nice textual features, i.e. superscripts and so on.\n请注意，这些点现在已按比例缩放，以表示每条河流的流域大小。“ggplot”添加了一个图例，默认显示比例。我们也可以对其进行修改。现在，我们得到了基本的图表。如果我们只是想将事物可视化，可以就此打住；但如果我们计划在报告或论文中使用该图表，我们可能需要使其看起来更专业一些。我们将首先添加一些标题、增强标签和带有一些美观文本特征（例如上标等）的说明文字。\n\nggplot(data, aes(x = Ran, y = Mean, size = Carea)) +\n  geom_point(\n  ) +\n  \n# Set the axis labels here for clarity\n  scale_x_continuous(name = expression(paste(\"Yearly Discharge Range \", \"(\", m^3, \"/\",s^-1,\")\"))) +\n  scale_y_continuous(name = expression(paste(\"Mean Daily Discharge \", \"(\", m^3, \"/\",s^-1,\")\"))) +\n  \n# Add the labels, title, subtitle and caption. Note expression(paste) functions to add the superscript\n  labs(\n    title = \"River Discharge Increases with Catchment Area\", # main title\n    subtitle = \"A comparison of mean daily discharge across various rivers\", # subtitle\n    size = expression(paste(\"Catchment Area \", \"(\",km^2,\")\")), # this sorts out the caption title\n    caption = \"Data points represent individual rivers. Bubble size is proportional to catchment area.\"  # caption \n  )  \n\n\n\n\n\n\n\n\nFig. 3.9: Some titles, tidier labels and a caption.\nNow let’s sort out the data points. You can see the points overlap and sit on top of each other. We can deal with that by changing the parameters that control, colour and opacity (Fig. 3.10).\n现在让我们整理一下数据点。您可以看到这些点重叠在一起。我们可以通过更改控制参数（颜色和不透明度）来解决这个问题（图 3.10）。\n\nggplot(data, aes(x = Ran, y = Mean, size = Carea)) +\n  geom_point(\n    shape = 21, # shape 21 is a circle but we are going to remove the default theme so we specify it.\n    colour = \"grey30\", # change the outline of the points to light grey \n    fill = \"#4682B4\", # change the point colour to light blue\n    alpha = 0.7 # Modify the opacity to make it more translucent\n  ) +\n# Set the axis labels here for clarity\n  scale_x_continuous(name = expression(paste(\"Yearly Discharge Range \", \"(\", m^3, \"/\",s^-1,\")\"))) +\n  scale_y_continuous(name = expression(paste(\"Mean Daily Discharge \", \"(\", m^3, \"/\",s^-1,\")\"))) +\n  \n# Add the labels, title, subtitle and caption. Note expression(paste) functions to add the superscript\n  labs(\n    title = \"River Discharge Increases with Catchment Area\", # main title\n    subtitle = \"A comparison of mean daily discharge across various rivers\", # subtitle\n    size = expression(paste(\"Catchment Area \", \"(\",km^2,\")\")), # this sorts out the caption title\n    caption = \"Data points represent individual rivers. Bubble size is proportional to catchment area.\"  # caption \n  )  \n\n\n\n\n\n\n\n\nFig. 3.10: Points are coloured and the opacity is lowered so point overlaps are visible.\nTo conclude and clean the visualisation, we are going to change the margins of the plot, set the limits, stop the clipping, remove the default grey background, place the legend across the top and set a base font size. It’s a lot of functions but don’t worry about holding on to all of it now. We’ll revisit things every week to reinforce the learning.\n为了完成并清理可视化效果，我们将更改绘图的边距、设置界限、停止裁剪、移除默认的灰色背景、将图例放置在顶部并设置基本字体大小。函数比较多，但不必担心现在就记住所有函数。我们每周都会复习一遍，以巩固所学内容。\n\nggplot(data, aes(x = Ran, y = Mean, size = Carea)) +\n  geom_point(\n    shape = 21, # shape 21 is a circle but we are going to remove the default theme so we specify it.\n    colour = \"grey10\", # change the outline of the points to light grey \n    fill = \"#4682B4\", # change the point colour to light blue\n    alpha = 0.7 # Modify the opacity to make it more translucent\n  ) +\n  scale_size_area(max_size = 12) +\n  \n  # Set the axis labels\n  scale_x_continuous(name = expression(paste(\"Yearly Discharge Range \", \"(\", m^3, \"/\",s^-1,\")\"))) +\n  scale_y_continuous(name = expression(paste(\"Mean Daily Discharge \", \"(\", m^3, \"/\",s^-1,\")\"))) +\n \n  # Add the labels. Note expression(paste) functions to add the superscript\n  labs(\n    title = \"River Discharge Increases with Catchment Area\",\n    subtitle = \"A comparison of mean daily discharge across various rivers\",\n    size = expression(paste(\"Catchment Area \", \"(\",km^2,\")\")),\n    caption = \"Data points represent individual rivers. Bubble size is proportional to catchment area.\"\n  ) +\n  \n  # Use coord_cartesian to set the viewport, start axes at 0, and turn off clipping.\n  coord_cartesian(\n    xlim = c(0, 600), # sets X axis limits\n    ylim = c(0, 60), # sets Y axis limits\n    expand = FALSE,  # This creates the tight axis look, starting at 0\n    clip = \"off\"     # This allows points to be drawn outside the panel\n  ) +\n  \n  theme_bw(base_size = 14) + # change the theme from default to black and white. Set base font size to 14\n  theme(\n    panel.grid.minor = element_blank(), #remove the minor grids\n    panel.border = element_blank(), # remove the plot border\n    axis.line = element_line(colour = \"grey50\"), #Change axis line colour to grey\n    plot.title.position = \"plot\", # aligns title to far left margin of the full plot. Change 'plot' to 'panel' to see the difference.\n    plot.caption.position = \"plot\", # as above\n    plot.caption = element_text(hjust = 0, color = \"grey60\"), # aligns the caption to the far left (hjust = 0) i.e. no left padding. Sets text colour to mid grey.\n    legend.position = \"top\", # place the legend on the top from the right hand size\n    # Add a little margin to the whole plot to ensure there's room\n    plot.margin = margin(10, 15, 10, 10) # Specify margins around the plot. Top, Right, Bottom, Left\n  )\n\n\n\n\n\n\n\n\nFig. 3.11: The final version.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#exploratory-data-analysis-eda",
    "href": "chap3.html#exploratory-data-analysis-eda",
    "title": "3  Data exploration strategies using R",
    "section": "3.6 Exploratory Data Analysis (EDA)",
    "text": "3.6 Exploratory Data Analysis (EDA)\n\n3.6.1 Displaying frequencies and distributions of variables\nWe need to use these to establish how our data are structured. This allows us to see whether data conform to to a normal distribution, which is an important criterion for the use of parametric techniques e.g. ANOVA, standard T-test and linear regression. We are going to use three plot types: histograms, Quantile-Quantile Plots (or Q-Q plots) and boxplots. The data are air pollution data and the file is called airpollution.csv. It is in your ‘data’ folder; add ‘your’ path the your ‘data’ folder into the code to locate the file.\n我们需要用它们来确定数据的结构。这使我们能够判断数据是否符合正态分布，这是使用参数化技术（例如方差分析、标准T检验和线性回归）的重要标准。我们将使用三种绘图类型：直方图、分位数-分位数图（或Q-Q图）和箱线图。数据是空气污染数据，文件名为airpollution.csv。它位于您的“data”文件夹中；在代码中添加“您的”路径和“data”文件夹即可找到该文件。\n\n#load file\nair &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/airpollution.csv\")\n\nRows: 336 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): Nitrogen Oxides, Respirable Particles\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHave a quick look at it.\n\nskim(air)\n\n\nData summary\n\n\nName\nair\n\n\nNumber of rows\n336\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nNitrogen Oxides\n4\n0.99\n187.96\n81.90\n50.9\n125.65\n164.65\n231.60\n587.5\n▇▇▃▁▁\n\n\nRespirable Particles\n6\n0.98\n32.05\n10.33\n12.2\n24.90\n30.05\n37.22\n77.9\n▅▇▃▁▁\n\n\n\n\n\nIt has 336 rows and 2 columns: one is NOx levels and the other (respiratory particles) is the number of PM10 particles. Notice that the column names have spaces in them. We need to sort that out. We could just rename them using colnames() but we’ll use the janitor package to do it for us with the function clean_names(). Note also there are missing values in the file, this has implications for visualisation using ggplot.\n它有 336 行和 2 列：一列表示氮氧化物 (NOx) 水平，另一列（呼吸性颗粒物）表示 PM10 颗粒物的数量。请注意，列名中有空格。我们需要解决这个问题。我们可以使用 colnames() 重命名它们，但我们将使用 janitor 包中的 clean_names() 函数来执行此操作。另请注意，文件中存在缺失值，这会对使用 ggplot 进行可视化产生影响。\n\n# sort out space is column names\nair &lt;- air %&gt;% clean_names()\n\nLook at the new names\n\nnames(air)\n\n[1] \"nitrogen_oxides\"      \"respirable_particles\"\n\n\n\n3.6.1.1 Histograms\nHistograms show how data is distributed by grouping values into intervals (bins) and displaying the frequency (count) of observations in each bin as bars. It helps visualize the shape, spread, and central tendency of a dataset. The code for a ggplot histogram uses the geom_histogram() function (Fig. 3.12). I will provide the base R code too, for reference (Fig. 3.13). See we are subsetting the data to remove the NAs or missing values with !is.na(). ggplot does not deal with NAs. We set the bin width to 20. Have a ploy and change that value to see what happens..\n直方图通过将值分组到区间（bin）中，并以条形显示每个bin中观测值的频率（计数），从而显示数据的分布情况。它有助于可视化数据集的形状、分布和集中趋势。ggplot 直方图的代码使用了 geom_histogram() 函数（图 3.12）。我还将提供基础 R 代码以供参考（图 3.13）。请注意，我们使用 !is.na() 对数据进行子集化，以移除 NA 值或缺失值。ggplot 不处理 NA 值。我们将 bin 宽度设置为 20。想个办法，更改该值看看会发生什么。\n\nggplot(data=subset(air, !is.na(nitrogen_oxides)), aes(x=nitrogen_oxides)) + geom_histogram(binwidth=20)\n\n\n\n\n\n\n\n# set binwidth = 20\n# ggplot does not like NAs in the data for plots (see data file - there are 4 rows)\n# To avoid the error message we use: data=subset(Air, !is.an(Nitrogen.Oxides)) to subset the rows with numbers\n\nFig. 3.12: plot histogram.\nWe can modify ggplot code to produce a tider outcome (Fig. 3.13). ggplot requires a little more coding effort for simple plots but comes into its own for complex, publication-standard plots, so in my view it is worth the initial investment in effort.\n人们可以修改 ggplot 代码来产生更准确的结果（图 3.13）。ggplot 对于简单的图表需要更多的编码工作，但对于复杂的、出版标准的图表则能发挥它的作用，所以在我看来，它值得最初投入的努力。\n\n# Draw with black outline, grey fill\nggplot(data=subset(air, !is.na(nitrogen_oxides)),aes(x=nitrogen_oxides)) +\n  geom_histogram(binwidth=20, colour=\"black\", fill=\"grey90\") +\n  labs(x=\"Nitrogren Oxide levels\") + # add a label\ntheme_bw() + # change dafualt scheme to black and white....this removes axes and other things!\n theme(\n  panel.grid.minor = element_blank(), #remove the minor grids\n  panel.grid.major = element_blank(), #remove the minor grids\n  panel.border = element_blank(), # remove the plot border\n  axis.line = element_line(colour = \"black\") # Add axis lines back for a cleaner look\n )\n\n\n\n\n\n\n\n\nFig. 3.13: A tidier ggplot version.\nSo what does this mean? We can see a number of things. First, on the right we so a number of bin counts that may be ‘outliers’ (we’ll come back to that later). Second the data show a right or positive skew, so not normally distributed. A right skew means there is a tail of higher values to the right and lump sits to the left (see Fig. 3.14 for more information). The plot in the middle is a classic bell-shaped normal curve. Pay attention to what’s happening with the mean, median and mode!\n那么这意味着什么呢？我们可以看出一些情况。首先，右侧有一些可能是“异常值”的箱体计数（我们稍后会讨论这个问题）。其次，数据右或正倾斜，因此不太可能呈正态分布。右倾斜意味着右侧有较高值的​​尾部，而左侧则有块状（更多信息请参见图 3.14）。中间的图是一条典型的钟形正态曲线。注意平均值、中位数和众数的变化！\n\n\n\n\n\n\n\n\n\nFig. 3.14: Skewness patterns.\n\n\n\n3.6.2 Q-Q plots\nAnother way to visual distributions in to use a Q-Q plot. A Q-Q plot, short for “quantile-quantile” plot, is used to assess whether or not a set of data potentially came from some theoretical distribution. In most cases, this type of plot is used to determine whether or not a set of data follows a normal distribution.\nIf the data are normally distributed, the points in a Q-Q plot will lie on a straight diagonal line. Conversely, if the points deviate significantly from the straight diagonal line, then it’s less likely that the data is normally distributed. We know from out histogram that our NOx data are skewed so it is unlikely to be normally distributed. We address why this might be important in later classes so don’t worry about it now.\nIn ggplot the we use you can use the stat_qq() and stat_qq_line() functions as follows (Fig. 2.15).\n另一种可视化分布的方法是使用 Q-Q 图。Q-Q 图是“分位数-分位数”图的缩写，用于评估一组数据是否可能来自某种理论分布。在大多数情况下，这种类型的图用于确定一组数据是否遵循正态分布。\n如果数据服从正态分布，则 Q-Q 图中的数据点将位于一条直对角线上。相反，如果数据点与直对角线的偏差较大，则数据服从正态分布的可能性较小。从我们的直方图中可以看出，我们的 NOx 数据存在偏斜，因此不太可能服从正态分布。我们将在后续课程中解释为什么这一点可能很重要，所以现在不用担心。\n在我们使用的 ggplot 中，您可以使用 stat_qq() 和 stat_qq_line() 函数，如下所示（图 2.15）。\n\n# plot some qq-plots\nggplot(data=subset(air, !is.na(nitrogen_oxides)), aes(sample=nitrogen_oxides)) +\n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\nFig. 3.15: A Q-Q plot generated with ggplot.\nAs we suspected from the histogram the data are not normally distributed or the points would sit on the line. You may be wondering what a normally distributed dataset might look like on a Q-Q plot. So we will simulate one with R (Fig. 3.16). You do not need to understand the code that does this.\n正如我们从直方图中推测的那样，数据并非呈正态分布，否则点会位于直线上。您可能想知道正态分布的数据集在Q-Q图上会是什么样子。因此，我们将使用 R 进行模拟（图 3.16）。您无需理解执行此操作的代码。\n\n#make this example reproducible\nset.seed(1)\n\n#create some fake data that follows a normal distribution for 200 data points\ndf &lt;- data.frame(col=rnorm(200))\n\n#create Q-Q plot\nggplot(df, aes(sample=col)) +\n  stat_qq() + \n  stat_qq_line()\n\n\n\n\n\n\n\n\nFig. 3.16: A normally distributed pattern on a Q-Q plot.\nNotice there is a bit of drift on the extremes but most of the data points sit on the line. We’ll be seeing a lot of these as the module progresses because R’s automated validation plots use Q-Q plots extensively.\n请注意，极值处略有漂移，但大多数数据点都位于线上。随着模块的进展，我们会看到很多这样的数据点，因为 R 的自动验证图广泛使用 Q-Q 图。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#boxplots",
    "href": "chap3.html#boxplots",
    "title": "3  Data exploration strategies using R",
    "section": "3.7 Boxplots",
    "text": "3.7 Boxplots\nBoxplots (Fig. 3.17) are very useful for visualising the distribution of data. The black in the middle is the median (not the mean), the box around it shows the upper (3rd) quartile (75%) and the lower (1st) quartile (25%) and the whiskers or lines are the maxima and minima. The dots are potential outliers (Fig. 3.18). They may not be true outliers in the statistical sense. We’ll discover how to be sure of this in later workshops.\n箱线图（图 3.17）对于可视化数据分布非常有用。中间的黑色区域表示中位数（而非平均值），周围的框表示上四分位数（第三四分位数）（75%）和下四分位数（第一四分位数）（25%），晶须或线表示最大值和最小值。图中的点是潜在的异常值（图 3.18）。它们可能并非统计学意义上的真正异常值。我们将在后续的研讨会中探讨如何确定这一点。\n\n#create a boxplot\nggplot(air, aes(x = 1, y = nitrogen_oxides)) + # Note I left the !is.na() element out and ggplot gives you a prod telling you 4 four rows are being removed.\n  geom_boxplot() +\n  scale_x_continuous(breaks = NULL) # we need this new code otherwise R will add a 'fake' x axis!\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nFig. 3.17: Boxplot of air quality data. See the offset median and potential outlier points.\n\n\n\n\n\nFig. 3.18: Components of a boxplot\n\n\n\n\nFig. 3.18: The elements of a boxplot named.\nFor a normally distributed dataset the boxplot would be much more symetrical with the median central in the box and the whiskers balanced with side of it. You can see from the above there are more data points above the median, with some higher ones that sit above 1.5 times the interquartile range. For comparison, Figure 3.18 below is based on the data we simulated to fit a normal distribution.\n对于正态分布的数据集，箱线图会更加对称，中位数位于箱线图的中心，晶须与箱线图的边线保持平衡。从上图可以看出，中位数以上的数据点更多，其中一些数据点甚至高于四分位距的 1.5 倍。为了进行比较，下图 3.18 是基于我们模拟拟合正态分布的数据绘制的。\n\n#create a boxplot\nggplot(df, aes(x = 1, y = col)) + \n  geom_boxplot() +\n  scale_x_continuous(breaks = NULL) \n\n\n\n\n\n\n\n\nFig: 3.18: A normal distribution pattern in a boxplot.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#exploring-mulitple-variables",
    "href": "chap3.html#exploring-mulitple-variables",
    "title": "3  Data exploration strategies using R",
    "section": "3.8 Exploring mulitple variables",
    "text": "3.8 Exploring mulitple variables\nEDA comes into its own when the graphical techniques are used to explore multiple variables simultaneously. Type of plot used depends on the type of variable. There are lots of types of variables but to start with we will focus on: - continuous variables such as counts of organisms, weights of things, chemical parameters, temperature and so on. - factors or grouped variables, such as classes of age, quartiles of river flow, countries, months in a year and so on.\n当使用图形技术同时探索多个变量时，EDA 便能大显身手。所用的绘图类型取决于变量的类型。变量的类型有很多，但首先我们将重点关注：- “连续变量”，例如生物数量、物体重量、化学参数、温度等等。- “因子”或分组变量，例如年龄组、河流流量四分位数、国家/地区、一年中的月份等等。\n\n3.8.1 Comparing two continous variables\nTypically here were would be thinking of these in terms of a x and y comparison using a something like a scatterplot, this sort of thing underpins the ideas of linear regression, which we’ll cover in week 4 of this module (chapter 4 of this module book). You have seen a complex example of this in Fig. 3.11 - the hydrology data set above. To explore this we’ll use a data set based on benthic organisms captured on beaches in the Netherlands. The data are discussed in Zuur, Elena N. Ieno, and Meesters (2009).\n通常，我们会用类似散点图的方法来比较 x 和 y 轴，这类图是线性回归的基础，我们将在本模块的第四周（本模块手册的第 4 章）讲解。您已经在图 3.11 中看到了一个复杂的例子——上面的水文数据集。为了探索这一点，我们将使用一个基于在荷兰海滩捕获的底栖生物的数据集。这些数据在 Zuur, Elena N. Ieno, and Meesters (2009) 中进行了讨论。\n\nBenthic &lt;- read.table(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/RIKZ.txt\", header = TRUE) # It is tab delineated so we use read.table\n\nLets look at it.\n\nglimpse(Benthic)\n\nRows: 45\nColumns: 5\n$ Sample   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ Richness &lt;int&gt; 11, 10, 13, 11, 10, 8, 9, 8, 19, 17, 6, 1, 4, 3, 3, 1, 3, 3, …\n$ Exposure &lt;int&gt; 10, 10, 10, 10, 10, 8, 8, 8, 8, 8, 11, 11, 11, 11, 11, 11, 11…\n$ NAP      &lt;dbl&gt; 0.045, -1.036, -1.336, 0.616, -0.684, 1.190, 0.820, 0.635, 0.…\n$ Beach    &lt;int&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5…\n\nskim(Benthic)\n\n\nData summary\n\n\nName\nBenthic\n\n\nNumber of rows\n45\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSample\n0\n1\n23.00\n13.13\n1.00\n12.00\n23.00\n34.00\n45.00\n▇▇▇▇▇\n\n\nRichness\n0\n1\n5.69\n5.00\n0.00\n3.00\n4.00\n8.00\n22.00\n▇▃▂▁▁\n\n\nExposure\n0\n1\n10.22\n0.93\n8.00\n10.00\n10.00\n11.00\n11.00\n▂▁▁▇▇\n\n\nNAP\n0\n1\n0.35\n0.99\n-1.34\n-0.38\n0.17\n1.12\n2.26\n▅▇▅▅▃\n\n\nBeach\n0\n1\n5.00\n2.61\n1.00\n3.00\n5.00\n7.00\n9.00\n▇▇▃▇▇\n\n\n\n\n\nIt has 5 variables and 45 rows.\n\nSample. This is the sample number identifier. Its class is int, or integer.\nRichness. This is the number of benthic invertebrates counted in each sample. Its class is int, or integer and it is a continuous variable.\nExposure. This is….Its class is int, or integer. The repeated numbers suggest you could use it as a factor variable.\nNAP. This is…..Its class is, dbl or double/decimal. It is a continuous variable and ranges from -1.33 to 2.255 (see the output from skim above).\nBeach. This is….Its class is, int or integer. You can see the numbers are repeated and each repeated number is effectively a sample with a beach. We can figure out how many beaches there are by counting the number of unique values using the unique() function.\n\n它包含 5 个变量和 45 行。\n\n样本。这是样本编号标识符。其类型为 int，即整数。\n丰富度。这是每个样本中计数的底栖无脊椎动物的数量。其类型为 int，即整数，它是一个连续变量。\n暴露度。这是……其类型为 int，即整数。重复的数字表明您可以将其用作因子变量。\n午睡。这是……其类型为 dbl，即双精度/小数。它是一个连续变量，范围从 -1.33 到 2.255（参见上面 skim 的输出）。\n海滩。这是……其类型为 int，即整数。您可以看到这些数字是重复的，每个重复的数字实际上都是一个包含海滩的样本。我们可以通过使用 unique() 函数计算唯一值的数量来计算海滩的数量。\n\n\nunique(Benthic$Beach) # there are 9 beaches \n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nThere are nine beaches, each with 5 repeated samples, so 9 x 5 = 45 (the number of rows in the dataframe). We can use scatterplots to look at the relationship between our continuous variables. We might want to know how Richness is related to NAP. We will use the ggplot defaults because we are quickly assessing the data (Fig. 3.19).\n有九个海滩，每个海滩有 5 个重复样本，因此 9 x 5 = 45（数据框中的行数）。我们可以使用散点图来查看连续变量之间的关系。我们可能想知道丰富度与 NAP 之间的关系。我们将使用 ggplot 的默认值，因为我们需要快速评估数据（图 3.19）。\n\nggplot(Benthic, aes(x = NAP, y = Richness)) +\n  geom_point(\n  )\n\n\n\n\n\n\n\n\nFig. 3.19: Scatterplot showing the relationship between benthic invertebrates (Richness) and the NAP variable.\nWe can see two clear patterns on this plot: - Firstly, as NAP increases, species richness of beach invertebrates decreases. - Second, and more subtle, there is a decrease in the variability of benthic richness as NAP increase. See how the way the points pinch out along the x axis?\nIf we want to know a little more about the shape of this relationship we can add a regression or best fit line to the plot with by adding one more geom, namely geom_smooth(). We’ll jazz this plot up a little (Fig. 3.20).\n我们可以从这张图上看到两个清晰的模式：- 首先，随着 NAP 的增加，海滩无脊椎动物的物种丰富度下降。- 其次，更微妙的是，随着 NAP 的增加，底栖生物丰富度的变异性下降。看到点沿 x 轴收缩的方式了吗？\n如果我们想更深入地了解这种关系的形状，可以通过添加一个 geom（即 geom_smooth()）来为图添加回归线或最佳拟合线。我们将稍微美化一下这张图（图 3.20）。\n\nggplot(Benthic, aes(x = NAP, y = Richness)) +\n  geom_point(alpha = 0.6) + # Added some transparency to the points\n  geom_smooth(\n    method = \"lm\", \n    se = TRUE, # add in some confidence interval\n    color = \"red\",       # Change line color to red\n    linewidth = 1.2    # Make the line slightly thicker\n  ) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFig. 3.20: A linear regression line with confidence intervals on the scatterplot.\nWe may wish to know whether the relationship is better visualised by a loess smoother (don’t worry about this term we’ll cover it elsewhere). To do this we simply add another geom that makes a wiggly line (Fig. 3.21).\n我们或许想知道，用 Loess 平滑器（不用担心这个术语，我们会在其他地方讨论）是否能更好地展现这种关系。为此，我们只需添加另一个用于绘制波浪线的“geom”（图 3.21）。\n\n# \nggplot(Benthic, aes(x = NAP, y = Richness)) +\n  geom_point(alpha = 0.5) +  # transparent points\n  \n  # Add linear model (lm) \n  geom_smooth(\n    method = \"lm\", \n    se = TRUE,          # Add confidence interval\n    color = \"red\",       # linear fit is a red line\n    linewidth = 1\n  ) +\n  \n  # Add loess smoother\n  geom_smooth(\n    method = \"loess\", \n    se = TRUE,          # Add confidence interval\n    color = \"blue\",      # loess curve is blue\n    linewidth = 1\n  ) +\n  \n  # Add labels and a clean theme \n  labs(\n    title = \"Comparison of Linear and Loess Models\",\n    subtitle = \"Red = Linear Model (lm), Blue = Loess Smoother\",\n    x = \"NAP\",\n    y = \"Richness\"\n  ) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFig. 3.21: A comparison of linear and loess fits to the beach dataset.\nWe see from this that the lines are very similar and that the confidence intervals sit on top of each other so a linear fit is fine in this instance. 由此我們可以看出，這些線非常相似，並且置信區間彼此重疊，因此在這種情況下線性擬合是可以的。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#using-one-continuous-and-one-factor-variable",
    "href": "chap3.html#using-one-continuous-and-one-factor-variable",
    "title": "3  Data exploration strategies using R",
    "section": "3.9 Using one continuous and one factor variable",
    "text": "3.9 Using one continuous and one factor variable\nBoxplots using factors (grouping or categorical) factors come into their own when mixed with continuous variables. We might wish to know how the samples on each beach differ in terms of their species richness. We can accomplish this in the following way. Lots of additional code her show other things. Read the comments in the code chunk. Important note - ggplot deals with different classes of variable in a difference manner. We want the Beach variable to be a grouping variable (or factor) here. It is currently an integer. To make sure we use a factor we add factor(Beach) to our code (see first line) (Fig. 3.22). We could use the mutate() function from dplyr to add a new variable to the dataframe called FBeach. Have a go. If you are successful you need to add FBeach in line one instead of factor(Beach).\n使用因子（分組或分類）的箱線圖在與連續變數混合時會發揮作用。我們可能希望了解每個海灘上的樣本在物種豐富度上有何不同。我們可以透過以下方式實現此目的。許多額外的程式碼展示了其他內容。閱讀程式碼區塊中的註解。 重要提示 - ggplot 以不同的方式處理不同類別的變數。我們希望 Beach 變數在這裡成為分組變數（或因子）。它目前是一個整數。為了確保我們使用了一個因子，我們在程式碼中加入了 factor(Beach)（見第一行）（圖 3.22）。我們可以使用 dplyr 中的 mutate() 函數在資料框中新增一個名為 FBeach 的新變數。 試一試。如果成功，則需要在第一行新增 FBeach 而不是 factor(Beach)。\n\nggplot(Benthic, aes(x = factor(Beach), y = Richness)) +\n\ngeom_boxplot(\n    fill = \"skyblue\",\n    alpha = 0.7,\n    # This tells the boxplot geom to NOT draw the outlier points. THIS IS CRUCIAL OTHERWISE IT WILL PLOT TOO MANY POINTS - IT'LL ADD ADDITIONAL POINTS - WHAT IT THINKS ARE OUTLIERS.\n    outlier.shape = NA \n  ) +\n  \n  # plot all points.\n  geom_jitter(\n    width = 0.2,\n    alpha = 0.5\n  ) +\n  \n  # Add labels and a clean theme \n  labs(\n    title = \"Species Richness Across 9 Beaches\",\n    subtitle = \"Each point represents one of five samples per beach\",\n    x = \"Beach\",\n    y = \"Species Richness\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFig. 3.22: Boxplot showing the pattern of species richness in our beach samples.\nTASK: To see why replace line one with the code below. Make sure the line ends with the plus + symbol [1-2 mins]\nggplot(Benthic, aes(x = Beach), y = Richness) +\nLooking at the plot you can see that a lot of the patterning in our regression (Fig. 3.21) is being driven by the data points in beaches 1 and 2 that have much higher values than beaches 3-9. You can also see that there is a lot of heterogeneous variability in the samples within each of the beaches.\nggplot(Benthic, aes(x = Beach), y = Richness) +\n查看图表，您会发现回归模型（图 3.21）中的许多模式是由海滩 1 和 2 中的数据点驱动的，这些数据点的值远高于海滩 3-9。您还可以看到，每个海滩内的样本都存在大量的异质性变异。\n\n3.9.1 Examining a mixture of three variables (continuous and factor)\nTo do this we’ll import a new dataset, Grazing.csv. 为此，我们将导入一个新的数据集 Grazing.csv。\n\nRye &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/Grazing.csv\")\n\nRows: 18 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Field, Grazing\ndbl (1): Abund\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHave a look at it.\n\nskim(Rye)\n\n\nData summary\n\n\nName\nRye\n\n\nNumber of rows\n18\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nField\n0\n1\n3\n5\n0\n2\n0\n\n\nGrazing\n0\n1\n3\n4\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAbund\n0\n1\n19.44\n12.41\n5\n9.5\n16\n30.25\n44\n▇▇▁▂▃\n\n\n\n\nstr(Rye)\n\nspc_tbl_ [18 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Abund  : num [1:18] 9 11 6 14 17 19 28 31 32 7 ...\n $ Field  : chr [1:18] \"Top\" \"Top\" \"Top\" \"Top\" ...\n $ Grazing: chr [1:18] \"Low\" \"Low\" \"Low\" \"Mid\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Abund = col_double(),\n  ..   Field = col_character(),\n  ..   Grazing = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nIt has 18 rows and 3 columns:\n\nAbund is the the amount of rye grass in plots within a field. It is num (numeric) class variable.\nField shows the location of plots in the field; it is chr (character) class with 2 unique values ‘top’ and ‘bottom’.\nGrazing indicate level of grazing in the plots; it is chr (character) class with 3 unique values ‘low’, ‘medium’ and ‘high’.\n\nThere are no missing values.\nWe could use two conditional boxplots to show the variation in Abund against both Field and Grazing, but it makes sense to try and combine them all. We can do this using geom_violin to show the pattern in rye grass abundance, and facet_wrap to compare the grazing levels (Fig. 3.23). geom_violin gets around the issue with boxplots print additional points on the plots.\n它有 18 行 3 列：\n\nAbund 表示田地中各样地的黑麦草丰度。它是一个 num（数值）类变量。\nField 表示田地中样地的位置；它是一个 chr（字符）类变量，包含两个唯一值：“top”和“bottom”。\nGrazing 表示样地的放牧程度；它是一个 chr（字符）类变量，包含三个唯一值：“low”、“medium”和“high”。\n\n没有缺失值。\n我们可以使用两个条件箱线图来显示 Abund 与 Field 和 Grazing 变量的变化，但尝试将它们全部结合起来更有意义。我们可以使用 geom_violin 来显示黑麦草丰度的模式，并使用 facet_wrap 来比较放牧程度（图 3.23）。 geom_violin 解决了箱线图在图上打印额外点的问题。\n\n# Load data and assign axes.\nggplot(Rye, aes(x = Field, y = Abund, fill = Field)) +\n  \n  # Use geom_violin to visualise the distribution shape\n  geom_violin(trim = FALSE, alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.6) +\n  \n  # This creates a separate plot for each level in the 'Grazing' column\n  facet_wrap(~ Grazing) +\n  \n  # Add labels and a clean theme\n  labs(\n    title = \"Abundance by Field Position, Faceted by Grazing Level\",\n    x = \"Field Position\",\n    y = \"Abundance\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\") # Hide redundant legend\n\n\n\n\n\n\n\n\nFig. 3.23: A violin plot showing the differences in rye grass abundance by grazing level and field location.\nThis shows a number of patterns. First, grazing level is linked to abundance. The more there is the higher the abundance of rye grass. This makes sens ab rye grass in less palatable to grazers because its stems etc have higher levels of silica. It also shows that the position in field only appears to be important where grazing levels are higher. Try flipping the factor variables around i.e. swapping Field and Gazing in the code and see what happens.\nWhen dealing with multivariate data, that has different classes we need to think about likely dependencies between variables. These types of issues can be problematic when we analyse data so it is important to think about in our EDA. Facetting is a powerful tool to do this. Let’s return to our Benthic dataframe. We have species richness counts for each of nine beaches and our global regression plot (Fig. 3.20) suggests a clear pattern. But does this relationship hold for all the nine beaches or is their variability in it? Let’s have a look (Fig. 3.24).\n这揭示了一些模式。首先，放牧水平与丰度相关。放牧水平越高，黑麦草的丰度就越高。这使得黑麦草对食草动物来说不太可口，因为它的茎等含有更高的二氧化硅。这也表明，只有在放牧水平较高的情况下，田间位置才显得重要。尝试翻转因子变量，即在代码中交换“Field”和“Gazing”，看看会发生什么。\n处理包含不同类别的多变量数据时，我们需要考虑变量之间可能存在的依赖关系。这类问题在分析数据时可能会造成问题，因此在EDA中考虑这些问题非常重要。分面是一个强大的工具。让我们回到我们的底栖生物数据框。我们有九个海滩的物种丰富度计数，我们的全局回归图（图3.20）显示出一个清晰的模式。但这种关系适用于所有九个海滩吗？还是说它们之间存在差异？我们来看一下（图3.24）。\n\nggplot(Benthic, aes(x = NAP, y = Richness)) +\n  geom_point(alpha = 0.6) + # Added some transparency to the points\n  geom_smooth(\n    method = \"lm\", \n    se = TRUE, # add in some confidence interval\n    color = \"red\",       # Change line color to red\n    linewidth = 1.2    # Make the line slightly thicker\n  ) +\n  facet_wrap(\n    ~ Beach\n  ) +\n  # Set the y-axis viewing window to start at 0. You cannot have a negative count of species richness.\n  coord_cartesian(ylim = c(0, NA)) +\n  \n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFig. 3.24: A facet plot showing the Richness / NAP relationship on the individual beaches.\nThis is complex but it tells us a lot. We see that:\n\nThe slopes of the relationship vary by beach, so NAP is has a bigger influence on richness on beaches 2, 3 and 9.\nRichness levels differ by beach. See the height of the lines on the respective Y axes.\nNAP levels are more negative on beaches 1,2,6,7 and 9. See the lines and data points are more to the left in each box.\nBeaches 2, 5 and 9 have more variability in richness values across the 5 samples. See width of confidence intervals shown in grey.\n\nWe have learned a lot here and we can take that information into any analysis we carry out.\n这很复杂，但它揭示了很多信息。我们发现：\n\n关系的斜率因海滩而异，因此NAP对海滩2、3和9的丰富度的影响更大。\n不同海滩的丰富度水平也不同。请参见相应Y轴上线条的高度。\n海滩1、2、6、7和9的NAP水平为负值较大。请参见每个方框中的线条和数据点更靠左。\n海滩2、5和9在5个样本中的丰富度值差异更大。请参见灰色显示的置信区间宽度。\n\n我们在这里学到了很多东西，并且可以将这些信息运用到我们进行的任何分析中。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#summary",
    "href": "chap3.html#summary",
    "title": "3  Data exploration strategies using R",
    "section": "3.10 Summary",
    "text": "3.10 Summary\nEverything we have done today is to show how various graphical representations can show patterns in our datasets. Understanding this is crucial ahead of any analyses to make sure that you select the correct approaches but also are informed enough to interpret any puzzling outcomes. Clearly, we have only scratched the surface of what is possible with these tools. We will reinforce EDA strategies as we move forward in our analytical journey, as the workshops progress.\n我们今天所做的一切都是为了展示各种图形表示如何展现数据集中的模式。在进行任何分析之前，理解这一点至关重要，这样才能确保你选择正确的方法，同时也能充分理解任何令人费解的结果。显然，我们只是触及了这些工具的皮毛。随着研讨会的进展，我们将在分析过程中不断推进，强化 EDA 策略。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#class-exercises",
    "href": "chap3.html#class-exercises",
    "title": "3  Data exploration strategies using R",
    "section": "3.11 Class Exercises",
    "text": "3.11 Class Exercises\n\n3.11.1 EXERCISE 1: boxplots\nUse the Deer.txt dataset. Create boxplots to illustrate the difference between:\n\nAnimal size v gender\nChange the default colour scheme, add labels to the axes\nSave it as jpeg file\n\n\n\n3.11.2 EXERCISE 2: Scatterplots\nUse the Vegetation2.csv dataset and:\n\nPlot a graph showing Species Richness against Exposed (BARESOIL)\nAdd a linear trend line\nSave it as a Tiff file\n\n\n\n3.11.3 EXERCISE 3: facetting\nUse the deep sea research data (ISIT.txt).\n\nUse ggplot2 to plot depth v bioluminscence for each station.\n\n\n\n3.11.4 EXERCISE 4: plot three continuous variables on one figure.\nReturn to the beach dataset (called Benthic). Recall it has three continuous variables Richness, NAP and Exposure (you did not use this one). Look at the hydrological example above (i.e. Fig. 3.11) and create a similar looking plot that uses all three variables in the Benthic data. Note: Richness is the y axis, NAP is the x axis. How will you use Exposure in your plot?\n\n\n3.11.5 练习 1：箱线图\n使用 Deer.txt 数据集。创建箱线图来说明以下两者之间的区别：\n\n动物大小与性别\n更改默认配色方案，为坐标轴添加标签\n保存为 jpeg 文件\n\n\n\n3.11.6 练习 2：散点图\n使用 Vegetation2.csv 数据集并：\n\n绘制物种丰富度与裸土（BARESOIL）关系的图表\n添加线性趋势线\n保存为 Tiff 文件\n\n\n\n3.11.7 练习 3：分面\n使用深海研究数据 (ISIT.txt)。\n\n使用 ggplot2 绘制每个站点的深度与生物发光关系图。\n\n\n\n3.11.8 练习 4：在一张图上绘制三个连续变量。\n回到海滩数据集（名为“底栖”）。回想一下，它有三个连续变量：丰富度、净水量 (NAP) 和暴露度（你没有使用这个变量）。参考上面的水文示例（即图 3.11），并创建一个类似的图表，使用底栖生物数据中的所有三个变量。注意：丰富度为 y 轴，NAP 为 x 轴。您将如何在图表中使用暴露度？",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#next-week",
    "href": "chap3.html#next-week",
    "title": "3  Data exploration strategies using R",
    "section": "3.12 Next Week",
    "text": "3.12 Next Week\nWe will be introducing you to means tests (i.e. t-tests and ANOVA), techniques used to test the assumptions surrounding their use and the alternatives that are available to them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#follow-up-work",
    "href": "chap3.html#follow-up-work",
    "title": "3  Data exploration strategies using R",
    "section": "3.13 Follow-up work",
    "text": "3.13 Follow-up work\n\nFinish the class exercises if you haven’t already done so.\nRead chapter 5 of (Beckerman, Childs, and Petchey 2017).\nRead Zuur, Ieno, and Elphick (2010).\n如果還沒有完成課堂練習，請完成。\n閱讀(Beckerman, Childs, and Petchey 2017)的第 5 章。\n閱讀 Zuur, Ieno, and Elphick (2010)。",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap3.html#references",
    "href": "chap3.html#references",
    "title": "3  Data exploration strategies using R",
    "section": "3.14 References",
    "text": "3.14 References\n\n\n\n\nBeckerman, Andrew, Dylan Childs, and Owen Petchey. 2017. Getting Started with R: An Introduction for Biologists. 2nd. ed. Oxford: Oxford University Press.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd Ed. O’Reilly.\n\n\nWickham, H, R Gentleman, K Hornick, and G Parmigiani. 2009. Ggplot2: Elegant Graphics for Data Analysis. New York: Springer.\n\n\nZuur, Alain F., Elena N. Ieno, and Erik Meesters. 2009. A Beginner’s Guide to R (Use R!). New York: Springer.\n\n\nZuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” Methods in Ecology and Evolution 1 (1): 3–14. https://doi.org/10.1111/j.2041-210X.2009.00001.x.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data exploration strategies using R</span>"
    ]
  },
  {
    "objectID": "chap4.html",
    "href": "chap4.html",
    "title": "4  Comparing groups: Means and related tests",
    "section": "",
    "text": "4.1 Introduction\nLast week, while learning how to create effective plots that display the patterns in your data as effectively as possible, you used a run of graphical tools to depict data structure. This week we are going to put that ‘plot prowess’ to good use by using it to test the assumptions that need to be met when we are comparing groups of things. That might be, for example, the differences in water quality indices (e.g. Total Organic Carbon (TOC), or Biological Oxygen Demand (BOD)) in groups of lakes with different nutrient loads (i.e. eutrophic, mesotrophic and oligotrophic). We will also be exploring how apply appropriate statistical tests to grouped data using the following tests (t-tests, ANOVA, Wilcoxon and Kruskal-Wallis tests).\n上週，在學習如何創建有效的圖表以盡可能有效地顯示資料中的模式時，你使用了一系列圖形工具來描繪資料結構。本週，我們將充分利用這種“繪圖能力”，並用它來檢驗在比較多組事物時需要滿足的假設。例如，這些假設可能是不同營養負荷（即富營養、中營養和貧營養）的湖泊組之間水質指標（例如總有機碳 (TOC) 或生物需氧量 (BOD)）的差異。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#introduction",
    "href": "chap4.html#introduction",
    "title": "4  Comparing groups: Means and related tests",
    "section": "",
    "text": "4.1.1 Learning Outcomes\nAt the end of this workshop your will be able to:\n\nUse ggplot to visualise data to look for normal distributions and homogeneity of variance.\nCarry out parametric and non-parametric tests for situations where you are comparing two groups.\nCarry out parametric and non-parametric tests for situations where you are comparing three or more groups.\nUse statistical tests to assess normality and homogeneity of variance.\nUse R’s in built validation plots to assess normality and homogeneity of variance, as well as the influence of potential outlier points.\n\n\n\n4.1.2 Load your libraries\n\n# List of packages\npackages &lt;- c(\"skimr\",\"readxl\",\"tidyverse\",\"janitor\",\"car\",\"patchwork\",\"coin\")\n\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: survival\n\n\n[[1]]\n[1] \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[7] \"methods\"   \"base\"     \n\n[[2]]\n[1] \"readxl\"    \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n [7] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"readxl\"    \"skimr\"    \n[13] \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"  \n[19] \"base\"     \n\n[[4]]\n [1] \"janitor\"   \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"    \n [7] \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"readxl\"   \n[13] \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[19] \"methods\"   \"base\"     \n\n[[5]]\n [1] \"car\"       \"carData\"   \"janitor\"   \"lubridate\" \"forcats\"   \"stringr\"  \n [7] \"dplyr\"     \"purrr\"     \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"  \n[13] \"tidyverse\" \"readxl\"    \"skimr\"     \"stats\"     \"graphics\"  \"grDevices\"\n[19] \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[6]]\n [1] \"patchwork\" \"car\"       \"carData\"   \"janitor\"   \"lubridate\" \"forcats\"  \n [7] \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tidyr\"     \"tibble\"   \n[13] \"ggplot2\"   \"tidyverse\" \"readxl\"    \"skimr\"     \"stats\"     \"graphics\" \n[19] \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[7]]\n [1] \"coin\"      \"survival\"  \"patchwork\" \"car\"       \"carData\"   \"janitor\"  \n [7] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n[13] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"readxl\"    \"skimr\"    \n[19] \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"  \n[25] \"base\"     \n\n\nOur new packages for today are:\n\ncar. Companion to Applied Regression (Fox and Weisberg 2019). Details of its use can be found here. And,\ncoin (Hothorn et al. 2008). This a package that allow permutation of mean type tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#some-statistical-theory",
    "href": "chap4.html#some-statistical-theory",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.2 Some statistical theory",
    "text": "4.2 Some statistical theory\nMean tests look for differences in either the group means, or in some cases the medians, and they differ depending upon the number of groups you are wishing to compare (i.e. two groups or more than two groups) and the structure of your data. Tests that use group means are known as parametric tests and test that use medians, or rank order, non-parametric tests. The main differences are:\n這些檢定會尋找組別平均值（在某些情況下是中位數）的差異，並且會根據您希望比較的組數（即兩組或兩組以上）和資料結構而有所不同。使用組別平均數的檢定稱為參數檢驗，而使用中位數或秩次的檢定稱為非參數檢定。主要區別在於：\n\nParametric tests assume that the data come from a specific distribution (commonly a normal distribution), have known or estimable parameters (e.g. mean and variance), and require assumptions such as homogeneity of variances. They tend to be more powerful when those assumptions are met and give precise estimates of error or confidence intervals. If you have two groups you’d use a t-test. If you have more than two groups you’d use Analysis of Variance (ANOVA).\nNon-parametric tests do not rely on distributional assumptions; instead, they often use ranks or medians rather than means. They’re more robust and flexible in the face of non-normal data or unequal variances, though they may be less powerful when parametric assumptions hold. The 2-sample equivalents here are Wilcoxon signed-rank test or Mann–Whitney U test. If you have three or more sample groups then you’d use a Kruskal-Wallis test.\n參數檢定假設資料服從特定分佈（通常是常態分佈），具有已知或可估計的參數（例如平均值和變異數），並且需要方差齊性等假設。當滿足這些假設時，參數檢定的效力往往更高，並能提供精確的誤差估計值或信賴區間。如果有兩個組，則可以使用t檢定。如果有兩個以上的組別，則可以使用變異數分析 (ANOVA)。\n非參數檢定不依賴分佈假設；它們通常使用秩或中位數，而不是平均數。在處理非常態資料或變異不均勻時，它們更加穩健且靈活，但在參數假設成立時，它們的效力可能較低。此處的雙樣本等效項是Wilcoxon符號秩檢定或Mann-Whitney U檢定。如果您有三個或更多樣本組，那麼您可以使用Kruskal-Wallis檢定。\n\n\n4.2.1 Testing assumptions\nThere are two key assumptions that need to be met to decide whether it is sensible to use a parametric test on your data:\n\nNormality of data. Tested for graphically with Q-Q plots, histograms and boxplots. The statistical test for this is a Shapiro-Wilk test. The test’s null hypothesis is that the data are normally distributed, and a low p-value (\\(p \\le 0.05\\)) suggests that the null hypothesis should be rejected, indicating the data are not normally distributed.\nHomogeneity of variance between the groups. Test for graphically using boxplots or scatterplot. The statistical test for this is a Levene test. The test’s null hypothesis is that the variance is homogeneous, or displays homoscedasticity, and a low p-value (\\(p \\le 0.05\\)) suggests that the null hypothesis should be rejected, indicating the variance is heterogeneous, ie. it displays heteroscedasticity. The figures below provide flow charts indicating what graphics and tests to use for your data, for both two (Fig. 4.1), and three or more sample situations (Fig. 4.2).\n\n要確定是否適合對資料進行參數檢驗，需要滿足兩個關鍵假設：\n\n資料的常態性。使用Q-Q圖、直方圖和箱型圖進行圖形檢定。用於此目的的統計檢定是Shapiro-Wilk檢定。此檢定的零假設是資料服從常態分佈，較低的p值（&lt; 0.05）表示應該拒絕虛無假設，即資料服從非常態分佈。\n組間變異數齊性。使用箱線圖或散佈圖進行圖形檢定。用於此目的的統計檢定是Levene檢定。此檢定的零假設是方差齊性，即表現出同方差性，較低的p值（&lt; 0.05）表示應該拒絕零假設，即方差異質性，即表現出異方差性。下圖提供了流程圖，指示針對兩個（圖 4.1）和三個或更多樣本情況（圖 4.2）應使用哪些圖形和測試來處理您的資料。\n\n\n\n\n\n\nFig. 4.1: Two group comparisons\n\n\n\n\n\n\n\n\n\nFig. 4.2: Three or more group comparisons\n\n\n\n\n\n\n4.2.2 Analysis of Variance (ANOVA)\nWe’ll consider how means tests work by outlining how ANalysis Of VAriance (ANOVA) operates. The t-test is very similar. We’ll start with the basics by defining the mean. Like, the median and mode, the mean is a measure of central tendency. Unlike the other two, it susceptible to movement due to extreme values.\nYou should all know how this is calculated. You measure a run of things (\\(x_1\\), \\(x_2\\), \\(x_3\\) to \\(x_n\\)) then sum them (\\(\\Sigma\\)) and divide by the number of variables (\\(n\\)).\n我們將透過概述變異數分析 (ANOVA) 的工作原理來探討平均值檢定的工作原理。變異數分析 (ANOVA) 的全名為「變異數分析 (AN)」。它是基於一種稱為變異數的統計數據，但我們不妨從最基本的層面開始，先定義一個平均值。大家都知道，變異數分析的原理是：測量一系列變數（\\(x_1\\)、\\(x_2\\)、\\(x_3\\) 到 \\(x_n\\)），然後對它們求和（\\(\\Sigma\\)），再除以變數個數（\\(n\\)）。\nOr: \\[ \\mu = \\frac{\\sum_{i=1}^{N} x_i}{N} \\] ANOVA uses both the mean, but crucially a statistic called variance. This is statistic that shows the spread of measured values around mean (or variability). Others statistics exist that do this, such as the range and standard deviation. You will cover on those as you move through the workshop code. The equation for variance is below, and, as you might expect, R has a function that calculates it, called var.\n它透過計算變異數來運作，變異數是一個顯示測量值圍繞平均值分佈的變數。 \\[ \\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N} \\] ANOVA calculates two types of variance to establish if there is a statistically significant difference between our sample groups (Fig. 4.3):\n\nBetween Group Variance. This is how much the mean of each group varies from the overall mean for all the data points. Think of it as the signal across the groups. A high “between-group” outcome indicates that the means of each group are highly different from each other. A low “between-group” outcome suggests that the means of all three groups are very close to each other.\nWithin Group Variance. This is the amount of natural, random variation among data points inside a single group. So in many ways it is a measure of noise in our measurements. A low “within-group” outcome (Low Noise) suggests all the data in very similar to each i.e. there is low variability. A high “within-group” outcome (high noise) means that there is a lot of natural variation. The data are very messy.\n\n變異數分析計算兩種類型的方差，以確定樣本組之間是否存在統計上的顯著差異（圖 4.3）： - 組間方差. 這是指每組平均值與所有數據點的總體平均值之間的差異。可以將其視為跨組訊號。較高的「組間變異數」結果表示每組的平均值彼此差異很大。較低的「組間變異數」結果表示所有三組的平均值彼此非常接近。 - 組內方差. 這是指單一組內資料點之間自然、隨機變異的量。因此，在很多方面，它是我們測量中訊號雜訊的度量。較低的「組內方差」（低雜訊）表示所有資料都非常相似，即變異性較低。較高的「組內方差」（高雜訊）表示存在很大的自然變異。數據非常混亂。\n\n\n\n\n\n\n\n\n\nFig. 4.3: The two components of variation in an ANOVA calculation\nANOVA compares these by calculating a ratio called the F-statistic.\nF-statistic = (Between group variance) / (Within group variance)\nThink of it as a Signal-to-Noise ratio. If the F-statistic is large, it means the variation between the groups (the signal) is larger than the (random) variation within the groups (the noise). This provides strong evidence that there is a real difference between the groups. Or, put another way, between group variance is larger than within group variance. If the F-statistic is small, it means the variety between the groups is similar to the background noise. This suggests that any difference you see in the averages is probably just due to chance. Or, the within group variance is larger than between group variance.\n可以將其視為“信噪比”。如果 F 統計量較大，則表示組間變異（訊號）大於組內（隨機）變異（雜訊）。這有力地證明了組間存在真實差異。或者換句話說，組間變異數大於組內變異數。如果 F 統計量較小，則表示組間變異與背景雜訊相似。這表明您在平均值中看到的任何差異可能只是偶然造成的。或者，組內變異數大於組間變異數。\nThe F-statistic is then used to calculate a p-value. A small p-value (usually set at threshold of \\(p \\le 0.05\\)) means it very unlikely we would see such a big difference between the groups by pure chance so we conclude that there is a statistically significant difference somewhere among the groups. A large p-value means it is likely that any difference is governed by random chance and we conclude that there is no significant difference between the group averages.\n然後使用 F 統計量計算 p 值。較小的 p 值（通常設定為 &lt;=0.05 的閾值）意味著我們不太可能純粹偶然地看到組間出現如此大的差異，因此我們得出結論，各組之間存在統計學上的顯著差異。較大的 p 值意味著任何差異都可能由隨機因素控制，我們得出結論，各組平均值之間沒有顯著差異。\nANOVA can be hand calculated in a series of simple steps. We will use an example to illustrate this. Imagine you want to compare the effectiveness of three different fertilizers on plant growth. You take three groups of plants:\n\nGroup A gets Fertilizer A.\nGroup B gets Fertilizer B.\nGroup C gets only water (this is your control group).\n\nAfter a month, you measure the height of all the plants. You calculate the average height for each group. You notice the average height for Group A is the tallest, followed by B, then C.\nANOVA allows you test whether that visual difference is statistically significant. Let’s walk through the process again.\n方差分析可以透過一系列簡單的步驟手動計算。我們將用一個例子來說明這一點。假設您想比較三種不同肥料對植物生長的有效性。 您選取三組植物：\n\nA組施用肥料A。\nB組施用肥料B。\nC組只澆水（這是您的對照組）。\n\n一個月後，您測量了所有植物的高度。您計算了每組植物的平均高度。您注意到A組的平均高度最高，其次是B組，最後是C組。\n變異數分析可以檢驗這種視覺差異是否具有統計意義。讓我們再回顧一下這個過程。\nStep 1: Measuring the Total Variation (Total Sum of Squares - SST)\nFirst, we forget about the groups and view all the plants as one big dataset. We start by calculating the grand (overall) average (or Population Mean) height of all the plants combined:\n首先，我們先不考慮植物的分組，將所有植物視為一個大數據集。我們首先計算所有植物的總平均高度（或族群平均值）： \\[\n\\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n\\] For every single plant, we measure the difference between its height and the population mean (we’ll call these values residuals). Some plants will be positive (taller than average) and some negative (shorter). If you just add them up, they will cancel each other out and sum to zero, so we square every single distance and then sum all of those squared distances together. This final number is known at the Total Sum of Squares (SST). It represents the overall variation in your dataset.\n對於每株植物，我們測量其高度與總體平均值之間的差異（我們將這些值稱為殘差）。有些植物會為正值（高於平均值），有些則為負值（低於平均值）。如果直接將它們相加，它們會相互抵消，總和為零，因此我們需要對每個距離求平方，然後將所有平方距離相加。最終的數字稱為總平方和 (SST)。它代表了數據集的整體變異。 \\[\nSST = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{\\bar{x}})^2\n\\] where,\n\n\\(\\sum_{i=1}^{k}\\) is the total number of groups.\n\\(\\sum_{j=1}^{n_i}\\) is the number of observations in group \\(i\\).\n\\(x_{ij}\\) is the \\(j\\)-th observation in the \\(i\\)-th group.\n\\(\\bar{\\bar{x}}\\) is the population mean of all observations.\n\n其中：\n\n\\(\\sum_{i=1}^{k}\\) 是組總數。\n\\(\\sum_{j=1}^{n_i}\\) 是組 \\(i\\) 中的觀測值個數。\n\\(x_{ij}\\) 是第 \\(i\\) 組中的第 \\(j\\) 個觀測值。\n\\(\\bar{\\bar{x}}\\) 是所有觀測值的總體平均值。\n\nStep 2: Measuring the Explained Variation (Between-Group Sum of Squares - SSB or Signal)\nNow, let’s measure the variation that we think by the application of fertilizers to the plants. We calculate the average height for each fertilizer group (Group A, Group B, Group C) and measure the difference of this to the population mean. We square each of these differences and multiply each squared difference by the number of plants in that group (an important step that gives more weight to larger groups). Finally, we sum these values together. This number is the Between-Group Sum of Squares (SSB):\n現在，讓我們來測量一下施肥對植物造成的變異。我們計算每組肥料（A組、B組、C組）的平均高度，並測量其與總體平均值的差異。我們將這些差值平方，再乘以該組中植物的數量（這一步很重要，因為它賦予較大的組更高的權重）。最後，我們將這些值相加。這個數字就是組間平方和（SSB）： \\[\nSSB = \\sum_{i=1}^{k} n_i (\\bar{x}_i - \\bar{\\bar{x}})^2\n\\] where,\n\n\\(\\bar{x}_i\\) is the mean of group \\(i\\).\nand the other terms are as above.\n\nIt represents the portion of the total variation that is explained by the differences between the group means. Or the signal.\n其中：\n\n\\(\\bar{x}_i\\) 是組 \\(i\\) 的平均數。\n其他項同上。\n\n它表示總變異中可由組別平均值之間的差異解釋的部分。或者說，信號。\nStep 3: Measuring the Unexplained Variation (Within-Group Sum of Squares - SSW or Error)\nThis is the most important part i.e. the calculation of the errors.\n\nWe focus on one group at a time (e.g., just Group A).\nCalculate the average height for just that group (the group or sample mean).\nFor every plant within Group A, we then measure the difference between its height and the Group A mean and square all of those distances.\nFinal we sum them up. This gives you the Sum of Squares for Group A.\nWe repeat this process for Group B and Group C.\n\nFinally, we add the Sum of Squares from all the groups together. This is the Within-Group Sum of Squares (SSW). It is also called the Error Sum of Squares (SSE) because it represents the random, inherent variation that our model (the fertilizer type) cannot explain (i.e the noise). We can also think of it as the leftover or residual error.\n這是最重要的部分，即誤差的計算。\n\n我們一次只專注於一個群組（例如，只專注於 A 組）。\n計算該組的平均高度（組或樣本平均值）。\n對於 A 組中的每一株植物，我們測量其高度與 A 組平均值之間的差異，並計算所有差異的平方。\n最後，我們將這些差值相加。這就得到了 A 組的平方和。\n我們對 B 組和 C 組重複此程序。\n\n最後，我們將所有組別的平方和相加。這就是組內平方和 (SSW)。它也被稱為誤差平方和 (SSE)，因為它代表了我們的模型（肥料類型）無法解釋的隨機固有變異（即雜訊）。我們也可以將其視為剩餘誤差或殘差。 \\[\nSSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2\n\\] where,\n\nall terms are defined above.\n\nStep 4: Calculating the mean squares\nThe mathematics of ANOVA means that these terms balance:\nSST = SSB + SSW (Total Variation = Explained Variation + Unexplained Variation)\nAlthough the Sum of Squares gives us the components of the raw variation, to compare them appropriately, we need to find the average variation of each one. To do this, we divide by the degrees of freedom (df), which is the number of items used to calculate the sum.\n變異數分析的數學原理保證了這些項之間的平衡：\nSST = SSB + SSW（總變異 = 解釋變異 + 未解釋變異）\n雖然平方和給出了原始變異的各組成部分，但為了正確地比較它們，我們需要找到每個組成部分的平均變異。為此，我們將平均值除以自由度 (df)，即用於計算和的項數。\nMean Square Between (MSB)\nMSB = SSB / \\(df_{between}\\)\nwhere,\n\\(df_{between}\\) = number of groups - 1).\nThis is the average explained variance, our final “Signal” measure.\nMSB = SSB / \\(df_{between}\\)\n其中，\n\\(df_{between}\\) = 組數 - 1)。\n這是平均解釋方差，也是我們最終的「訊號」指標。\nMean Square Within (MSW or MSE)\nMSE = SSW / \\(df_{within}\\)\nwhere,\n\\(df_{within}\\) = total number of plants - number of groups)\nThis is the average unexplained variance, our final “Noise” or “Error” measure.\nMSE = SSW / \\(df_{within}\\)\n其中，\n\\(df_{within}\\) = 植物總數 - 組數)\n這是平均未解釋方差，也是我們最終的「雜訊」或「誤差」指標。\nStep 5: The F-statistic\nThe F-statistic is the ratio of these two “average” variances:\nF = Mean Square Between / Mean Square Within\nor\n\\(MSB / MSW\\)\nFrom here, the F-statistic is used to generate the p-value, which tells us if the signal from our fertilizers is strong enough to be heard over the background noise of random error.\nThat’s all the theory for today!! Let’s get on with some coding.\nF 統計量是這兩個「平均」變異數的比值：\nF = 組間均方 / 組內均方\n或\n\\(MSB/MSW\\)\n由此，F 統計量用於產生 p 值，該 p 值告訴我們肥料發出的訊號是否足夠強，足以在隨機誤差的背景雜訊中被檢測到。\n今天的理論講解就到這裡！讓我們開始編寫程式碼吧。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#exploring-variability-in-data",
    "href": "chap4.html#exploring-variability-in-data",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.3 Exploring variability in data",
    "text": "4.3 Exploring variability in data\nWe create some basic data to play with. 我們創建了一些基本數據來使用.\n\ny &lt;- c(13, 7, 5, 12, 9, 15, 6, 11, 9, 7, 12) # create some basic data\n\nLet’s plot it. We will use base R as ggplot doesn’t accept vectors (Fig. 4.4). 讓我們來繪製它。我們將使用基礎 R，因為 ggplot 不接受向量（圖 4.4）。\n\nplot(y, ylim = c(0, 20))    # plot it as a scatter limiting the axis 0 - 20\n\n\n\n\n\n\n\n\nFig. 4.4: XY plot of simulated data\nFor reference, the ggplot code is featured below (Fig. 4.5). Note it just needs the data placing in a dataframe. 供參考，ggplot 程式碼如下所示（圖 4.5）。注意，它只需要將資料放置在資料框中即可。\n\n# ggplot2 needs a data frame.\n# We create one with the 'y' values and their 'index'.\ndf &lt;- data.frame(\n  index = seq_along(y),\n  value = y\n)\n\n# 3. Create the ggplot\nggplot(data = df, aes(x = index, y = value)) +\n  geom_point() +  # The equivalent of the default plot type\n  coord_cartesian(ylim = c(0, 20)) + # Sets the visible y-axis range\n  labs(\n    title = \"ggplot2 equivalent of plot(y)\",\n    x = \"Index\",\n    y = \"Value\" \n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n** Fig. 4.5: Simulated data plotted with ggplot**\nHow can we quantify this scatter? We could use the range range() which returns the minimum and maximum values. Check out the min() and max() R functions. The difference is 10 units and we can use that as an measure of variability in our data. 我們如何量化這種散度？我們可以使用範圍函數 range()，它會傳回最小值和最大值。查看 R 函數 min() 和 max()。差異是 10 個單位，我們可以用它來衡量資料的變異性。\n\nrange(y)\n\n[1]  5 15\n\n\nThis is okay but we really need to understand how each measured \\(y\\) value varies against the mean of each one. These numbers are the residuals were discussing earlier today. To do this we need to calculate the mean. 這沒問題，但我們真正需要了解的是，每個測量的 \\(y\\) 值相對於每個值的平均值是如何變化的。這些數字就是我們今天早些時候討論過的殘差。為此，我們需要計算平均值。\n\nmean(y)\n\n[1] 9.636364\n\n\nNow we subtract the mean for each value of \\(y\\) giving us a new vector. 現在我們減去 \\(y\\) 每個值的平均值，得到一個新的向量。\n\ny - mean(y)  # The result shows both negative and positive values (not ideal when we sum it)\n\n [1]  3.3636364 -2.6363636 -4.6363636  2.3636364 -0.6363636  5.3636364\n [7] -3.6363636  1.3636364 -0.6363636 -2.6363636  2.3636364\n\n\nYou see they are positive and negative so is we sum them they’ll zero out. So we square and sum it, which gives us the sum of squares. 你看，它們有正有負，所以如果我們把它們相加，它們就會歸零。所以我們求平方，再求和，就得到了平方和。\n\nsum((y-mean(y))^2)\n\n[1] 102.5455\n\n\nRemember above how needed to standardise our ANOVA error terms to make them comparable? Well, we need to the same here or this value is going to increase every time we add to it! We need to standardise it by dividing the value by the number of samples (\\(n\\)). 還記得上面提到的如何將變異數分析的誤差項標準化，使它們具有可比性嗎？好吧，我們需要在這裡保持一致，否則每次增加這個值時，它都會增加！我們需要將數值除以樣本數（\\(n\\)）來標準化它。\n\nsum((y - mean(y))^2)/(length(y) - 1)\n\n[1] 10.25455\n\n\nNotice we are subtracting one from the number of samples to give us the degrees of freedom (df). What we have done here is hand calculate the variance. Obviously, can just call the var() function from R. 請注意，我們從樣本數減去 1 得到自由度 (df)。我們在這裡手動計算的是方差。顯然，可以直接從 R 中呼叫 var() 函數。\n\nvar(y)\n\n[1] 10.25455",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#comparisons-of-two-independent-samples",
    "href": "chap4.html#comparisons-of-two-independent-samples",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.4 Comparisons of two independent samples",
    "text": "4.4 Comparisons of two independent samples\nRefer to Fig. 4.1 above to support this section of code. We are going to start by carrying a t-test. The data you need is called ozone.csv.請參閱上圖 4.1 來支援這部分程式碼。我們將首先進行 t 檢定。您需要的資料是 ozone.csv。\n\no3 &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/ozone.csv\") # load datafile. Use YOUR directory.\n\nRows: 20 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Garden\ndbl (1): O3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHave look at it.\n\nglimpse(o3)\n\nRows: 20\nColumns: 2\n$ O3     &lt;dbl&gt; 3, 4, 4, 3, 2, 3, 1, 3, 5, 2, 5, 5, 6, 7, 4, 4, 3, 5, 6, 5\n$ Garden &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\"…\n\nskim(o3)\n\n\nData summary\n\n\nName\no3\n\n\nNumber of rows\n20\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGarden\n0\n1\n1\n1\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nO3\n0\n1\n4\n1.52\n1\n3\n4\n5\n7\n▅▇▆▇▅\n\n\n\n\n\nIt is a simple df of 20 rows and 2 columns. O3 is ozone concentrations and ‘garden’ is a character variable with two levels, gardens A and B - R correctly interprets this as a factor variable but there are occasions when we need to tranform the variable from the character class to a factor class variable. We’ll cover this in later workshops. Please note the upper and lowercase ‘o’ and ‘O’ in the code. These are not zeros! We’ll start out by testing the assumption of normality using a histogram and a Q-Q plot (Figs. 4.6, 4.7) and running a Shapiro-Wilk test (Fig. 4.1 above). 這是一個簡單的 20 行 2 列的自由度函數。 O3 表示臭氧濃度，「garden」是一個具有兩個等級的字元變數。花園 A 和 B - R 正確地將其解釋為因子變量，但有時我們需要將其從字元類變數轉換為因子類變數。我們將在後續的研討會中討論這個問題。請注意程式碼中的大小寫“o”和“O”。它們不是零！我們將先使用直方圖和 Q-Q 圖（圖 4.6、4.7）來檢定常態性假設，並執行 Shapiro-Wilk 檢定（上圖 4.1）。\n\n# plot histogram\nggplot(o3,aes(x=O3)) +\n  geom_histogram(binwidth=1, colour=\"black\", fill=\"grey90\") +\n  labs(x=\"Ozone levels\") + # add a label\ntheme_bw() + # change default scheme to black and white....this removes axes and other things!\n theme(\n  panel.grid.minor = element_blank(), #remove the minor grids\n  panel.grid.major = element_blank(), #remove the minor grids\n  panel.border = element_blank(), # remove the plot border\n  axis.line = element_line(colour = \"black\") # Add axis lines back for a cleaner look\n )\n\n\n\n\n\n\n\n# add Q-Q plot\nggplot(o3, aes(sample=O3)) +\n  stat_qq() + \n  stat_qq_line(\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigs. 4.6-4.7: Histogram and Q-Q plot of the Ozone dataset\nTASK: What do you think these plots indicate? Make a few notes on each one.\nWe can use the Shapiro-Wilk test to check for normality to confirm your thoughts. I am not a fan of these sort of tests for lots of reasons. The main one is that these tests, because of how the null hypothesis is framed, is the opposite way to a standard means test and this causes a fair bit of confusion. In this instance we are looking for an outcome that is not significant i.e. \\(p\\ge 0.05\\). A significant finding i.e. \\(p\\le 0.05\\) means that the data do not fit a normal distribution. 我們可以使用 Shapiro-Wilk 檢定來檢驗常態性，以驗證你的想法。我不太喜歡這類檢驗，原因有很多。最主要的是，由於零假設的建構方式，這些檢驗與標準的經濟狀況檢驗相反，這很容易造成混淆。在本例中，我們尋找的是不顯著的結果，即 \\(p\\ge 0.05\\)。顯著的發現，即 \\(p\\le 0.05\\)，表示資料不符合常態分佈。\n\nshapiro.test(o3$O3)\n\n\n    Shapiro-Wilk normality test\n\ndata:  o3$O3\nW = 0.96508, p-value = 0.6495\n\n\nIs the outcome of this test confirmation of what you thought looking at the histogram and Q-Q plot? So that is our first assumption test out of the way. The second one relates to homoscedasticity, or homogeneity of variance across the groups (which are defined by the factor variable). To do this we’ll plot a boxplot to visually inspect (Fig. 4.8) it and run a Levene test to check. 這項檢驗的結果是否證實了你透過直方圖和Q-Q圖所想？所以，我們的第一個假設檢定已經完成。第二個假設檢定與同方差性有關，即各組（由因子變數定義）之間的變異數齊性。為此，我們將繪製一個箱線圖進行視覺化檢查（圖4.8），並執行Levene檢定進行驗證。這項檢驗的結果是否證實了你透過直方圖和Q-Q圖所想？所以，我們的第一個假設檢定已經完成。第二個假設檢定與同方差性有關，即各組（由因子變數定義）之間的變異數齊性。為此，我們將繪製一個箱線圖進行視覺化檢查（圖4.8），並執行Levene檢定進行驗證。\n\n#create a boxplot\nggplot(o3, aes(x = Garden, y = O3)) + \n  geom_boxplot() +\n  theme_bw() +\n      theme(\n        panel.grid.minor = element_blank(), #remove the minor grids\n        panel.grid.major = element_blank(), #remove the minor grids\n     )\n\n\n\n\n\n\n\n\nFig. 4.8: Boxplot of the Ozone data\nTASK: You have the code from week 3 to overlay the data points onto boxplots. Can you adapt it to fit this plot? [~ 5 mins].\nIt should look like this when you are done (Fig. 4.9).\n\n\n\n\n\n\n\n\n\nFig. 4.9: Boxplot with data points\nThe indicates that the variance is likely to be homogeneous. The boxplots a very similar. They both are symmetrical with central median, balanced interquartile ranges and symmetrical and equal whiskers, with no potential outliers.\nWe can confirm with a test. Here is the code for the Levene test. The function is called leveneTest and it is in the car package. We don’t really need to do this but we ensure R treats the O3 variable as a factor class by adding in the as.factor() argument.\n這表示方差很可能是同質的。箱線圖非常相似。它們都是對稱的，具有中心中位數、平衡的四分位距、對稱且相等的須線，並且沒有潛在的異常值。\n我們可以透過測試來確認。這是 Levene 測試的程式碼。此函數名為 leveneTest，位於 car 套件中。我們實際上不需要這樣做，但我們透過添加 as.factor() 參數來確保 R 將 O3 變數視為因子類別。\n\nleveneTest(O3 ~ as.factor(Garden), data = o3)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1       0      1\n      18               \n\n\nThe p-value is returned as 1, so the data do exhibit homoscedasticity. We can are secure that both assumptions have been met: data normality and homogeneity of variance. So we are good to use a parametric test, which is a t-test. Our null hypothesis here is that the there is no difference in ozone concentrations in the two gardens.\np 值傳回為 1，因此資料確實表現出同方差性。我們可以確信兩個假設都已滿足：資料常態性和變異數齊性。因此，我們最好使用參數檢驗，也就是 t 檢定。我們這裡的零假設是兩個花園的臭氧濃度沒有差異。\n\n# t-test with using a grouping factor\nt.test(O3 ~ Garden, data = o3) # Note - the data argument and '~' (tilde) operator (you'll see a lot of this)\n\n\n    Welch Two Sample t-test\n\ndata:  O3 by Garden\nt = -3.873, df = 18, p-value = 0.001115\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -3.0849115 -0.9150885\nsample estimates:\nmean in group A mean in group B \n              3               5 \n\n\nWe see that the p-value = 0.001115 so we accept the alternative hypothesis that the ozone concentrations in the two gardens are significantly different to each other.\nIf our data failed both, or one or the other of the assumption tests we’d need to use a non-parametric** version of which is called a Wilcoxon test. We’ll illustrate it here using the same data (even though it fits the criteria for a t-test). The function is wilcox_test and it is in the coin package. Note: base R also has a variant of this test called wilcox.test.\n我們看到 p 值 = 0.001115，因此我們接受備擇假設，即兩個花園的臭氧濃度彼此之間存在顯著差異。\n如果我們的資料未能通過兩個或其中一個假設檢驗，我們需要使用一種非參數版本，稱為Wilcoxon 檢定**。我們將在這裡使用相同的數據（即使它符合 t 檢定的標準）來說明這一點。該函數是 wilcox_test，它位於 coin 套件中。注意：基礎 R 語言中也有此檢定的變體，稱為 wilcox.test。\n\nwilcox_test(O3 ~ as.factor(Garden), data = o3)\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  O3 by as.factor(Garden) (A, B)\nZ = -3.0075, p-value = 0.002634\nalternative hypothesis: true mu is not equal to 0\n\n\nWe see a similar significant difference between ozone concentrations in the gardens. Note, though, the p-value is higher. That is because the test has less has less statistical power than a t-test.\n我們發現花園中的臭氧濃度也有類似的顯著差異。不過請注意，p值更高。這是因為此檢定的統計效力低於t檢定。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#means-tests-for-more-than-two-groups",
    "href": "chap4.html#means-tests-for-more-than-two-groups",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.5 Means tests for more than two groups",
    "text": "4.5 Means tests for more than two groups\nWe will return to the fertilizer example to do this and create a data set that fits it using R’s simulation functions. You do not need to understand this code - it’s means to an end!. 我們將回到肥料範例來執行此操作，並使用 R 的模擬函數建立一個適合該範例的資料集。 您無需理解這段程式碼 - 它只是達到目的的手段！ 。\n\n# Set.seed() to make our \"random\" data reproducible i.e. if we select 42 here it will create the exact same data! Cool eh?\nset.seed(42)\n\n# Define group parameters\nn &lt;- 30                # Number of plants in each group (our sample size)\nsd_within &lt;- 5         # The noise which is the standard deviation in height each group (in cm)\n\n# Define the signal. The sample means (of height) for each group (in cm)\nmean_a &lt;- 65           # Fertilizer A (best)\nmean_b &lt;- 58           # Fertilizer B (good)\nmean_c &lt;- 50           # Water control (baseline)\n\n# Create random data using rnorm()\nheights_a &lt;- rnorm(n, mean = mean_a, sd = sd_within)\nheights_b &lt;- rnorm(n, mean = mean_b, sd = sd_within)\nheights_c &lt;- rnorm(n, mean = mean_c, sd = sd_within)\n\n\n# A tidy data frame is the standard format for analysis and plotting in R.\nfertilizer_data &lt;- data.frame(\n  # Combine all the height measurements into one column\n  height = c(heights_a, heights_b, heights_c),\n  \n  # Create a corresponding group label for each measurement. Treat = the fertilizer Treatments\n  group = factor(rep(c(\"TreatA\", \"TreatB\", \"Control\"), each = n))\n)\n\n# look at the data\nglimpse(fertilizer_data)\n\nRows: 90\nColumns: 2\n$ height &lt;dbl&gt; 71.85479, 62.17651, 66.81564, 68.16431, 67.02134, 64.46938, 72.…\n$ group  &lt;fct&gt; TreatA, TreatA, TreatA, TreatA, TreatA, TreatA, TreatA, TreatA,…\n\nskim(fertilizer_data)\n\n\nData summary\n\n\nName\nfertilizer_data\n\n\nNumber of rows\n90\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ngroup\n0\n1\nFALSE\n3\nCon: 30, Tre: 30, Tre: 30\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1\n57.9\n7.86\n43.03\n52.82\n57.28\n63.55\n76.43\n▅▇▆▅▂\n\n\n\n\n\nSo 90 rows = 90 plants, two columns (height and group):\n\nheight = measurements of plant height in cm\ngroup = the fertilizer treatments A and B, and the control (water only)\n\nWe’ll check for the assumptions graphically (Fig. 4.10) and confirm with a test.\n因此，90 行 = 90 株植物，兩列（高度和組）：\n\n高度 = 植物高度測量值（公分）\n組 = 施肥處理 A 和 B，以及對照（僅澆水）\n\n我們將透過圖表（圖 4.10）驗證這些假設，並透過測試進行確認。\n\n# Normality check using a Q-Q plot\nggplot(fertilizer_data, aes(sample=height)) +\n  stat_qq() + \n  stat_qq_line(\n  ) +\n  theme_bw() +\n  theme(\n        panel.grid.minor = element_blank(), #remove the minor grids\n        panel.grid.major = element_blank(), #remove the minor grids  \n  )\n\n\n\n\n\n\n\n# check with shapiro.test\nshapiro.test(fertilizer_data$height)\n\n\n    Shapiro-Wilk normality test\n\ndata:  fertilizer_data$height\nW = 0.98234, p-value = 0.2614\n\n\nFig. 4.10: Q-Q plot of the plant height from the fertilizer example\nLooks pretty good. And the Shapiro-Wilk test confirms this with a p-value of 0.2614. Now to visualise it for homogeneity of variance and the Levene test. We’ll use a boxplot (Fig.4.11). 看起來不錯。 Shapiro-Wilk 檢定的 p 值為 0.2614，證實了這一點。現在，為了將其視覺化，以顯示方差齊性和 Levene 檢定的結果，我們將使用箱線圖（圖 4.11）。\n\nggplot(fertilizer_data, aes(x = group, y = height, fill = group)) +\n  geom_boxplot(outlier.shape = NA) + # make sure the outliers are not added twice as points\n  geom_jitter(width = 0.1, alpha = 0.5) + # Add individual points for clarity\n  labs(\n    title = \"Simulated Plant Growth by Fertilizer Treatment\",\n    x = \"Treatment Group\",\n    y = \"Plant Height (cm)\"\n  ) +\n  theme_bw() +\n  guides(fill = \"none\") + # Hide the redundant legend \n theme(\n    panel.grid.minor = element_blank(), #remove the minor grids\n    panel.grid.major = element_blank(), #remove the minor grids \n )\n\n\n\n\n\n\n\n# Levene test\nleveneTest(height ~ as.factor(group), data = fertilizer_data)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  1.4376 0.2431\n      87               \n\n\nFig.4.11: Boxplot of fertilizer experiment data\nThis show a little more variability. Treatments A and B are a little skewed and their variation is greater than the Control group. No potential outliers are present. It looks okay. The Levene test confirms this with p-value of 0.2431. Because the data conform to both assumptions we can run a parametric test, which because there are more than two groups, is an ANOVA test (Fig. 4.2). We use the function aov() from base R to do this. Note this is a single factor ANOVA, because we only have one grouping variable (called group). Here is code; notice the similarities with the Shapiro-Wilk test syntax. The first element of the code is our Y (or response) variable and the second one is an X (or explanatory) variable. the tilde operator ~ separates them and the data= argument tells R what the dataframe is called. 這表示變異性略大一些。處理組 A 和 B 略有偏差，且其變異大於對照組。不存在任何潛在異常值。結果看起來還不錯。 Levene 檢定證實了這一點，其 p 值為 0.2431。由於數據符合這兩個假設，我們可以進行參數檢定；由於組數多於兩組，因此檢定為變異數分析 (ANOVA) 檢定（圖 4.2）。我們使用 R 中的函數 aov() 來執行此操作。請注意，這是單因子變異數分析，因為我們只有一個分組變數（稱為組別）。程式碼如下；請注意它與 Shapiro-Wilk 檢定語法的相似之處。程式碼的第一個元素是我們的 Y（或回應）變量，第二個元素是 X（或解釋）變數。波浪號運算子 ~ 將它們分隔開，data= 參數告訴 R 資料框的名稱。\n\nM1 &lt;- aov(height ~ group, data = fertilizer_data)\n\nWe have piped this into an object called M1 (it is in the environment window - top right). To see the results we use the summary() function and call the object. 我們將其透過管道傳輸到名為 M1 的物件中（它位於環境視窗的右上角）。為了查看結果，我們使用 summary() 函數並呼叫該物件。\n\nsummary(M1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \ngroup        2   3117  1558.6   56.83 &lt;2e-16 ***\nResiduals   87   2386    27.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe table lists the results. The first line is the group (or factor variable). The degrees of freedom are 2 (because there are three levels in the factor and 3-1 =2). Then we have the error terms we discussed above, the Sum of Squares, the Mean Sum of Squares, the F value the p-value. The latter shows a highly significant difference, p=2e-16, or 0.0000000000000002 to be exact. 2e-16 tells you to add 16 elements to the right of the decimal point. The last one is a 2. So that’s 15 zeros! The chances of the difference we see between the treatments is almost zero! The second line shows the number of residuals. 表格列出了結果。第一行是組別（或因子變數）。自由度為 2（因為因子有三個水平，3-1 = 2）。然後是上面討論過的誤差項，分別是平方和、平均平方和、F 值和 p 值。 p 值顯示出高度顯著的差異，p=2e-16，準確地說是 0.0000000000000002。2e-16 表示在小數點右邊加入 16 個元素。最後一個是 2。所以有 15 個零！我們看到不同處理之間存在差異的可能性幾乎為零！第二行顯示殘差的數量。\nThe Df is 87. Remember there are 90 rows in our data and the residuals represent the difference in height between each measured data point and the population mean. So where have three points gone? Degrees of freedom are calculated here by substraction the group Df from 90 (-88) and subtracting another Df for the number of factors in the test, so 1 in this case (88-1=87). The global Sum of Squares error and mean global sum of squares is also reproduced. 自由度 (Df) 為 87。記住，我們的數據中有 90 行，殘差表示每個測量數據點與總體平均值之間的身高差。那麼，這三個點去哪了呢？這裡計算自由度的方法是：用 90（-88）減去組別自由度 (Df)，再減去檢驗因子數對應的自由度 (Df)，在本例中為 1（88-1=87）。全域平方和誤差和全域平方和平均值也重現了。\nBecause aov is a global ANOVA model the summary table does not show us the contrasts. These are differences between the baseline, which is our control (labelled control in the group variable), and the other levels (Treat1 and Treat2). We can analyse this with a lm() model and then summary() will provide them. Or we can use a posthoc test to provide them. This is usually a Tukey test. The function to call it is TukeyHSD(). Remember R lists the levels in the factor alphabetically and level closest to A will be listed first. In this instance, it doesn’t matter because our control level comes before the TreatA, TreatA levels in the alphabet. So it makes sense. If it didn’t we need to reorder the levels. We will do this in the two-factor ANOVA example below. 由於 aov 是一個全域變異數分析模型，因此總表不會顯示 對比。這些是基線（即我們的對照組（在組別變數中標記為 control））與其他水平（Treat1 和 Treat2）之間的差異。我們可以使用 lm() 模型進行分析，然後 summary() 會提供這些差異。或者，我們可以使用 posthoc 檢定來提供這些差異。這通常是 Tukey 檢定。呼叫它的函數是 TukeyHSD()。請記住，R 按字母順序列出因子中的水平，最接近 A 的水平將首先列出。在本例中，這並不重要，因為我們的控制水平在字母表中位於 TreatA、TreatA 水平之前。所以這是合理的。如果不是這樣，我們需要重新排序這些水平。我們將在下面的雙因子變異數分析範例中執行此操作。\n\nTukeyHSD(M1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = height ~ group, data = fertilizer_data)\n\n$group\n                    diff        lwr       upr    p adj\nTreatA-Control 14.389088  11.164807 17.613369 0.00e+00\nTreatB-Control  6.436611   3.212331  9.660892 2.27e-05\nTreatB-TreatA  -7.952477 -11.176757 -4.728196 2.00e-07\n\n\nThe summary table above now show the differences between the levels of the factor contrasting all the possible pairs:\n\nTreatA-Control. This is 14.38. So Treatment A plants are ~ 14.4 cm taller than the Control plants.\nTreatB-Control. This is 6.436611. Treatment B plants are ~6.4 cm taller\nTreatB-TreatA. And Treatment B plants are 7.95 cm smaller than Treatment A plants.\n\nAll the contrasts are significant. See the adjusted p-values (p adj).\n\n4.5.1 Graphical validation of ANOVA analyses\nEvery time we run a statistical model, we need to validate the outcomes. This is standard practice and generally done after we have run the model. We do this the plot() command, where we add the object name between the (). But first we are going to make R plot 4 graphs in a 2 x 2 matrix (Fig. 4.12). We do this by setting the graphic parameters using the par function with the mfrow() argument. 每次運行統計模型時，我們都會驗證結果。這是標準做法，通常在模型運行後進行。我們使用 plot() 命令執行此操作，並在 () 之間新增物件名稱。但首先，我們將使用 R 在一個 2 x 2 矩陣中繪製 4 個圖形（圖 4.12）。我們使用 par 函數和 mfrow() 參數來設定圖形參數來實現。\n\npar(mfrow = c(2, 2)) # set the plotting window to a 2 x 2 frame\nplot(M1)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1)) # reset the window back to 1 x 1 panel\n\nFig. 4.12: Standard R validation plots for a linear object\nWow! Let’s talk through each panel. The table below identifies what each panel is testing and what to look out for if there are issues. I know this is difficult, but you’ll have seen lots of these by the end of the module and it will become second nature. 哇！讓我們來逐一討論一下每個面板。下表列出了每個面板的測試內容以及出現問題時需要注意的事項。我知道這很難，但在本模組結束時，你會看到很多類似的面板，這將成為你的第二天性。\n\n\n\n\n\n\n\n\n\n\nPlot\nKey Assumption\nSigns of a Problem\n\n\n\n\nResiduals vs Fitted\nLinearity and Homoscedasticity\nCurves, funnels, or patterns\n\n\nNormal Q-Q\nNormality of Residuals\nDeviations from the 45-degree line\n\n\nScale-Location\nHomoscedasticity\nSystematic trends or increasing spread\n\n\nResiduals vs Leverage\nInfluence of Points\nPoints outside Cook’s distance lines\n\n\n\n\n\nTable 4.1: Interpreting the panels in the R plot function\nBack to our plot. The Residual v Fitted plot compares the residuals against the fitted values from the ANOVA calculation. There are no patterns in the points, aside of a slight indication of increasing residual spread across the three treatments (this was visible in the boxplots). The Q-Q residual plot shows sufficient normality. The Scale - Location plot is fine the spreads are similar, but the control is slightly lower. The Residuals v Leverage shows no visible outlier points. I am comfortable with these patterns; others may have different views. We will run with that and turn our attention to a situation where there are more than one explanatory variables. 回到我們的圖。 殘差與擬合值圖將殘差與變異數分析計算的擬合值進行比較。除了三種處理之間殘差分佈略有增加（這在箱線圖中可見）外，這些點沒有任何規律。 Q-Q殘差圖顯示出足夠的常態性。 尺度-位置圖很好，分佈相似，但對照組略低。 殘差與槓桿率圖沒有顯示可見的異常點。我對這些規律感到滿意；其他人可能有不同的看法。我們將以此為基礎，並將注意力轉向存在多個解釋變數的情況。\nIf the validation plots had shown significant patterns in the residual spreads, then we would need to act by either standardising the response variable (height) with a Log10 or square-root transformation (not my preferred option but you’ll see examples of this in the literature) or apply an non-parametric test (Fig. 4.2). The test we use is a Kruskal Wallis rank test. The code is straightforward and we’ll use the same data. As above, contrast the p-values on the ANOVA with this test. The function call from the coin package is kruskal_test(). The base R versions is kruskal.test(). This test requires the grouping variables to be factor class variables. We know the group variable is a factor. If we didn’t we’d use mutate() from dplyr to change it. 如果驗證圖在殘差差分中顯示出顯著的模式，那麼我們需要採取行動，要么使用 Log10 或平方根變換對響應變量（身高）進行標準化（這不是我的首選，但您會在文獻中看到相關示例），要么應用非參數檢驗（圖 4.2）。我們使用的檢定是Kruskal Wallis秩檢定。程式碼很簡單，我們將使用相同的數據。如上所述，將變異數分析的 p 值與此檢定進行比較。來自 coin 套件的函數呼叫是 kruskal_test()。基本 R 版本是 kruskal.test()。此檢定要求分組變數是因子類變數。我們知道組變數是一個因子。如果不知道，我們會使用 dplyr 中的 mutate() 來更改它。\n\nkruskal_test(height ~ group, data = fertilizer_data)\n\n\n    Asymptotic Kruskal-Wallis Test\n\ndata:  height by group (Control, TreatA, TreatB)\nchi-squared = 52.339, df = 2, p-value = 4.312e-12\n\n\nWe see it is still highly significant with p=0.000000000004312. But not as significant as the ANOVA above. 我們看到它仍然高度顯著，p=0.000000000004312。但不如上面的變異數分析顯著。\n\n\n4.5.2 Two-way ANOVA\nThere are many occasions when our dataset has more than one grouping variable, especially in experimental biology/ecology where we might use simulation experiments to understand ecological processes. Have a look at the work of Dr Mark Ledger, who manages GEES’s EcoLaboratory facility on campus for excellent examples ofthis. 在許多情況下，我們的資料集會包含多個分組變量，尤其是在實驗生物學/生態學中，我們可能會使用模擬實驗來理解生態過程。您可以參考馬克‧萊傑博士（Dr Mark Ledger）的研究成果，他負責管理GEES校園內的生態實驗室設施，其中就有一些絕佳的例子。\nWe will use a simple two-way example to illustrate the application of multiple grouping factors but there are very many variants of these types of ANOVA designs. Please look at chapters 12-14 in Logan (2010) for the range of possibilities. 我們將使用一個簡單的雙向範例來說明多分組因子的應用，但這類變異數分析設計有許多變異。請參閱@Logan2010的第12-14章，以了解各種可能性。\nThe data set we need for this is called ‘Grazing.csv’.\n\nRye &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/Grazing.csv\")\n\nRows: 18 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Field, Grazing\ndbl (1): Abund\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLook at it. The data relates to how rye grass abundance varies in relation to two variables, the level of grazing (Grazing) and the location of the sample plots in the field (Field). 看看這個。這些數據與黑麥草豐度如何隨兩個變數而變化有關，即放牧水平（放牧）和田地中樣地的位置（田地）。\n\nglimpse(Rye)\n\nRows: 18\nColumns: 3\n$ Abund   &lt;dbl&gt; 9, 11, 6, 14, 17, 19, 28, 31, 32, 7, 6, 5, 14, 17, 15, 44, 38,…\n$ Field   &lt;chr&gt; \"Top\", \"Top\", \"Top\", \"Top\", \"Top\", \"Top\", \"Top\", \"Top\", \"Top\",…\n$ Grazing &lt;chr&gt; \"Low\", \"Low\", \"Low\", \"Mid\", \"Mid\", \"Mid\", \"High\", \"High\", \"Hig…\n\nskim(Rye)\n\n\nData summary\n\n\nName\nRye\n\n\nNumber of rows\n18\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nField\n0\n1\n3\n5\n0\n2\n0\n\n\nGrazing\n0\n1\n3\n4\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAbund\n0\n1\n19.44\n12.41\n5\n9.5\n16\n30.25\n44\n▇▇▁▂▃\n\n\n\n\n\nTwo-way ANOVA with the default error structure requires the grouping factors to be balanced. We check for this using the table() function. 採用預設誤差結構的雙因子變異數分析要求分組因子保持平衡。我們使用 table() 函數來檢查這一點。\n\ntable(Rye$Field,Rye$Grazing)\n\n       \n        High Low Mid\n  Lower    3   3   3\n  Top      3   3   3\n\n\nThe results show that we have the same number of replicate samples i.e. it is balanced so we can proceed. The next step is to examine some of the patterns, as they are factors we’ll do this with some boxplots (Fig. 4.13). 結果表明，我們擁有相同數量的重複樣本，即平衡，因此我們可以繼續進行。 下一步是檢查一些模式，因為它們是因子，我們將使用一些箱線圖來做到這一點（圖 4.13）。\n\n# Boxplots to look at difference of factors - we are using base R graphics here. They are a little quicker to generate. For nice plots for projects and publications we'd use ggplot (you have the code to do this already)\npar(mfrow = c(1,2))\nboxplot(Abund ~ Field, data = Rye)\nboxplot(Abund ~ Grazing, data = Rye)\n\n\n\n\n\n\n\n# rest the graphics\npar(mfrow = c(1,1))\n\nFig. 4.13: Boxplot of Rye grass dataset\nEach plot shows a different factor. You can see abundance is variable between the lower and top parts of the field. There is also a difference between the levels of grazing, with some variability within the groups. We can see that the grazing groups are ordered alphanumerically, which makes no sense; we want to order by the magnitude of grazing, Low - Mid - High. 每個圖都顯示了不同的因素。您可以看到，田地下部和上部的豐度有差異。不同放牧程度之間也存在差異，不同組別內也存在一些差異。我們可以看到，放牧組是按字母數字順序排列的，這沒有道理；我們希望按放牧強度排序，即低 - 中 - 高。\n\n# Reorder the categorical variable. R plots factorial variables alphabetically - not useful when your factors relate to a quantity of something!!!!\nRye$Grazing &lt;- factor(Rye$Grazing, levels = c(\"Low\", \"Mid\", \"High\"), ordered = TRUE)\n\nThe code to run the 2-way ANOVA is similar but includes one more term. We will run the straight 2-way ANOVA.\n\nM2 &lt;- aov(Abund ~ Grazing + Field, data = Rye)\n\nLet’s look at the summary table.\n\nsummary(M2)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGrazing      2 2403.1  1201.6   84.48 1.54e-08 ***\nField        1   14.2    14.2    1.00    0.334    \nResiduals   14  199.1    14.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou see that there is an additional row: one for each factor (Grazing and Field). The results show a significant difference between the Grazing levels with a p-value of 1.54e-08. There is no difference between the different Field locations. When dealing two variables we need to check for one additional thing, whether there is an interaction between the two factors. We can do this graphically using an interaction plot (Fig. 4.14). The function is interaction.plot(). This can be done in ggplot if you want a nicer looking result but it requires a lot more code. 您會看到多出了一行：每個因素（放牧和田地）各一行。結果顯示，放牧水平之間存在顯著差異，p值為1.54e-08。田地不同位置之間沒有差異。處理兩個變數時，我們還需要檢查另一項：兩個因素之間是否有交互作用。我們可以使用交互作用圖（圖4.14）以圖形方式執行此操作。函式是“interaction.plot()”。如果您想要更美觀的結果，也可以使用ggplot來實現，但這需要更多的程式碼。\n\ninteraction.plot(Rye$Grazing, Rye$Field, Rye$Abund, col=c(2,3), xlab = \"Grazing Regime\", ylab = \"Rye Grass abundance\", trace.label = \"Field\")\n\n\n\n\n\n\n\n\nFig. 4.14: An interaction plot for the Rye grass ANOVA model\nThe lines here converge initially and eventually cross. The plot shows that rye grass abundance is higher in the lower field at higher levels of grazing intensity (this is the interaction effect). We can test for it statistically using an interaction term in the aov model. 此處的線條最初匯合，最終相交。此圖顯示，在較高的放牧強度下，低地黑麥草豐度較高（這是交互效應）。我們可以使用「aov」模型中的交互項對此進行統計檢定。\n\nM3 &lt;- aov(Abund ~ Grazing * Field, data = Rye)\n\nWe are just replacing the + symbol with a *. The summary call reveals the results. 我們只是用 * 取代了 + 符號。摘要呼叫會顯示結果。\n\nsummary(M3)\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGrazing        2 2403.1  1201.6 207.962 4.86e-10 ***\nField          1   14.2    14.2   2.462  0.14264    \nGrazing:Field  2  129.8    64.9  11.231  0.00178 ** \nResiduals     12   69.3     5.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction effect is shown as third row in the results. And it is significant, p=0.00178. The last thing we need to do is validate the model with the plot() function (Fig. 4.15). 交互效應顯示在結果的第三行。它顯著，p=0.00178。最後，我們需要使用 plot() 函數來驗證模型（圖 4.15）。\n\npar(mfrow = c(2,2)) # set the graphics parameter\nplot(M3)\n\n\n\n\n\n\n\npar(mfrow = c(1,1)) # reset the parameter.\n\nFig. 4.15: Validation plots for the two-way Rye grass ANOVA model\nThe plots look good, with no clear patterns, despite the variability shown in the boxplots! Had we seen issues in our plot we could use permutation ANOVA, as a form of non-parametric two-way comparisons. This method uses computational resampling to build a distribution and calculate p-values without assuming normality. The lmPerm package provides the aovp() function, which is a ‘drop-in’ replacement for the standard aov(). 儘管箱線圖顯示了變量，但這些圖看起來不錯，沒有明顯的模式！如果我們發現圖中有問題，可以使用置換變異數分析（Permutation ANOVA）作為非參數雙向比較方法。此方法使用計算重採樣來建立分佈並計算 p 值，而無需假設正態性。 “lmPerm”套件提供了“aovp()”函數，它是標準“aov()”函數的“嵌入式”替代品。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#class-exercises",
    "href": "chap4.html#class-exercises",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.6 Class Exercises",
    "text": "4.6 Class Exercises\nDo the following for all data files:\n\nLoad in the data and examine it’s structure (including experimental balance)\nPosit your hypotheses (null and alternative)\nplay with some pictures\ndraw a contingency table to figure out how the data are structured\nSelect an appropriate ANOVA-based model (NOTE: we have more than 2 samples…so no t tests!)\nValidate the model\nBriefly interpret the results - in your script file!!!!\n\n對所有資料檔案執行以下操作：\n\n載入資料並檢查其結構（包括實驗平衡）\n提出假設（零假設和備擇假設）\n玩圖片\n繪製列聯表以了解資料的結構\n選擇合適的基於變異數分析的模型（注意：我們的樣本超過 2 個…因此無法進行 t 檢定！）\n驗證模型\n簡要解釋結果 - 在您的腳本文件中！ ！ ！ ！\n\n\n4.6.1 EXERCISE 1\n\nFilename: Hoglouse.csv - a file of water louse distribution along a rivers in Devon response - hoglouse numbers.\nFile contents: explanatory variable - Upper, Mid and lower sites (i.e. longitudinal profile).\n檔案名稱：Hoglouse.csv - 德文郡某河流水蝨分佈狀況文件 - 豬蝨數量\n檔案內容：解釋變數 - 上游、中游和下游站點（即縱向剖面）\n\n\n\n4.6.2 EXERCISE 2\n\nFilename: Medley.csv (from Medley and Clements (1998) investigated the impact of zinc contamination (and other heavy metals) on the diversity of diatom species in the USA Rocky Mountains (from Box 8.1 of Quinn and Keough (2002).\nFile contents: DIATOM - number of different species of diatoms on the a known area of rocks in streams (continous variable). ZINC - mpm of zinc in the water column (background, low, medium, high) (factor - explanatory variable).\n文件名：Medley.csv（源自 Medley 和 Clements (1998) 研究的鋅污染（及其他重金屬）對美國落基山脈矽藻物種多樣性的影響（源自 Quinn 和 Keough (2002) 的 Box 8.1）\n文件內容：DIATOM - 溪流中已知岩石區域內不同種類矽藻的數量（連續變數）。 ZINC - 水體中鋅的含量（單位：毫升/分鐘）（背景、低、中、高）（因子 - 解釋變數）\n\n\n\n4.6.3 EXERCISE 3\n\nFilename: Quinn.csv (data from By G. P. Quinn and M. J. Keough, 2002 - “Experimental Design and Data Analysis for Biologists” ).\nFile contents: DENSITY - urchin density treatment (L1 = 8 individuals per 225 cm2 enclosure, L2 = 15, L3 = 30 and L4 = 45) (factor - explanatory variable). SEASON - Season of the year (Spring or summer) (factor - explanatory variable). EGGS - egg production by limpets (continuous response variable).\n檔案名稱：Quinn.csv（資料來自 G. P. Quinn 和 M. J. Keough，2002 年出版的《生物學家實驗設計與資料分析》）\n文件內容：密度 - 海膽密度處理（L1 = 每 225 平方公分圍欄 8 只，L2 = 15 只，L3 = 30 只，L4 =​​ 每 45 隻）（因子 - 解釋變數）。季節 - 季節（春季或夏季）（因子 - 解釋變數）。卵 - 帽貝產卵量（連續反應變數）\n\n\n\n4.6.4 EXERCISE 4 [WILL BE YOUR FORMATIVE EXERCISE]\n\nFilename: fish_pred.csv (data from Doncaster and Davey, 2002, p. 50) fish predation experiment - using enclosures of two species of fish to assess their impact on predation on chironomids.\nFile contents: Density - density of chironomids left activity fish predation exercise (response variable - continuous). Loach - Presence of a loach in the enclosure (factor - 0 = absent, 1 = present). Bullhead - Presence of a bullhead in the enclosure (factor - 0 = absent, 1 = present).\n檔案名稱：fish_pred.csv（資料來自 Doncaster 和 Davey，2002，第 50 頁）魚類捕食實驗 - 使用兩種魚類的圍欄來評估它們對搖蚊捕食的影響。\n檔案內容：密度 - 搖蚊在活動性魚類捕食練習中的密度（反應變數 - 連續型）。泥鰍 - 圍欄內是否有泥鰍（因數 - 0 = 不存在，1 = 存在）。牛頭魚 - 圍欄內是否有牛頭魚（因子 - 0 = 不存在，1 = 存在）。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#next-week",
    "href": "chap4.html#next-week",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.7 Next Week",
    "text": "4.7 Next Week\nWe will introduce you to linear regression in R. We will revisit many of the plotting and model validation functions you have examined today, so do not be overwhelmed. Things will become easier and clearer as the module progresses! 我們將向您介紹 R 中的線性迴歸。我們將重新回顧您今天學習的許多繪圖和模型驗證函數，所以不要感到不知所措。隨著模組的進展，事情會變得越來越簡單和清晰！",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#follow-up-work",
    "href": "chap4.html#follow-up-work",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.8 Follow-up work",
    "text": "4.8 Follow-up work\n\nComplete the class exercises if you haven’t already done so.\nUnderstanding how to interpret means tests errors is an important skill. This wonderful paper by Cumming, Fidler, and Vaux (2007) will assist you. Please make sure you read it.\nFor a more in depth understanding to the wide variety of ANOVA type models refer to chapters 12-14 in Logan (2010). We have online access to this book.\nRevisit the section on linear regression in chapter 5 of Beckerman, Childs, and Petchey (2017) in preparation for next week’s class.\n如果您還沒有完成課堂練習，請完成。\n了解如何解讀平均值檢定誤差是一項重要技能。 Cumming, Fidler, and Vaux (2007) 的這篇精彩論文將對您有所幫助。請務必閱讀。\n如需更深入了解各種變異數分析類型的模型，請參閱 Logan (2010) 的第 12-14 章。我們可以在線訪問這本書。\n重溫 Beckerman, Childs, and Petchey (2017) 第五章關於線性迴歸的部分，為下週的課程做準備。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap4.html#references",
    "href": "chap4.html#references",
    "title": "4  Comparing groups: Means and related tests",
    "section": "4.9 References",
    "text": "4.9 References\n\n\n\n\nBeckerman, Andrew, Dylan Childs, and Owen Petchey. 2017. Getting Started with R: An Introduction for Biologists. 2nd. ed. Oxford: Oxford University Press.\n\n\nCumming, Geoff, Fiona Fidler, and David L. Vaux. 2007. “Error Bars in Experimental Biology.” The Journal of Cell Biology 177 (1). https://doi.org/10.1083/jcb.200611141.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Sage Publications.\n\n\nHothorn, Torsten, Kurt Hornik, Mark A. van de Wiel, and Achim Zeileis. 2008. “Implementing a Class of Permutation Tests: The Coin Package.” Journal of Statistical Software 28 (November): 1–23. https://doi.org/10.18637/jss.v028.i08.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using R : A Practical Guide. Chichester: Wiley and Sons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing groups: Means and related tests</span>"
    ]
  },
  {
    "objectID": "chap5.html",
    "href": "chap5.html",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "",
    "text": "5.1 Outline:\nThis week we will explore linear regression in R. A fundamental understanding of this is critical for the more complex regression techniques we we explore in the coming weeks. Today we will cover:\n本周我们将探索 R 语言中的线性回归。对 R 语言的基本理解对于我们接下来几周探索更复杂的回归技术至关重要。今天我们将涵盖：",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#outline",
    "href": "chap5.html#outline",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "",
    "text": "Correlation between variables.\nBasic theory of linear regression.\nIts application in R using simple data sets.\nHow to understand the regression coefficient outputs.\nCreating a graphic to display the results.\n\n\n\n变量间的相关性。\n线性回归的基本理论。\n如何在 R 语言中使用简单数据集进行线性回归分析。\n如何理解回归系数的输出。\n创建图表来展示结果。\n\n\n5.1.1 Essential Reading:\n\nChapter 5 in Beckerman, Childs, and Petchey (2017).\nChapter 7 Field, Miles, and Field (2012) Discovering Statistic using R. Sage Publications.\nZuur and Ieno (2016) ‘A Protocol for Conducting and Presenting Results of Regression-Type Analyses’. Methods in Ecology and Evolution 7, no. 6 (2016): 636–45. https://doi.org/10.1111/2041-210X.12577. This paper presents an excellent framework outlining what you should present as results from regression analyses.\nBeckerman, Childs, and Petchey (2017) 的第 5 章。\nField, Miles, and Field (2012) 的第 7 章：使用 R 語言探索統計資料。 Sage 出版品。\nZuur and Ieno (2016)：「迴歸分析結果的執行與呈現方案」。 《生態與演化方法》7，第 6 期 (2016): 636–45。 https://doi.org/10.1111/2041-210X.12577 本文提出了一個優秀的框架，概述了迴歸分析結果的呈現方式。\n\n\n\n5.1.2 Session Aim:\nTo understand linear regression and its application in R.\n\n\n5.1.3 Learning Outcomes:\nUsing lm() function to:\n\nUndertake linear regression in R with one continuous variable and one factor variable.\nEvaluate the summary results table.\nUse the plot() function to validate the model.\n\n使用 lm() 函数：\n\n在 R 语言中对一个连续变量和一个因子变量进行线性回归分析。\n计算结果汇总表。\n使用 plot() 函数验证模型。\n\n\n\n5.1.4 Today’s Session\n\n\n5.1.5 Create your code file\nCreate a new codefile and call it something memorable that links to the week and it the content of the workshop and save to your ‘Codefiles directory’.\n创建一个新的代码文件，并给它起一个容易记住的名字，链接到本周的内容，并将其保存到您的“代码文件目录”中。\n\n\n5.1.6 Load required libraries\nMost of what we need today is the base R installation but we need to install four new packages:\n\nThe corrplot package (Wei and Simko 2024) which provides visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables: https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html.\nThe car package (Fox and Weisberg 2019) which provides a run of functions to support applied regression: https://cran.r-project.org/web/packages/car/index.html.\nThe skimr package (Waring et al. 2022) which provides simple functions to commute commonly use descriptive data functions: https://docs.ropensci.org/skimr/.\nThe broom package Robinson, Hayes, and Couch (2024) which creates ‘tidy’ dataframes from model objects: https://CRAN.R-project.org/package=broom.\n\nLoad packages and install those that are not installed.\n我们今天所需的大部分内容是基础的 R 安装，但我们需要安装四个新软件包：\n\ncorrplot 软件包 (Wei and Simko 2024)，它提供了一个基于相关矩阵的可视化探索工具，支持自动变量重新排序，以帮助检测变量之间的隐藏模式：https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html。\ncar 软件包 (Fox and Weisberg 2019)，它提供了一系列支持应用回归的函数：https://cran.r-project.org/web/packages/car/index.html。\nskimr 软件包 (Waring et al. 2022) 提供了一些简单的函数来转换常用的描述性数据函数：https://docs.ropensci.org/skimr/。\n\n加载软件包并安装尚未安装的软件包。\n\nbroom 套件 (Robinson, Hayes, and Couch 2024) 從模型物件建立「整潔」資料框：https://CRAN.R-project.org/package=broom。\n\n\n# Load libraries and install packages that are not is our system\n# List of packages\npackages &lt;- c(\"tidyverse\", \"corrplot\", \"car\",\"skimr\", \"broom\")\n\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\ncorrplot 0.95 loaded\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\n[[1]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n [7] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"     \"graphics\" \n[13] \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[2]]\n [1] \"corrplot\"  \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"    \n [7] \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"car\"       \"carData\"   \"corrplot\"  \"lubridate\" \"forcats\"   \"stringr\"  \n [7] \"dplyr\"     \"purrr\"     \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"  \n[13] \"tidyverse\" \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[19] \"methods\"   \"base\"     \n\n[[4]]\n [1] \"skimr\"     \"car\"       \"carData\"   \"corrplot\"  \"lubridate\" \"forcats\"  \n [7] \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tidyr\"     \"tibble\"   \n[13] \"ggplot2\"   \"tidyverse\" \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[19] \"datasets\"  \"methods\"   \"base\"     \n\n[[5]]\n [1] \"broom\"     \"skimr\"     \"car\"       \"carData\"   \"corrplot\"  \"lubridate\"\n [7] \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tidyr\"    \n[13] \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"     \"graphics\"  \"grDevices\"\n[19] \"utils\"     \"datasets\"  \"methods\"   \"base\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#simple-correlation",
    "href": "chap5.html#simple-correlation",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.2 Simple correlation",
    "text": "5.2 Simple correlation\nWe will start by introducing you to correlation which looks at the direction and strength of association between variables. The two functions we need are cor() and cor.test(). We are doing this because when we move to multivariate regression next week it is important that we understand the relationship between our explanatory variables before we run the regression. I’ll explain more next week.\n我們將首先介紹相關性，它考察變數之間關聯的方向和強度。我們需要兩個函數：cor() 和 cor.test()。我們這樣做是因為，下週我們將學習多元迴歸，在進行迴歸分析之前，了解解釋變數之間的關係非常重要。下週我會更詳細地解釋。\nR lets you access the Pearson or Product Moment Correlation (https://www.statsref.com/HTML/?pearson_product_moment_correla.html), a parametric tool and Spearman’s Rank Order Correlation, a non-parametric version. The cor() function uses the former as the default.\nWe need out datafile. We’ll load it, it is called: nelson.csv.\nR 允許您使用皮爾遜相關係數（或稱積矩相關係數）(https://www.statsref.com/HTML/?pearson_product_moment_correla.html)，這是一個參數化工具，以及斯皮爾曼秩相關係數（非參數化工具）。 cor() 函數預設使用前者。\n我們需要一個資料檔。我們將載入它，它的名稱是：nelson.csv。\n\nbeetle &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/nelson.csv\")\n\nRows: 9 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): HUMIDITY, WEIGHTLOSS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis is an experiment on 9 batches of flour beetles assessing their weight loss measured in mg at different humidity levels ranging from 0-93% humidity. The experiment lasted 6 days. It a simple file with 9 rows and 2 column (or variables), no missing values and both variables are numerical (dbl).\n這是一項針對9批麵粉甲蟲的實驗，評估它們在0-93%濕度範圍內不同濕度水平下的重量損失（以毫克為單位）。實驗持續了6天。這是一個包含9行2列（或變數）的簡單文件，沒有缺失值，並且兩個變數都是數值型變數（“dbl”）。\n\nglimpse(beetle)\n\nRows: 9\nColumns: 2\n$ HUMIDITY   &lt;dbl&gt; 0.0, 12.0, 29.5, 43.0, 53.0, 62.5, 75.5, 85.0, 93.0\n$ WEIGHTLOSS &lt;dbl&gt; 8.98, 8.14, 6.67, 6.08, 5.90, 5.83, 4.68, 4.20, 3.72\n\nskim(beetle)\n\n\nData summary\n\n\nName\nbeetle\n\n\nNumber of rows\n9\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHUMIDITY\n0\n1\n50.39\n32.21\n0.00\n29.50\n53.0\n75.50\n93.00\n▅▂▅▂▇\n\n\nWEIGHTLOSS\n0\n1\n6.02\n1.74\n3.72\n4.68\n5.9\n6.67\n8.98\n▆▁▇▁▃\n\n\n\n\n\nWe will run a Pearson or Product Moment Correlation on the data.\n\n# Runs a Pearson's correlation\ncor(beetle$WEIGHTLOSS, beetle$HUMIDITY)\n\n[1] -0.9871523\n\n\nPearson’s correlations (\\(r\\)) range between 1 and -1 both ends suggesting a perfect linear either positive (in the case of a 1) or negative (in the case of a -1) relationship. We can see we have a strong negative relationship (\\(r\\)=-0.987), which suggests that beetle weightloss is lower where humidity is higher. We can test whether this pattern is statistically significant using the cor.test() function.\n皮爾森相關係數 (\\(r\\)) 介於 1 和 -1 之間，表示兩者之間存在完美的線性關係，要麼為正（1 的情況），要麼為負（-1 的情況）。我們可以看到，兩者之間有強烈的負相關關係 (\\(r\\)=-0.987)，這表示濕度越高，甲蟲的減重就越少。我們可以使用 cor.test() 函數來檢驗這種模式是否具有統計顯著性。\n\ncor.test(beetle$WEIGHTLOSS, beetle$HUMIDITY)\n\n\n    Pearson's product-moment correlation\n\ndata:  beetle$WEIGHTLOSS and beetle$HUMIDITY\nt = -16.346, df = 7, p-value = 7.816e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9973935 -0.9379224\nsample estimates:\n       cor \n-0.9871523 \n\n\nThis tests for significance using a t-test (t = -16.346) and returns a significant outcome with a p-value of 7.816e-07. This means it is a highly significant finding. To see the p-value in full we merely need to move the decimal point 7 steps to the left of the decimal place (p=0.0000007816), put another we add 6 zeros after the decimal place!!! The test also returns the 95% confidence intervals which capture the range of values of which there is a defined probability that the coefficient falls within.\n這使用「t檢定」（t = -16.346）進行顯著性檢驗，結果顯示p值為7.816e-07，顯著。這意味著這是一個高度顯著的發現。要查看完整的p值，我們只需將小數點向小數點左側移動7位（p=0.0000007816），並在小數點後面添加6個零即可！該檢定也傳回95%的置信區間，該區間捕捉了係數落入的機率確定的數值範圍。\nPearson’s correlation is only appropriate when we believe the relationship between the two variables is linear. As we will discuss later on today, this is also the case for linear regression. If we think the pattern is not linear then we use a Spearman’s rank correlation. This statistic is generated using the rank order of the observations which means it is also suitable for large-scale ordinal explanatory variables.\n皮爾遜相關僅適用於我們認為兩個變數之間的關係是線性的。正如我們今天稍後將討論的，線性迴歸也是如此。如果我們認為模式不是線性的，則使用斯皮爾曼秩相關。這個統計數據是根據觀測值的秩產生的，這意味著它也適用於大規模有序解釋變數。\nTASK: Now take a few minutes to create an XY plot in using ggplot2 that shows the relationship between the weight loss of the beetles (WEIGHTLOSS) and humidity (HUMIDITY). HINT: Look at the code in Week 2 if you cannot remember how to do this [~ 5 min].\n任務：現在花幾分鐘時間使用 ggplot2 創建一個 XY 圖，展示甲蟲體重減輕 (WEIGHTLOSS) 和濕度 (HUMIDITY) 之間的關係。提示：如果您記不住如何操作，請查看第 2 週的代碼 [~ 5 分鐘]。\nIt should look like this (Fig. 5.1):\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFig. 5.1: Scatterplot of the flour beetles relationship\nYou see a strongly linear relationship. We will revisit this dataset below and explore what the results mean. But first let’s work through a bit of statistical theory goodness.\n你會看到一種強線性關係。我們將在下文中重新檢視這個資料集，並探討其結果的意義。不過，首先讓我們先來了解一下統計理論。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#what-is-linear-regression",
    "href": "chap5.html#what-is-linear-regression",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.3 What is linear regression?",
    "text": "5.3 What is linear regression?\nAt its most simple, linear regression tests for relationship between a response (or dependent) variable and explanatory (or independent variable). The relationship is typically illustrated as a scatterplot graph with the response variable shown on the Y axis and the explanatory variable (or variables) shown on the X axis. Modelling data in this way serves two purposes (Ismay, Kim, and Valdivia 2025):\n\nModelling for explanation, where it serves to quantify and / or describe the relation between the response variable \\(y\\) and a set of explanatory variables \\(x\\) to determine the statistical significance of the relationship.\nModelling for prediction, when you want to predict the value of \\(y\\) with a set of explanatory variables \\(x\\).\n\n最简单的线性回归检验是检验响应（或因变量）变量与解释（或**自变量）之间的关系。这种关系通常用散点图来表示，响应变量显示在 Y 轴上，解释变量（或多个解释变量）显示在 X 轴上。以这种方式建模数据有两个目的 (Ismay, Kim, and Valdivia 2025)：\n\n解释建模，用于量化和/或描述响应变量 \\(y\\) 与一组解释变量 \\(x\\) 之间的关系，以确定该关系的统计显著性。\n预测建模，用于用一组解释变量 \\(x\\) 预测 \\(y\\) 的值。\n\nThese two things are not mutually exclusive and you may wish to do both things i.e. model an ecological process and then apply or generalise it to elsewhere, using say a Species Distribution Model (SDM).\nThe ‘shape’ of the regression relationship is shown by a regression line. As you may recall from your school mathematics the equation that describes a straight line: \\[\nY = mx + c\\]\nWhere, \\(Y\\) is our response variable, \\(x\\) is our explanatory variable, \\(m\\) is a variable indicating the slope of the line and, \\(c\\) is a constant capturing the intercept, the point at which the regression line crosses the Y axis. We can abstract this using statistical notations as:\n\\[Y \\sim X\\beta_0 + X\\beta_1\\]\nWhere, Y is our response and X, our explanatory variable, has two regression (or beta) terms associated with it:\n\n\\(X\\beta_{0}\\) is the term for the intercept and,\n\\(X\\beta_{1}\\) is the slope term.\n\nBoth these terms are known as regression coefficients and will be visible in the tabular output of any lm() model call in R. Let’s explore how to run regression analyses in R using one response variable and one explanatory variable, which represents the most simple case.\n这两件事并不相互排斥，你或许希望同时做这两件事。例如，你可能想测试一个响应变量 \\(y\\)（例如患有呼吸系统疾病的人数）是否可以由他们是否居住在交通繁忙的道路附近来解释（我们的 \\(x\\) 变量）。完成此操作后，你可能希望使用回归结果（一个方程）来预测道路密度越高，呼吸系统疾病的发病率就越高。\n回归关系的“形状”由回归线表示。你可能还记得学校数学课上描述直线的方程：\\[\nY = mx + c\\]\n其中，\\(Y\\) 是响应变量，\\(x\\) 是解释变量，\\(m\\) 是表示直线斜率的变量，\\(c\\) 是一个常数，表示截距，即回归线与 Y 轴的交点。我们可以用统计符号将其抽象为：\n\\[Y \\sim X\\beta_0 + X\\beta_1\\]\n其中，Y 是响应变量，X 是解释变量，它有两个相关的回归（或 beta）项：\n\n\\(X\\beta_{0}\\) 是截距项，\n\\(X\\beta_{1}\\) 是斜率项。\n\n这两个项被称为回归系数，它们会在 R 中任何 lm() 模型调用的表格输出中显示。让我们探索如何在 R 中使用一个响应变量和一个解释变量进行回归分析，这代表了最简单的情况。\n\n5.3.1 Linear regression with one numerical explanatory variable\nLet’s return to the beetle dataset to work through basic linear regression.\nWhen undertaking regression analyses we need to work through a sequence of steps:\nStep 1. Load in and tidy the data.\nStep 2. Undertake some exploratory data analysis (EDA).\nStep 3. Run the regression analysis.\nStep 4. Examine and interpret the regression tables.\nStep 5. Plot the a graph that displays relationship and generate the regression equation.\nStep 6. Validate the regression model.\n进行回归分析时，我们需要完成一系列步骤：\n步骤 1. 加载并整理数据。\n步骤 2. 进行一些探索性数据分析 (EDA)。\n步骤 3. 运行回归分析。\n步骤 4. 检查并解释回归表。\n步骤 5. 绘制显示关系的图表并生成回归方程。\n步骤 6. 验证回归模型（我们将在下周讨论此部分）。\nWe have already loaded the data and decided what variables to use (so step 1 is done!). As you may recall from the discussion in the week 3 workshop, we should start our work using some Exploratory Data Analysis (EDA). The steps are (Zuur, Ieno, and Elphick 2010):\n\nExamine the raw data.\nCompute summary statistics, e.g. means, medians, counts of missing values and so on.\nCreate some visualisations to look at the shape of relationships and consider dependencies (the latter especially where we have more than one explanatory variable).\n\n我們已經加載了數據並確定了要使用的變數（所以第一步完成了！）。你可能還記得第三週研討會上的討論，我們應該使用一些探索性資料分析 (EDA) 來開始我們的工作。步驟如下：\n\n檢查原始資料。\n計算總計統計數據，例如平均數、中位數、缺失值計數等等。\n建立一些視覺化圖表來查看關係的形狀並考慮依賴關係（後者特別適用於有多個解釋變數的情況）。\n\nWe have already used the glimpse() and skim() functions to look at the data so we don’t need to revisit this.\n我們已經使用 glimpse() 和 skim() 函數來查看數據，因此我們不需要重新存取它。\nWe are going to use a new function scatterplot() from the car package to from basic visualisation. It is not only quick, but it also provides some useful additional visualisation (Fig. 5.2). We format this R command like we would a regression model (see below) by using WEIGHTLOSS ~ HUMIDITY and adding in a data argument that identifies the dataframe where the data are situated (in this case, beetle).\n我們將使用“car”套件中的新函數“scatterplot()”來實現基本的可視化。它不僅速度快，還能提供一些實用的附加視覺化效果（圖 5.2）。我們像處理回歸模型（見下文）一樣格式化此 R 命令，使用“WEIGHTLOSS ~ HUMIDITY”，並添加資料參數來識別資料所在的資料框（在本例中為 beetle）。\n\ncar::scatterplot(WEIGHTLOSS ~ HUMIDITY, data = beetle,\n    xlab = \"Relative Humidity\", # add an x label\n    ylab = \"Weightloss\") # add a y label\n\n\n\n\n\n\n\n\nFig. 5.2: car generated scatterplot of beetle weightloss due to changes in humidity\nThis plot shows a number of things:\n\nthe solid blue line is the linear regression line.\nthe hashed blue line is a loess smoother frequent used to show non linear patterns.\nthe fill shows the confidence intervals (95% and 5%) either side of the loess smoother. If we look at the output you can see a large spread of data points around the blue solid line.\nthe boxplots plots on the \\(x\\) and \\(y\\) axes show the variability of the each data set.\n\n這張圖展示了一些資訊：\n\n藍色實線是線性迴歸線。\n藍色虛線是 Loess 平滑線，常用於顯示非線性模式。\n填充部分顯示了 Loess 平滑線兩側的置信區間（95% 和 5%）。如果我們查看輸出結果，可以看到藍色實線周圍有大量資料點分佈。\nx 軸和 y 軸上的箱線圖顯示了每個資料集的變異性。\n\nLook at the spread of the points around the regression line. We can now see why the correlation coefficient is so high ! We can also see that the linear regression line matches the loess smoother which tells us that the data are likely linearly associated. The boxplots provide additional information. We see that both variables are likely distributed normally and we have no potential outlier points. You should have already created the ggplot earlier in this session.\n觀察迴歸線周圍點的分佈。現在我們就能明白為什麼相關係數這麼高了！我們還可以看到線性迴歸線與黃土平滑線相匹配，這表明數據很可能呈線性相關。箱線圖提供了更多資訊。我們發現兩個變數都呈現常態分佈，且沒有潛在的異常點。你應該已經在本節早些時候創建了ggplot圖。\nWe can measure the difference between the ‘best fit’ regression line and data points to provide the residuals (Fig. 5.3).\n Fig. 5.3: How residuals are measured in a linear regression (Source: Field et al. 2014). The points under the regression line are -ve residuals, while those above it are +ve residuals.\nWhile the residuals are used to generate the regression output, they are also a measure of noise and error (just like ANOVA, which is also a linear technique!). We thus need to update our regression equation to account for them. So the equation becomes:\n\\[Y \\sim X\\beta_0 + X\\beta_1 + \\epsilon\\] Where, Y is our response and X, our explanatory variable, has two beta terms associated with it:\n\n\\(X\\beta_{0}\\) is the term for the intercept and\n\\(X\\beta_{1}\\) is the slope term\n\\(\\epsilon\\) (epsilon) is the additional term. It is the error term indicating the residuals of the data points.\n\n虽然残差用于生成回归输出，但它们也是噪声和误差的度量。因此，我们需要更新回归方程以考虑这些因素。因此，方程变为：\n\\[Y \\sim X\\beta_0 + X\\beta_1 + \\epsilon\\] 其中，Y 是响应变量，X 是解释变量，它包含两个 beta 项：\n\n\\(X\\beta_{0}\\) 是截距项，\n\\(X\\beta_{1}\\) 是斜率项，\n\\(\\epsilon\\) (epsilon) 是附加项，表示数据点残差的误差项。\n\n\n\n5.3.2 Linear Regression (under the hood)\nLinear regression finds the best possible straight line through a set of data points using a method called the sum of least squares. In simple terms, this method tests countless possible lines and selects the one that has the smallest overall distance to all the data points. To measure this distance, it calculates the “residuals”—the vertical gap between each observed data point and the line (Fig. 5.4).\nBecause some residuals are positive (above the line) and some are negative (below), we can’t just add them up. Instead, we square each residual to make them all positive. Adding all these squared values together gives us the sum of squared differences (SS). A small SS value means the line is a good fit for the data; a large SS value means it’s a poor fit.\n\\[deviation = \\Sigma(observed - model)^2\\]\nTo know if our regression model is actually useful, we need to compare it to a baseline. The most basic model imaginable is just a horizontal line at the mean (average) of our response variable, \\(y\\). We calculate the total variation in the data by finding the sum of squared differences between each data point and this mean line. This is called the Total Sum of Squares (SS\\(_T\\)).\nNext, we calculate the sum of squared residuals for our actual regression line. This is the Residual Sum of Squares (SS\\(_R\\)), which represents the error our model couldn’t explain.\n线性回归使用一种称为最小二乘和的方法，通过一组数据点找到最佳直线。简单来说，该方法测试无数条可能的直线，并选择与所有数据点总距离最小的那条。为了测量这个距离，它会计算“残差”，即每个观测数据点与直线之间的垂直间隙（图 5.4）。\n由于有些残差为正（位于直线上方），有些为负（位于直线下方），我们不能简单地将它们相加。相反，我们会对每个残差求平方，使它们都为正。将所有这些平方值相加，就得到了平方差和 (SS)。较小的 SS 值表示直线与数据的拟合度良好；较大的 SS 值表示拟合度较差。\n\\[deviation = \\Sigma(observed - model)^2\\]\n要了解我们的回归模型是否真的有用，我们需要将其与基线进行比较。最基本的模型，其实就是在响应变量 \\(y\\) 的均值处画一条水平线。我们通过计算每个数据点与这条均值线之间的平方差之和来计算数据的总变异。这被称为总平方和 (SS\\(_T\\))。\n接下来，我们计算实际回归线的残差平方和。这就是残差平方和 (SS\\(_R\\))，它表示我们的模型无法解释的误差。\nBy comparing these two, we can see how much better our regression model is than just using the average. The difference between them (SS_T - SS_R) is called the Model Sum of Squares (SS\\(_M\\)). A large SS\\(_M\\) means our regression model is a significant improvement over the basic mean, indicating it does a good job of explaining the relationship in the data. A small SS\\(_M\\) means our model isn’t much better than just guessing the average. Figure 5.5 shows this relationship graphically.\n通过比较这两个值，我们可以看到我们的回归模型比仅仅使用平均值要好得多。它们之间的差值（SS_T - SS_R）称为模型平方和（SS\\(_M\\)）。较大的 SS\\(_M\\) 表示我们的回归模型比基本平均值有显著的改进，表明它能够很好地解释数据中的关系。较小的 SS\\(_M\\) 表示我们的模型并不比仅仅猜测平均值好多少。Fig. 5.5 以图形方式展示了这种关系。\n Fig. 5.5: Showing where the sum of squares calculations are derived (Source: Field et al. 2012)\nWe can quantify the fit or effect size by dividing the model sum of squares with the total sum of squares giving us a value known as the R\\(^2\\) value. This effectively shows how well the observed data fits the regression line:\n\\(R^2 = \\frac{SS_M}{SS_T}\\)\nIt is a faction and displayed as a decimal ranging from 0 to 1. A value of 1 would mean all the our observed data would sit on the regression line. It is also a percentage, so if the R\\(^2\\) value was 0.58, then the regression equation accounts for 58% of the variability in the observed data.\nPhew! That’s enough theory for now, let’s move on and run a regression using R. We have our dataframe (beetles) and to run a linear regression we use the lm() function. Just like everything else in R, we feed the regression model into an object, we’ll call it M1 (short for model 1). When you run the code look at the top right window. Your object (M1) will arrive there. Double click on it to see what is stored in it.\n我们可以用模型平方和除以总平方和来量化拟合度，得到一个称为R\\(^2\\)的值。这有效地表明了观测数据与回归线的拟合程度：\n\\(R^2 = \\frac{SS_M}{SS_T}\\)\n它是一个分数，因此显示为 0 到 1 之间的小数，其中 1 表示所有观测数据都位于回归线上。由于它的范围是 0 到 1，因此它也是一个百分比，所以如果 R\\(^2\\) 值为 0.58，我们可以说回归方程解释了观测数据 58% 的变异性。另外，如果我们只有一个解释变量，我们可以对 R\\(^2\\) 值取平方根，得到皮尔逊相关系数！\n呼！理论知识到此为止，让我们继续使用 R 运行回归分析。我们有数据框 (evals)，要运行线性回归，我们使用 lm() 函数。就像 R 中的其他函数一样，我们将回归模型输入到一个对象中，我们将其命名为 M1（模型 1 的缩写）。运行代码时，请查看右上角的窗口。您的对象 (M1) 将出现在那里。双击它，查看其中存储的内容。\n\nM1 &lt;- lm(WEIGHTLOSS ~ HUMIDITY, data = beetle) # this is our regression model\n\nThe formula for the model is straightforward, we add the ‘WEIGHTLOSS’ (the \\(y\\) axis) first, then tilde, which is this symbol ~ , then ‘HUMIDITY’ (the \\(x\\) axis), supported by with a data = statement calling the dataframe. Now we have this we can examine the coefficients table using the summary() function.\n這個模型的公式很簡單，我們先加入「WEIGHTLOSS」（y軸），然後再加入波浪號（也就是符號 ~），最後加上「HUMIDITY」（x軸），最後使用 data = 語句呼叫資料框。現在，我們可以使用 summary() 函數檢查係數表。\n\nsummary(M1) # base r regression table call\n\n\nCall:\nlm(formula = WEIGHTLOSS ~ HUMIDITY, data = beetle)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46397 -0.03437  0.01675  0.07464  0.45236 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.704027   0.191565   45.44 6.54e-10 ***\nHUMIDITY    -0.053222   0.003256  -16.35 7.82e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2967 on 7 degrees of freedom\nMultiple R-squared:  0.9745,    Adjusted R-squared:  0.9708 \nF-statistic: 267.2 on 1 and 7 DF,  p-value: 7.816e-07\n\n\nThe output shows the formula we used, data on the residuals and coefficients, the significance codes, standard errors, R-squared and F statistics. The coefficients are the important elements.\n輸出顯示了我們使用的公式、殘差和係數的資料、顯著性代碼、標準誤差、R平方和F統計量。係數是重要的元素。\nThe first one is the intercept which has a value of 8.70. This is \\(\\beta_0\\) from our regression equation above and the point at which the regression lines crosses the \\(y\\) axis where \\(x\\)=0. Have a look at the line on your ggplot graph and you’ll see it crosses the \\(y\\) axis around 9.\n第一個是截距，值為 8.70。這是上面迴歸方程式中的 \\(\\beta_0\\)，也是迴歸線與 \\(y\\) 軸相交的點，其中 \\(x\\)=0。看看 ggplot 圖上的這條線，你會發現它在 9 左右與 \\(y\\) 軸相交。\nThe second, and possibly more important value, is the slope value for the explanatory \\(x\\) variable (HUMIDITY), which rounds to -0.053. This is \\(\\beta_1\\) in our regression equation. It is a negative, suggesting that beetle weight loss decreases with increasing humidity. In fact, because it is a slope, it indicates that, on average, the weight loss decreases by -0.053 for every one unit increase in the humidity. Now we have these we can generate our regression equation:\n第二個值，也可能是更重要的值，是解釋變數 \\(x\\)（濕度）的斜率值，四捨五入為 -0.053。這在我們的迴歸方程式中就是 \\(\\beta_1\\)。它是一個負數，表示甲蟲的體重減輕會隨著濕度的增加而減少。事實上，因為它是一個斜率，它表明平均而言，濕度每增加一個單位，體重減輕就會減少 -0.053。有了這些，我們就可以產生回歸方程式了：\n\\(y = \\beta_{0} + \\beta_{1} \\cdot \\text{HUMIDITY}\\)\nWhere, the estimated coefficients are: \\(\\beta_{0} = 8.70\\) and \\(\\beta_{1} = -0.05\\).\n\nThe p-values for both coefficients are highly significant, for the intercept = 6.4e-10 and for the slope it is 7.82e-07.\nThe residual standard error is quite high at 0.2967 but you could have guessed this already from plotting the graph - it’s almost a straight line!\nThere are 7 degrees of freedom in the calculation. This is number of samples (or rows) minus 1 for intercept term and 1 for the slope term (so 9-2 = 7).\nThe R-squared value for the model is 0.975 which suggests only ~98% of the variation of the data is accounted for by the model.\nThe adjusted R-squared is a little lower at 0.970 because the model is penalised by having one term in it i.e. one explanatory variable.\n\n其中，估計的係數為：\\(\\beta_{0} = 8.70\\) 和 \\(\\beta_{1} = -0.05\\)。\n\n這兩個係數的 p 值都非常顯著，截距為 6.4e-10，斜率為 7.82e-07。\n殘差標準誤差相當高，為 0.2967，但您可以從繪製圖表中猜到這一點——它幾乎是一條直線！\n計算中有 7 個自由度。這是樣本數（或行數）減去 1（截距項）和 1（斜率項）（因此 9-2 = 7）。\n該模型的 R 平方值為 0.975，這表示模型僅解釋了約 98% 的資料變異。\n調整後的 R 平方略低，為 0.970，因為模型因包含一個項（即一個解釋變數）而受到懲罰。\n\nWe would normally create a plot to show the results using the predictions from the regression equation to complete the analyses and write out the regression equation (Fig. 5.3). The code to do this is shown below. First, we sequence across the explanatory variable using the seq() function and setting the limits using the max() and min() functions. We choose 1000 points with the length=100 argument. Next, we use that object to dimensionalise at dataframe to take the prediction, called ‘predictions’. We then generate the predictions from the regression model (M1) and write that to a new dataframe called ‘predictions_with_CI’. We then add the predicted the values and upper and lower confidence intervals (CIs) and lastly extract the regression coefficients from the model (M1) for the regression equation. Then we plot the graph, pulling the points from the initial ‘beetle’ dataframe and plotting the geom_line() using predicted dataframe,‘predictions’ and adding the CIs with geom_ribbon() (Fig. 5.3). You know the code for labels and finally we add in equation using annotate(). YOU DO NOT NEED TO BE ABLE TO UNDERSTAND ALL THIS CODE AT THE MOMENT!!!!\n我們通常會建立一個圖表來顯示使用迴歸方程式的預測結果，以完成分析並寫出迴歸方程式（圖 5.6）。執行此操作的程式碼如下所示。首先，我們使用 seq() 函數對解釋變數進行排序，並使用 max() 和 min() 函數設定限制。我們使用 length=100 參數來選擇 1000 個點。接下來，我們使用該物件在資料框中對維度進行維度化以進行預測，稱為「predictions」。然後，我們從迴歸模型 (M1) 產生預測並將其寫入名為「predictions_with_CI」的新資料框。然後，我們將預測值和上下置信區間 (CI) 相加，最後從模型 (M1) 中提取迴歸方程式的迴歸係數。然後，我們繪製圖表，從初始“甲蟲”資料框中提取點，並使用預測資料框“predictions”繪製“geom_line()”，並使用“geom_ribbon()”新增CI（圖5.3）。您知道標籤的程式碼，最後我們使用“\n\n# Create a sequence of 100 values for the explanatory variable\nseq_HUMIDITY &lt;- seq(min(beetle$HUMIDITY), max(beetle$HUMIDITY), length = 100)\n\n# Create Df for the predictions\npredictions &lt;- data.frame(HUMIDITY = seq_HUMIDITY)\n\n# Add  confidence intervals \npredictions_with_CI &lt;- predict(M1, newdata = predictions, interval = \"confidence\")\n\n# Add the predicted values and CI bounds to the predictions dataframe\npredictions$predicted_score &lt;- predictions_with_CI[, 1]\npredictions$lwr &lt;- predictions_with_CI[, 2]\npredictions$upr &lt;- predictions_with_CI[, 3]\n\n# extract coefficient and set up the equation\nintercept &lt;- round(coef(M1)[1], 2)\nslope &lt;- round(coef(M1)[2], 2)\nequation_string &lt;- paste(\"WEIGHTLOSS =\", intercept, \"+\", slope, \"* HUMIDITY\")\n\n# create plot with regression line, points and CIs\nggplot() +\n  geom_point(data = beetle, aes(x = HUMIDITY, y = WEIGHTLOSS), color = \"black\", alpha = 0.6) +\n  geom_line(data = predictions, aes(x = HUMIDITY, y = predicted_score), color = \"blue\", linewidth = 1) +\n  geom_ribbon(data = predictions, aes(x = HUMIDITY, ymin = lwr, ymax = upr), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"Relationship between Beetle weightloss and Humidity\",\n       x = \"Humidity\",\n       y = \"Weightloss\") +\n  \n  # Add the equation\n annotate(\"text\", \n         x = Inf,       # Anchor to the far right\n         y = Inf,       # Anchor to the very top\n         label = equation_string, \n         parse = FALSE,\n         hjust = 1.1,     # Right-justify to pull it leftwards from the edge\n         vjust = 1.5,     # Top-justify to pull it downwards from the edge\n         color = \"grey25\", \n         size = 5) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(), #remove the minor grids\n    panel.grid.major = element_blank(), #remove the minor grids \n  )\n\n\n\n\n\n\n\n\nFig 5.3: The predicted relationship between beetle weightloss and humidity with the regression equation.\n\n\n5.3.3 Linear regression with one categorical explanatory variable\nFor this example we will revisit the fertilizer data set you carried out a single factor ANOVA with last week. We are doing this for a number of reasons but mainly to show that the aov and lm() functions effectively do the same thing. This is because they are derived in the same manner - a straightforward linear analysis. We will use the same code to generate the dataset.\n在這個例子中，我們將重新審視您上週進行單因子變異數分析的肥料資料集。這樣做的原因有很多，但主要是為了證明 aov 和 lm() 函數實際上做了同樣的事情。這是因為它們的推導方式相同——簡單的線性分析。我們將使用相同的程式碼來產生資料集。\n\n# Set.seed() to make our \"random\" data reproducible i.e. if we select 42 here it will create the exact same data! Cool eh?\nset.seed(42) \n\n# Define group parameters\nn &lt;- 30                # Number of plants in each group (our sample size)\nsd_within &lt;- 5         # The noise which is the standard deviation in height each group (in cm)\n\n# Define the signal. The sample means (of height) for each group (in cm)\nmean_a &lt;- 65           # Fertilizer A (best)\nmean_b &lt;- 58           # Fertilizer B (good)\nmean_c &lt;- 50           # Water control (baseline)\n\n# Create random data using rnorm()\nheights_a &lt;- rnorm(n, mean = mean_a, sd = sd_within)\nheights_b &lt;- rnorm(n, mean = mean_b, sd = sd_within)\nheights_c &lt;- rnorm(n, mean = mean_c, sd = sd_within)\n\n\n# A tidy data frame is the standard format for analysis and plotting in R.\nfertilizer_data &lt;- data.frame(\n  # Combine all the height measurements into one column\n  height = c(heights_a, heights_b, heights_c),\n  \n  # Create a corresponding group label for each measurement. Treat = the fertilizer Treatments\n  group = factor(rep(c(\"TreatA\", \"TreatB\", \"Control\"), each = n))\n)\n\n# look at the data\nglimpse(fertilizer_data)\n\nRows: 90\nColumns: 2\n$ height &lt;dbl&gt; 71.85479, 62.17651, 66.81564, 68.16431, 67.02134, 64.46938, 72.…\n$ group  &lt;fct&gt; TreatA, TreatA, TreatA, TreatA, TreatA, TreatA, TreatA, TreatA,…\n\nskim(fertilizer_data)\n\n\nData summary\n\n\nName\nfertilizer_data\n\n\nNumber of rows\n90\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ngroup\n0\n1\nFALSE\n3\nCon: 30, Tre: 30, Tre: 30\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1\n57.9\n7.86\n43.03\n52.82\n57.28\n63.55\n76.43\n▅▇▆▅▂\n\n\n\n\n\nYou have examined this data before, so step one of your EDA is done already. But have a good look at it again. Looking at the numerical variable (height). This shows the same information as our previous example: number of missing values, computes the means, standard deviation (sd), minimum score (p0), maximum score (p100) and quartiles (p25, p50 which is the median, p75) and lastly a histogram of each variable. So we can see mean plant height was 57.89 cm and the median (p50) height is 57.28 cm. These numbers are similar, indicating that the distribution is fairly balanced.\n您之前已經檢查過這些數據，因此 EDA 的第一步已經完成。但請再仔細檢查一次。查看數值變數（高度）。它顯示的資訊與我們先前的範例相同：缺失值的數量，計算平均值、標準差 (sd)、最小值 (p0)、最大值 (p100) 和四分位數（p25、p50（即中位數）和 p75），最後產生每個變數的直方圖。因此，我們可以看到平均植物高度為 57.89 厘米，中位數（p50）高度為 57.28 厘米。這些數字相似，顯示分佈相當均衡。\nDo some plotting to look at the patterns. We cannot use the car::scatterplot because it requires both variables to be continuous but we can use histograms and boxplots. Revisit week 2, where you were introduced to these plots, if you cannot remember what they are!\n畫一些圖來觀察其中的規律。我們不能使用“car::散點圖”，因為它要求兩個變數都是連續的，但我們可以使用直方圖和箱線圖。如果你記不住這些圖是什麼，可以回顧第二週介紹的內容！\nTASK: plot a histogram for plant height and a boxplots for the different treatment groups. HINT: you already have the code for the boxplot from last week! [~ 5 min]. 任務：繪製植物高度的直方圖和不同處理組的箱型圖。提示：你已經擁有上週繪製箱型圖的程式碼！ [~ 5 分鐘]。\nThe plots should look like the ones below: Fig. 5.4 is the histogram and Fig. 5.5 is the boxplot.\n這些圖應該看起來像下面的圖：圖 5.4 是直方圖，圖 5.5 是箱線圖。\n\n\n\n\n\n\n\n\n\nFig. 5.4: Histogram of plant heights.\nHere is the boxplot (Fig. 5.5).\n\n\n\n\n\n\n\n\n\nFig. 5.5: Boxplot showing the variability in plant height by treatment.\nWe can quantify the differences in the means and medians using the group_by() function from dplyr. This is useful as a means of baselining, especially as we plan to use the ‘group’ variable as a explanatory variable in a regression analysis.\n我們可以使用 dplyr 中的 group_by() 函數來量化平均數和中位數之間的差異。這可以作為一種基準測試方法，尤其是在我們計劃在迴歸分析中使用「組」變數作為解釋變數的情況下。\n\nheight_summary &lt;- fertilizer_data %&gt;% \n  group_by(group) %&gt;%\n  summarize(median = median(height),\n            mean = mean(height))\nheight_summary\n\n# A tibble: 3 × 3\n  group   median  mean\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Control   51.5  51.0\n2 TreatA    64.5  65.3\n3 TreatB    58.7  57.4\n\n\nWe can use the mutate() from function dplyr to calculate the differences in heights between the treatment groups and control. Type ‘?mutate’ into the console to get information on how this function works.\n我們可以使用函數「dplyr」中的「mutate()」來計算實驗組和對照組之間的身高差異。在控制台中輸入“?mutate”即可了解此函數的工作原理。\n\n# Calculate the difference from Africa's mean\nheight_diff &lt;- height_summary %&gt;%\n  mutate(\n    diff_from_contol = mean - mean[group == \"Control\"]\n  )\nheight_diff\n\n# A tibble: 3 × 4\n  group   median  mean diff_from_contol\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n1 Control   51.5  51.0             0   \n2 TreatA    64.5  65.3            14.4 \n3 TreatB    58.7  57.4             6.44\n\n# look at the table headings to see what new columns mutate is creating.\n\nThese are the contrasts we discussed in last week’s class. We see that fertilization increases plant productivity for both treatments A and B. 這些是我們上週課堂上討論過的對比。我們發現，施肥提高了 A 和 B 處理組的植物生產力。\nNow we are ready to run our regression. Recall from last week that the summary() function for the single factor ANOVA only provided the aggregate outcome and not the individual contrasts i.e. for the levels in the group variable. We used had to use a Tukey test to show those.\n現在我們準備運行迴歸分析了。回想一下上週，單因子變異數分析的 summary() 函數只提供了整體結果，而不是組別變數中各個層級的個體對比。我們不得不使用 Tukey 檢定 來顯示這些。\n\nM2 &lt;- lm(height ~ group, data = fertilizer_data)\n\nWhen we look at the lm() model output using the summary() function we see that it lists the levels in the group factor. If you type anova(M2), you would have the exact same outcome as when you used the summary() for the ANOVA model last week. Try typing it in your code and compare with the outcome from last week ANOVA.\n當我們使用 summary() 函數來查看 lm() 模型輸出時，我們會發現它列出了組因子中的水平。如果您輸入 anova(M2)，您將獲得與上週使用 summary() 進行變異數分析模型時完全相同的結果。 嘗試將其輸入到您的程式碼中，並與上週變異數分析的結果進行比較。\n\nsummary(M2)\n\n\nCall:\nlm(formula = height ~ group, data = fertilizer_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3559  -2.9655   0.7578   3.1498  11.0903 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.9538     0.9561   53.29  &lt; 2e-16 ***\ngroupTreatA  14.3891     1.3522   10.64  &lt; 2e-16 ***\ngroupTreatB   6.4366     1.3522    4.76 7.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.237 on 87 degrees of freedom\nMultiple R-squared:  0.5664,    Adjusted R-squared:  0.5565 \nF-statistic: 56.83 on 2 and 87 DF,  p-value: &lt; 2.2e-16\n\n\nNotice we have a very different table to the one above (the beetle regression). This is because R uses the levels of the group variable as a factor so returns estimates for each level. Notice too that ‘Control’ appears to be missing. It isn’t. Because R uses an alphanumeric sorting algorithm it orders the levels with the factor alphabetically so ‘Control’ is first in the list, followed by treatments. As a result, it is the baseline and wrapped into the intercept term. You can see this because the estimate is 50.95, the mean height for all the ‘Control’ plants (see the difference table above). If you look at our difference table you can see that the other estimates relate to the mean differences in plant height for each treatment. So TreatA plants are on average 14.39cm (rounded to 2 decimal places) taller that the Control plants and Treat B plants are on average 6.44 cm taller. As you know from above (our beetle regression) these numbers are also the (partial) slope coefficients for each treatment level.\n請注意，我們得到的表格與上面的表格（甲蟲回歸）截然不同。這是因為 R 使用組變數的水平作為因子，因此會傳回每個水平的估計值。也要注意，「對照」似乎缺失了。但事實並非如此。由於 R 使用字母數字排序演算法，它按字母順序排列包含因子的水平，因此「對照」位​​於列表第一位，其次是處理組。因此，它是基線，並被包含在截距項中。您可以看到這一點，因為估計值為 50.95，這是所有「對照」植物的平均高度（請參閱上面的差異表）。如果您查看我們的差異表，您會發現其他估計值與每個處理組的植物高度平均差異有關。因此，處理 A 植株平均比處理組植株高 14.39 公分（四捨五入至小數點後兩位），處理 B 植株平均高 6.44 公分。正如您從上面（我們的甲蟲回歸）中所知，這些數字也是每個處理組的（部分）斜率係數。\nWe are nearly there. All we need to do now is figure out the regression equation. This a little more daunting than the last one! R uses the levels in the factors as indicator functions. As we described above, the levels are ordered alphabetically so Control precedes, TreatA which comes before TreatB.\n我們快完成了。現在我們需要做的就是找出迴歸方程式。這比上一個更令人生畏！ R 使用因子中的水平作為指示函數。正如我們上面所述，水平按字母順序排列，因此 Control 位於前，TreatA 位於 TreatB 之前。\nThe equation is: \\[\n\\begin{align}\n\\hat{y} = \\widehat{\\text{height}} &= \\beta_0 + \\beta_{\\text{TreatA}} \\cdot \\mathbb{1}_{\\text{TreatA}}(x) + \\beta_{\\text{TreatB}} \\cdot \\mathbb{1}_{\\text{TreatB}}(x)\n\\end{align}\n\\] Don’t stress about this. It isn’t as hideous as it looks! Let’s work through it section by section. First, \\(\\mathbb{1}_A{(x)}\\) is known as an indicator function in mathematics. It returns either a 1 or a 0 where,\n別擔心，它並沒有看起來那麼難看！我們來一節一節地分析一下。 首先，\\(\\mathbb{1}_A{(x)}\\) 在數學中稱為指示函數。它返回 1 或 0，其中，\n\\[\n\\mathbb{1}_A{(x)} = \\begin{cases} 1 & \\text{if } x &gt; A, \\\\ 0 & \\text{if otherwise.} \\end{cases}\n\\] In statistics this is also known as dummy variable. Remember the Control is our baseline so our first dummy is \\(\\mathbb{1}_{TreatA}{(x)}\\). This returns a 1 if a plant is in the TreatA, or a 0 otherwise:\n在統計學中，這也稱為虛擬變數。記住，對照組是我們的基線，所以我們的第一個虛擬變數是\\(\\mathbb{1}_{TreatA}{(x)}\\)。如果植物屬於TreatA組，則回傳1，否則回傳0： \\[\n\\mathbb{1}_A{(x)} =\n\\begin{cases}\n1 & \\text{if group } x \\text{ is in the TreatA,} \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\] Second, \\(\\beta_0\\) is the intercept, as usual, but in this case it is the mean plant height of all the plants in the Control group.\nThird, the \\(\\beta_{TreatA}\\) and \\(\\beta_{TreatB}\\) represent the 2 offsets relative to our baseline (the Control group). These are listed in our regression table as: groupTreatA and groupTreatB.\nWe can put this all together to compute the fitted value \\(\\hat{y} = \\widehat{height}\\) for a Control plant. If a plant is in the Control group \\(\\mathbb{1}_{Control}(x)\\) its indicator will be a 1 (this is always the case because it is the baseline). In contrast, all the other indicator functions, \\(\\mathbb{1}_{TreatA}(x)\\), \\(\\mathbb{1}_{TreatB}(x)\\) will equal 0 (zero) and thus:\n第二，\\(\\beta_0\\) 是截距，與往常一樣，但在本例中，它是對照組所有植物的平均株高。\n第三，\\(\\beta_{TreatA}\\) 和 \\(\\beta_{TreatB}\\) 代表相對於基線（對照組）的兩個偏移量。它們在我們的迴歸表中列為：groupTreatA 和 groupTreatB。\n我們可以將所有這些結合起來，計算出對照組植物的擬合值 \\(\\hat{y} = \\widehat{height}\\)。如果一株植物位於對照組 \\(\\mathbb{1}_{Control}(x)\\)，它的指標將為 1（由於它是基線，因此始終如此）。相較之下，所有其他指示函數 \\(\\mathbb{1}_{TreatA}(x)\\)、\\(\\mathbb{1}_{TreatB}(x)\\) 將等於 0（零），因此： \\[\n\\begin{align*}\n\\hat{y} &= \\widehat{height} = \\beta_0 + \\beta_{TreatA} \\cdot \\mathbb{1}_{TreatA}(x) + \\beta_0 + \\beta_{TreatB} \\cdot \\mathbb{1}_{TreatB}(x) \\\\\n&= 50.95 + 14.39 \\cdot \\mathbb{1}_{TreatA}(x) + 6.44 \\cdot \\mathbb{1}_{TreatB}(x) \\\\\n& = 50.95 + 14.39 \\cdot 0 + 6.44\\cdot0\\\\\n& = 50.95\n\\end{align*}\n\\] As you know if you multiply by a 0 you end up with zero so all that’s left in the intercept value of 59.95, which the mean height for the Control plants.\nLet’s play this through. Let’s say our plant is in TreatB group. The other Treatment (TreatA) will be zero, but the indicator function \\(\\mathbb{1}_{TreatB}(x)\\) will be a 1. The equation thus looks like this:\n眾所周知，如果乘以 0，最終結果會是零，所以剩下的截距值是 59.95，也就是對照組植物的平均高度。\n我們來演練一下。假設我們的植物在處理組 B。另一個處理組（處理組 A）的值為零，但指示函數 \\(\\mathbb{1}_{TreatB}(x)\\) 為 1。因此，方程式如下所示： \\[\n\\begin{align*}\n\\hat{y} &= \\widehat{height} = \\beta_0 + \\beta_{TreatA} \\cdot \\mathbb{1}_{TreatA}(x) + \\beta_0 + \\beta_{TreatB} \\cdot \\mathbb{1}_{TreatB}(x) \\\\\n&= 50.95 + 14.39 \\cdot \\mathbb{1}_{TreatA}(x) + 6.44 \\cdot \\mathbb{1}_{TreatB}(x) \\\\\n& = 50.95 + 14.39 \\cdot 0 + 6.44\\cdot1\\\\\n& = 50.95 + 6.44\\\\\n& = 57.39\n\\end{align*}\n\\] Notice that the final number is the mean for TreatB that we calculated above! 請注意，最終數字是我們上面計算的 TreatB 的平均值！\nTASK: Can you figure out how this might work for treatment group TreatA? [~ 5 min].\n任務：你能弄清楚這對治療組 TreatA 有何作用嗎？ [~ 5 分鐘]。\n\n\n5.3.4 Model Validation\nOur last task it to validate the model. We’ll do with the the plot() function (Fig. 5.6). 我們的最後一個任務是驗證模型。我們將使用 plot() 函數來完成（圖 5.6）。\n\npar(mfrow = c(2, 2)) # set the plotting window to a 2 x 2 frame\nplot(M2)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1)) # reset the window back to 1 x 1 panel\n\nFig. 5.6: validation plots for model M3\nThere is nothing to worry here! 這裡沒有什麼好擔心的！\nNow we need to look at the residual spreads for the explanatory variables in the model (Fig. 5.7). 現在我們需要查看模型中解釋變數的殘差差幅（圖 5.7）。\n\nres &lt;- resid(M2) # strip out the residuals.\nboxplot(res ~ group, data = fertilizer_data) # we use boxplot from base R.\n\n\n\n\n\n\n\n\nFig. 5.7: The residuals by group boxplot\nWe have some variability across the groups visible in the boxplot (Fig. 5.7), which we might need to try and rectify, but we’ll come back to this is later sessions. The histogram indicates normal patterns across the whole pool of samples, confirming the interpretation of the four panel plot above.\n箱線圖中可以看到不同組別之間存在一些差異，我們可能需要嘗試修正，但我們會在後續環節中再討論這個問題。直方圖顯示整個樣本池呈現正常模式，證實了上面四幅圖的解釋。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#the-analysis-of-covariance-ancova-one-continuous-and-one-factor-variable",
    "href": "chap5.html#the-analysis-of-covariance-ancova-one-continuous-and-one-factor-variable",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.4 The Analysis of Covariance ANCOVA (one continuous and one factor variable)",
    "text": "5.4 The Analysis of Covariance ANCOVA (one continuous and one factor variable)\nWe’ll finish today by looking a the situation where we have one continuous variable and one factor variable. Analyses of this nature has a special tag: The Analysis of Covariance.\nThe data we use is analysed in Chapter 6 of Beckerman, Childs, and Petchey (2017) but orginates from an experiment outlined in Quinn and Keough (2002) ‘Experimental Design and Data Analysis for Biologists’. The data relate egg production by limpets subjected to four density conditions across two seasons (spring and summer). It is a test of density dependence impacts on fecundity levels.\n今天，我們將以一個連續變數和一個因子變數的情況作為結束。這類分析有一個特殊的標籤：協方差分析。\n我們使用的數據是在@Beckerman2017的第6章中進行分析的，但源自@Quinn2002在《生物學家的實驗設計和數據分析》中概述的一項實驗。這些數據涉及了帽貝在兩個季節（春季和夏季）中四種密度條件下的產卵情況。這是一項關於密度依賴性對繁殖力水平影響的測試。\nThe datafile is called limpet.csv.\n\nlimpet &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/limpet.csv\")\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): SEASON\ndbl (2): DENSITY, EGGS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLook at the data.\n\nglimpse(limpet)\n\nRows: 24\nColumns: 3\n$ DENSITY &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 15, 15, 15, 15, 15, 15, 30, 30, 30, 30, 30, …\n$ SEASON  &lt;chr&gt; \"spring\", \"spring\", \"spring\", \"summer\", \"summer\", \"summer\", \"s…\n$ EGGS    &lt;dbl&gt; 2.875, 2.625, 1.750, 2.125, 1.500, 1.875, 2.600, 1.866, 2.066,…\n\nskim(limpet)\n\n\nData summary\n\n\nName\nlimpet\n\n\nNumber of rows\n24\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSEASON\n0\n1\n6\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDENSITY\n0\n1\n24.50\n14.56\n8.00\n13.25\n22.50\n33.75\n45.00\n▇▁▃▁▃\n\n\nEGGS\n0\n1\n1.47\n0.71\n0.36\n0.92\n1.43\n1.92\n2.88\n▇▇▇▅▃\n\n\n\n\n\nWe have 24 rows and 3 columns (variables). The response variable (EGGS) is the average number of eggs produced by the limpet in the enclosures. The explanatory variables are DENSITY, which is the number of limpets in each density treatment, and SEASON, which are the seasons that the experiment covered (spring and summer).\nWe need to start by drawing some pictures. We’ll do this using ggplot2 and using an additional bit of code that allow us to plot regression lines for each season on the same plot. We will do it step by step. First up, we find the data limpet and allocate the \\(x\\) and \\(y\\) via the aes() function. We need to show the points so we add geom_point() and we want a regression line, hence: geom_smooth(method=\"lm\"). We use formula = 'y ~ x' to get rid of the irritating error message (delete the argument to see what I mean!). We add labels with labs and set the theme to black and white with theme_bw(). That’s most of the prettying work done (Fig. 5.8).\n我們有 24 行 3 列（變數）。反應變數（EGGS）是圍欄中帽貝的平均產卵量。解釋變數是密度（DENSITY），即每個密度處理中的帽貝數量；季節（SEASON），即實驗涵蓋的季節（春季和夏季）。\n我們需要先畫一些圖。我們將使用 ggplot2 和一些額外的程式碼，以便在同一張圖上繪製每個季節的迴歸線。我們將逐步完成。首先，我們找到資料“limpet”，並透過“aes()”函數分配 x 和 y 值。我們需要顯示這些點，因此我們添加了“geom_point()”，並需要一條回歸線，因此使用“geom_smooth(method=“lm”)”。我們使用“labs”添加標籤，並使用“theme_bw()”將主題設定為黑白。至此，大部分美化工作就完成了。\n\nggplot(limpet, aes(x = DENSITY, y = EGGS)) +\n  # Add the raw data points\n  geom_point(alpha = 0.6) +\n  \n  # Add a linear regression line for each season\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = TRUE, linewidth = 1) + # se=TRUE adds the confidence ribbon\n  labs(\n    title = \"Limpets: Egg Count by Density and Season\",\n    x = \"Density (Limpets per square metre)\",\n    y = \"Egg Count\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFig. 5.8: First plot with a single regression line\nNow we add two little bit of code to plot a line for each season, we merely add the colour=SEASON argument into the aes() (Fig. 5.9).\n現在我們加入兩小段程式碼來為每個季節繪製一條線，我們只需將「colour=SEASON」參數新增到「aes()」中。\n\nggplot(limpet, aes(x = DENSITY, y = EGGS, color = SEASON)) +\n  # Add the raw data points\n  geom_point(alpha = 0.6) +\n  \n  # Add a linear regression line for each season\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = TRUE, linewidth = 1) + # se=TRUE adds the confidence ribbon\n   labs(\n    title = \"Limpets: Egg Count by Density and Season\",\n    x = \"Density (Limpets per square metre)\",\n    y = \"Egg Count\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFig. 5.9: Adding the second regression line\nWe can make it look a little nicer by specifying the colours we want using scale_color_manual() and the point sizes with size (Fig. 5.10).\n我們可以使用「scale_color_manual()」指定我們想要的顏色，使用「size」指定點大小，讓它看起來更漂亮一些。\n\n ggplot(limpet, aes(x = DENSITY, y = EGGS, color = SEASON)) +\n  # Add the raw data points\n  geom_point(alpha = 0.8, \n             size=3) +\n  \n  # Add a linear regression line for each season\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = TRUE, linewidth = 1) + # se=TRUE display the confidence ribbon\n  \n  # Manually set the colors\n  scale_color_manual(values = c(\"spring\" = \"red\", \"summer\" = \"blue\")) +\n  \n  labs(\n    title = \"Limpets: Egg Count by Density and Season\",\n    x = \"Density (Limpets per square metre)\",\n    y = \"Egg Count\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n5.10: colouring the dots\nTo colour the CIs the same as the points we add fill = SEASON to the aes() argument and an additional statement scale_fill_manual to match the points and CI colours (Fig. 5.11).\n為了將 CI 的顏色與點的顏色相同，我們在 aes() 參數中加入 fill = SEASON，並加入一個附加語句 scale_fill_manual 來符合點和 CI 的顏色。\n\n # add fill to the aes\nggplot(limpet, aes(x = DENSITY, y = EGGS, color = SEASON, fill = SEASON)) +\n  \n  # The raw data points controlled by 'color'\n  geom_point(alpha = 0.8, size =3) +\n  \n  # The regression lines controlled by 'color' and CIs are controlled by 'fill'\n  # Add a bit of transparency to the ribbon so it looks lighter.\n  geom_smooth(method = \"lm\", formula = 'y~x', se = TRUE, linewidth = 1, alpha = 0.2) +\n  \n  # Manually set the colors for the points and lines\n  scale_color_manual(values = c(\"spring\" = \"red\", \"summer\" = \"blue\")) +\n\n  # Use the exact same color values to ensure they match.\n  scale_fill_manual(values = c(\"spring\" = \"red\", \"summer\" = \"blue\")) +\n  \n  labs(\n    title = \"Limpets: Egg Count by Density and Season\",\n    x = \"Density (Limpets per square metre)\",\n    y = \"Egg Count\"\n  ) +\n  theme_bw() +\n  theme(  # this function allows us to change parameters in the theme we select\n    legend.position = \"top\",# put the legend on the top\n    panel.grid.minor = element_blank(), #remove the minor grids\n    panel.grid.major = element_blank() # remove the plot border\n  )\n\n\n\n\n\n\n\n\nFig. 5.11: standardizing dot colour to CIs\nWe see that as limpet density increases egg production decreases and also that egg production is higher in spring than summer. As the lines are not overlapping it’s doesn’t look like there is any form on interaction. The basic ANCOVA model is EGGS ~ DENSITY + SEASON we can use this as our lines look parallel, but we ought to check for an interaction effect so we go with: EGGS ~ DENSITY * SEASON.\n我們發現，隨著帽貝密度的增加，產蛋量會下降，而且春季的產蛋量高於夏季。由於線條不重疊，看起來似乎不存在任何交互作用。基本的變異數分析模型是“蛋量 ~ 密度 + 季節”，由於線條看起來平行，我們可以使用它，但我們應該檢查交互作用，所以我們採用“蛋量 ~ 密度 * 季節”模型。\n\n5.4.1 The ANCOVA model\nWe can use multiple regression to model these relationships. We can do it two ways:\n这个情节有很多有趣的事情。首先，它显示 生育率随着所有人预期寿命的增加而下降 收入群体。其次，这表明低收入国家的预期寿命较低。 收入国家，但随着国家变得 richier - 查看每个回归起点的顺序 线。第三，生育率随着收入的增加而下降。第四， 生育率随着预期寿命的增加而下降的速度较慢 收入国家 - 线的斜率较浅。\n我们可以使用多元回归来模拟这些关系。我们可以做 有两种方式：\n\nA additive model that adds each explanatory variables sequentially, where (按顺序添加每个解释变量的加法模型， 在哪里)：\n\n\\(y = X_1 + X_2 + \\epsilon\\), and a\n\nA multiplicative or interactive model, where (乘法或交互模型，其中)\n\n\\(y = X_1 * X_2 + \\epsilon\\).\nThe two regression generalised equations for these models are: 这些模型的两个回归广义方程是：\n\nFor an additive version:\n\n\\(y = {\\beta}0_{xi} + {\\beta}1_{xi} + {\\beta}2_{xi} + \\epsilon\\) where,\n\\({\\beta}0_{xi}\\) is a common intercept for both \\(x\\) variables\n\\({\\beta}1_{xi}\\) is the partial slope coefficient for \\(x1\\)\n\\({\\beta}2_{xi}\\) is the partial slope coefficient for \\(x2\\)\n\\(\\epsilon\\) is the error term\n\\(y = {\\beta}0_{xi} + {\\beta}1_{xi} + {\\beta}2_{xi} + \\epsilon\\) 其中，\n\\({\\beta}0_{xi}\\) 是两个 \\(x\\) 变量的公共截距\n\\({\\beta}1_{xi}\\) 是 \\(x1\\) 的部分斜率系数\n\\({\\beta}2_{xi}\\) 是 \\(x2\\) 的部分斜率系数\n\\(\\epsilon\\) 是误差项\n\nFor a multiplicative/interactive version:\n\n\\(y = {\\beta}0_{xi} + {\\beta}1_{xi} + {\\beta}2_{xi} + {\\beta}3_{xi1xi2} + \\epsilon\\) where,\n\\({\\beta}0_{xi}\\) is a common intercept for both \\(x\\) variables\n\\({\\beta}1_{xi}\\) is the partial slope coefficient for \\(x1\\)\n\\({\\beta}2_{xi}\\) is the partial slope coefficient for \\(x2\\)\n\\({\\beta}3_{xi1xi2}\\) is partial slope of the interactive effect \\(x1 \\cdot x2\\)\n\\(\\epsilon\\) is the error term\n\n对于乘法/交互版本：\n\n\\(y = {\\beta}0_{xi} + {\\beta}1_{xi} + {\\beta}2_{xi} + {\\beta}3_{xi1xi2} + \\epsilon\\) 在哪里，\n\\({\\beta}0_{xi}\\) 是两个 \\(x\\) 变量的公共截距\n\\({\\beta}1_{xi}\\) 是 \\(x1\\) 的部分斜率系数\n\\({\\beta}2_{xi}\\) 是 \\(x2\\) 的部分斜率系数\n\\({\\beta}3_{xi1xi2}\\) 是交互效果的部分斜率 \\(x1 \\cdot x2\\)\n\\(\\epsilon\\) 是误差项\nLet’s start with in additive of non-interactive model.\n\n\n5.4.2 Non interactive or additive model\nWe will explore the simpler non-interactive model first. This still represents different SEASON groups with different regression lines by allowing different intercepts (one for each level of the SEASON variable), but all the lines have the same slope (which it looks like they do).\n我們將首先探索更簡單的非互動式模型。該模型仍然透過允許不同的截距（SEASON 變數的每個等級一個截距）來表示具有不同迴歸線的不同 SEASON 組，但所有迴歸線具有相同的斜率（看起來確實如此）。\nThe additive model looks like this.\n\nM3 &lt;- lm(EGGS ~ DENSITY + SEASON, data = limpet)\n\n\nsummary(M3)\n\n\nCall:\nlm(formula = EGGS ~ DENSITY + SEASON, data = limpet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66324 -0.24009 -0.04174  0.27996  0.56676 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.62602    0.18116  14.496 2.08e-12 ***\nDENSITY      -0.03209    0.00571  -5.621 1.41e-05 ***\nSEASONsummer -0.73600    0.16280  -4.521 0.000187 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3988 on 21 degrees of freedom\nMultiple R-squared:  0.7125,    Adjusted R-squared:  0.6851 \nF-statistic: 26.02 on 2 and 21 DF,  p-value: 2.072e-06\n\n\nThe table shows the following:\n\nLine one intercept (2.626) for the first level of the SEASON variable (=spring). This is highly significant.\nLine two is the slope for all the variables (-0.03209). This is highly significant.\nLine three is the intercept offset for the summer SEASON. This is highly significant.\n\n表格顯示以下內容：\n\n第一行是季節變數第一級（春季）的截距（2.626）。該值高度顯著。\n第二行是所有變數的斜率（-0.03209）。該值高度顯著。\n第三行是夏季季節的截距偏移量。該值高度顯著。\n\nAs discussed previously, R operates alphanumerically so it sorts the various levels in the factor from A to B. If we only added the SEASON variable is as an explanatory the level then the ‘spring’ level of the factor will be taken as the baseline for intercept, because it comes earlier in the alphabet than ‘summer’.\n如前所述，R 按字母數字順序操作，因此它對因子中從 A 到 B 的各個層級進行排序。如果我們只添加 SEASON 變數作為解釋級別，則該因子的「春季」級別將被視為截距的基線，因為它在字母表中的位置比「夏季」更早。\nRecall also that the levels in the factor (or categorical) variable are effectively coded as dummy variables (we’ll call them \\(D\\)) so it is represented as either a 1 or 0. R does this automatically for you, but for reference you can do it manually by the setting the contrasts. We will not cover this aspect in this module.\n還要記住，因子（或分類）變數的水平實際上被編碼為虛擬變數（我們稱之為 \\(D\\)），因此它要么表示為 1，要么表示為 0。 R 會自動為您執行此操作，但作為參考，您可以透過設定對比度來手動執行此操作。本模組不會介紹這方面的內容。\n\\[\n\\mathbb{D}_1 =\n\\begin{cases}\n1 & \\text{if SEASON } x \\text{ is spring\n,} \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\n\\[\n\\mathbb{D}_2 =\n\\begin{cases}\n1 & \\text{if SEASON } x \\text{ is summer,} \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\] So for spring \\(D1\\) = 1, and \\(D2\\) = 0. And, for summer \\(D1\\) = 0, and \\(D2\\) = 1. The regression equations for these dummy variables is shown below. Note that first dummy variable has been dropped (i.e. there is no \\(\\beta_1\\)) and the intercept captures this effect.\n因此，春季 \\(D1\\) = 1，\\(D2\\) = 0。夏季 \\(D1\\) = 0，\\(D2\\) = 1。這些虛擬變數的迴歸方程式如下所示。請注意，第一個虛擬變數已被刪除（即沒有 \\(\\beta_1\\)），而截距捕捉到了這一效應。\n\\[\n\\begin{align*}\n\\hat{y} &= \\widehat{\\text{EGGS}}=\\beta_0 +\\beta_2{D}_2\n\\end{align*}\n\\]\nwhere, \\(\\beta_0\\) is the intercept for the first level in the category (spring), and \\(\\beta_2\\) is the offset for summer.\nSo for the situation where SEASON is spring, the regression equation only includes \\(\\beta_0\\) or the intercept is the average EGG production for spring, which happens to be 2.664. We can look at all the betas (or coefficients) by running the regression model below. We are using the tidy() function from the broom package to tidy the coefficients table up a little.\n其中，\\(\\beta_0\\) 是類別（春季）第一個等級的截距，\\(\\beta_2\\) 是夏季的偏移。\n因此，對於 SEASON 為春季的情況，迴歸方程式僅包含 \\(\\beta_0\\)，或截距是春季的平均蛋產量，恰好是 2.664。我們可以透過執行下面的迴歸模型來查看所有的 beta 值（或係數）。我們使用 broom 套件中的 tidy() 函數對係數表進行了一些整理。\n\ntidy(M3)[, c(\"term\", \"estimate\")] # we extract two objects from the model list\n\n# A tibble: 3 × 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 (Intercept)    2.63  \n2 DENSITY       -0.0321\n3 SEASONsummer  -0.736 \n\n\nNow we have that we can calculate the intercepts for each category in SEASON variable. We only have one left, that is summer and the intercept for that is 2.626 + -0.736 = 0.774.\n現在我們可以計算 SEASON 變數中每個類別的截距了。只剩下一個類別，那就是夏季，它的截距為 2.626 + -0.736 = 0.774。\nNow we have solid understanding of how the factorial regression is working let’s run the model. We can see why is sometimes called an additive model because we add in the explanatory variables using a ‘\\(+\\)’ in the R formula.\n现在我们对阶乘回归有了深入的了解 让我们运行模型。我们可以明白为什么有时被称为 加性模型因为我们使用 R 公式中的“\\(+\\)”。\nWe need to validate this model (Fig. 5.12). 我們需要驗證這個模型 (Fig. 5.12)。\n\n#set up the plot window to receive 4 plots in a 2 by 2 matrix\npar(mfrow=c(2,2)) # par sets the graphics window and mfrow=c(2,2) argument sets the window to four panels in a 2 x s grid. We need this because the plot() function generates four plots\n#Plot the graphs\nplot(M3)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nFig. 5.12: validation plots for M3\nThis looks pretty good. We ought to look at the residual spreads on the explanatory factors too, but we need to move on. The next step is to look at the interactive of multiplicative model.\n看起來不錯。我們也應該看看解釋因素的殘差差幅，但我們需要繼續。下一步是研究乘法模型的交互作用。\n\n\n5.4.3 The interactive (multiplicative model)\nThe model specification is:\n\nM4 &lt;- lm(EGGS ~ DENSITY * SEASON, data = limpet)\n\nWe look at the model summary.\n\nsummary(M4)\n\n\nCall:\nlm(formula = EGGS ~ DENSITY * SEASON, data = limpet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65468 -0.25021 -0.03318  0.28335  0.57532 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           2.664166   0.234118  11.380 3.45e-10 ***\nDENSITY              -0.033650   0.008259  -4.074 0.000591 ***\nSEASONsummer         -0.812282   0.331092  -2.453 0.023450 *  \nDENSITY:SEASONsummer  0.003114   0.011680   0.267 0.792533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4079 on 20 degrees of freedom\nMultiple R-squared:  0.7135,    Adjusted R-squared:  0.6705 \nF-statistic:  16.6 on 3 and 20 DF,  p-value: 1.186e-05\n\n\nThe table is the same as the additive model, although the coefficients differ a little. It also has on additional line, line 4 DENSITY:SEASONsummer. This is the difference in slopes between the two regression lines i.e. spring(slope) - summer(slope). You can also see that DENSITY is highly significant (line 2) and that there is a difference between egg production in summer from spring (-0.8 eggs), with a less significant p = 0.023 (line 3). The difference in slopes is tiny at 0.003114 and not significant.\n表與加法模型相同，儘管係數略有不同。它還增加了一行，即第 4 行“DENSITY:SEASONsummer”。這是兩條迴歸線（即春季（斜率）- 夏季（斜率））之間的斜率差。您還可以看到，DENSITY 非常顯著（第 2 行），並且夏季和春季的產蛋量存在差異（-0.8 個產蛋），但 p = 0.023（第 3 行）的顯著性較低。斜率差異很小，為 0.003114，並不顯著。\nWe are almost there. But how do we know which of these two models in the best? We could use a number of approaches to compare them, such as the Akaike’s Information Criterion (AIC).\n我們快完成了。但是，我們如何知道這兩個模型中哪一個是最好的呢？我們可以使用多種方法來比較它們，例如赤池資訊準則 (AIC)。\n\nAIC(M3,M4)\n\n   df      AIC\nM3  4 28.77452\nM4  5 30.68940\n\n\nWe’ll revisit AIC later in the module. For the time being, we select the model with the lowest AIC as the best fitting model. As you can see here, that is model M3. But Burnham and Anderson (2002) suggest that all models with 2 AIC points are equally likely.\n我們將在本模組的稍後部分重新討論 AIC。目前，我們選擇 AIC 最低 的模型作為最佳擬合模型。正如您在此處所見，該模型是 M3。但 Burnham and Anderson (2002) 建議，所有 2 個 AIC 點的模型都具有同等可能性。\n\nAIC1 &lt;- AIC(M3) # Paste m3 aic into an object\nAIC2 &lt;- AIC(M4) # Paste m4 aic into an object\nAIC2 - AIC1 # subract the smallest from the largest\n\n[1] 1.914883\n\n\nEither model is acceptable. 兩種模型都是可以接受的。\nOr we can use ANOVA to check, if the models are nested. 或者我們可以使用變異數分析來檢查模型是否嵌套。\n\nanova(M3,M4)\n\nAnalysis of Variance Table\n\nModel 1: EGGS ~ DENSITY + SEASON\nModel 2: EGGS ~ DENSITY * SEASON\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     21 3.3394                           \n2     20 3.3275  1  0.011822 0.0711 0.7925\n\n\nM4 does not differ significantly from M3, indicating that M4 is not an improvement on M3 i.e. it confirms the AIC contrast. However, we need to think about this. Remember what we are trying to understand here? We are interested in whether the effect of density depends on season, so we need the interaction term, on top the effects of density and season on their own. That is, we need a term in which the effect of density on egg production could depend on season. So we let our hypotheses do the selection, and settle for model M4.\nM4 與 M3 並無顯著差異，這表明 M4 並非 M3 的改進，即它證實了 AIC 對比。然而，我們需要思考一下這一點。還記得我們在這裡試圖理解的內容嗎？我們感興趣的是密度的影響是否依賴季節，因此除了密度和季節本身的影響之外，我們還需要交互項。也就是說，我們需要一個項，其中密度對產蛋量的影響可能依賴於季節。因此，我們根據假設進行選擇，最終選擇了模型 M4。\nFinally, we need to validate model M4 (Fig. 5.13). 最後，我們需要驗證模型M4 (Fig. 5.13)。\n\n#set up the plot window to receive 4 plots in a 2 by 2 matrix\npar(mfrow=c(2,2)) \nplot(M4)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n** Fig. 5.13: Validation plots for model M4**\nThese look fine. Now we need to check each individual explanatory variable. 這些看起來不錯。現在我們需要檢查每個單獨的解釋變數。Here is the DENSITY residual spreads (Fig. 5.14)\n\nres &lt;- rstandard(M4) # strip out the residuals\nplot(res ~ limpet$DENSITY) # DENSITY validation\n\n\n\n\n\n\n\n\nFig. 5.14: Residual plot for the DENSITY variable (for M4)\nThese look okay, perhaps the density levels pinch out on (Fig. 5.14) but the boxplots look okay (Fig. 5.15). Overall, the plots are fine. We can accept the model.\n\nboxplot(res~limpet$SEASON) # SEASON validation\n\n\n\n\n\n\n\n\nFig. 5.15: Residual boxplots for the SEASON variable (for M4)\nThe last thing we need to do is create the final supporting figure with some predicted values.\n\n# Create a Grid of Predictor Values. We need to tell the model *what* values to predict at. This grid should contain all the combinations of predictors in your model.\nprediction_grid &lt;- expand.grid(\n  # Create a smooth sequence of 100 DENSITY values from the minimum to the maximum\n  DENSITY = seq(min(limpet$DENSITY), max(limpet$DENSITY), length.out = 100),\n  # Include BOTH levels of the SEASON factor\n  SEASON = c(\"spring\", \"summer\")\n)\n\n\n# Generate the Predictions\n# Use the predict() function with your model and the new grid of data. interval = \"confidence\" creates the confidence intervals\nprediction_values &lt;- predict(M4, newdata = prediction_grid, interval = \"confidence\")\n\n\n# Combine the Grid and the Predictions into a Final Data Frame. The output of predict() is a matrix, so we'll combine it with our grid using cbind.\npredictions_df &lt;- cbind(prediction_grid, prediction_values)\n\n\n# Create the Plot Using the New Prediction Data Frame -\nggplot() +\n  \n  # Add raw data points from the original 'limpet' data frame\n  geom_point(data = limpet, aes(x = DENSITY, y = EGGS, color = SEASON), alpha = 0.7) +\n  \n  # Add the regression lines from our 'predictions_df'\n  geom_line(data = predictions_df, aes(x = DENSITY, y = fit, color = SEASON), linewidth = 1) +\n  \n  # Add the confidence interval ribbons from our 'predictions_df'\n  geom_ribbon(data = predictions_df, aes(x = DENSITY, ymin = lwr, ymax = upr, fill = SEASON), alpha = 0.2) +\n  \n  # Manually set your desired colors for both scales\n  scale_color_manual(values = c(\"spring\" = \"red\", \"summer\" = \"black\")) +\n  scale_fill_manual(values = c(\"spring\" = \"red\", \"summer\" = \"black\")) +\n  \n  # Add labels and a title\n  labs(\n    title = \"Egg Count vs. Density by Season\",\n    x = expression(paste(\"Density of Limpets (m\"^2, \")\")),\n    y = \"Egg Count\"\n  ) +\n  \n  theme_bw() +\n  theme(\n    legend.position = \"top\",# put the legend on the top\n    panel.grid.minor = element_blank(), #remove the minor grids\n    panel.grid.major = element_blank() # remove the plot border \n  )\n\n\n\n\n\n\n\n\nA few words are needed here to explain the code above. The expand.grid() is the key function. It creates a data frame with all possible combinations of the values you select to use. In this case, we opt for 100 DENSITY values (you could have used fewer if need be). The functions pairs DENSITY with ‘spring’ and ‘summer’, creating a ‘prediction’ grid. predict(M4, newdata = prediction_grid, ...)generates the prediction values by taking the final model ‘M4’ and matching each row in the ‘prediction grid’ to calculate the predicted EGGS value (fit) and the confidence interval (lwr, upr) for each combination of DENSITY and SEASON. The other element worth explaining is the expression(paste(\"Density of Limpets (m\"^2, \")\")). This pastes in an expression allowing you to use special characters, in this case a superscript created with the ^ character. The rest of the code you’ve seen before.\n這裡需要簡單解釋一下上面的程式碼。 「expand.grid()」是關鍵函數。它會建立一個資料框，其中包含你選擇使用的所有可能值的組合。在本例中，我們選擇了 100 個 DENSITY 值（如果需要，你可以使用更少的值）。此函數將 DENSITY 與「spring」和「summer」配對，建立一個「預測」網格。 「predict(M4, newdata = prediction_grid, …)」透過採用最終模型「M4」並匹配「預測網格」中的每一行來產生預測值，從而計算出 DENSITY 和 SEASON 每種組合的預測 EGGS 值（擬合值）和置信區間（lwr, upr）。另一個值得解釋的元素是「expression(paste(“Density of Limpets (m”^2, “)”))」。這會貼上一個允許你使用特殊字元的表達式，在本例中，這個表達式是用「^」字元建立的上標。其餘代碼您之前已經看過。\n\n\n5.4.4 Interpretation of regression analyses\nIt is important to remember the old adage that states correlation does not equal causation. What we have done in this workshop is generate statistical linear regression models using three different datasets. We need be careful how far we are willing to push our inferences from these analyses. As Nick (Prof. Kettridge) will tell you at great length (and at my expense); these are not mechanistic models!\n務必牢記一句古老的格言：相關性不等於因果關係。在本次研討會上，我們利用三個不同的資料集建構了統計線性迴歸模型。我們需要謹慎考慮，究竟該如何從這些分析中得出推論。正如 Nick（Kettridge 教授）會詳細地（並且以我為代價）告訴大家的那樣：這些並非機械模型！",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#class-exercises",
    "href": "chap5.html#class-exercises",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.5 Class Exercises",
    "text": "5.5 Class Exercises\nTASKS - repeat for all the datasets:\n\n5.6 Create plots to assess the assumptions:\n(a) On the raw data;\n\n\nRun the model;\n\n\nValidate it.\n\n\nCreate a final model plot with predicted data, 95% CIs, regression equation, R-squared value;\nInterpret the model output.\n\n任務 - 對所有資料集重複此操作：\n\n建立圖表以評估假設：\n\n\n基於原始資料；\n運行模型；\n驗證模型。\n\n\n使用預測資料、95% 信賴區間、迴歸方程式和 R 平方值建立最終模型圖；\n解釋模型輸出。\n\n\n5.6.1 EXERCISE 1\nThese data come from from Peake and Quinn (1993); and were analysed in Quinn and Keough (2023) and Logan (2010).\n\nFILE: mussel.csv\nRESPONSE: Species richness of invertebrates (SPECIES).\nEXPLANATORY: area of mussel beds patches (AREA).\n\n這些數據來自@Peake1993；並由@Quinn2023 和@Logan2010 進行了分析。\n\n檔：mussel.csv\n反應：無脊椎動物物種豐富度（物種）。\n解釋：貽貝床斑塊面積（面積）。\n\n\n\n5.6.2 EXERCISE 2\nThese data relate to apple yield (kg) from cultivars grafted onto rootstocks of varying basal diameters (mm). The orchard design further incorporates two management regimes: plots either exposed to cattle grazing or maintained as grazing-free controls. Grazing is hypothesised to reduce understory grass biomass, thereby potentially mitigating competitive interactions for soil resources between grasses and apple trees. The data are covered extensively used in Beckerman, Childs, and Petchey (2017).\nFILE: compensation.csv\n\nRESPONSE: Apple yield (fruit)\nEXPLANATORY 1: Root stock diameter (root)\nEXPLANATORY 2: Grazing (ungrazed/grazed) (Grazing)\n\n這些數據指的是嫁接到不同基部直徑（毫米）砧木上的蘋果品種的產量（公斤）。果園設計進一步包含兩種管理方案：要麼進行牛放牧，要麼作為無放牧對照。據推測，放牧會減少林下草類生物量，這可能會減輕草類和蘋果樹之間對土壤資源的競爭。此數據在@Beckerman2017中被廣泛使用。\n文件：compensation.csv\n\n反應：蘋果產量（果實）\n解釋 1：砧木直徑（根）\n解釋 2：放牧（非放牧/放牧）（放牧）",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#follow-up-work",
    "href": "chap5.html#follow-up-work",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.7 Follow-up work",
    "text": "5.7 Follow-up work\n\nComplete the tasks / exercises if you haven’t done so in class.\nRead chapter 6 in Beckerman, Childs, and Petchey (2017).\n如果課堂上沒有完成任務/練習，請完成。\n閱讀@Beckerman2017的第6章。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#next-week",
    "href": "chap5.html#next-week",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.8 Next week",
    "text": "5.8 Next week\nNext week we will introduce you to multivariate linear regression where two or more explanatory variables. We will also examine how we might select the best possible fitting model in such circumstances i.e. model selection.\n下週我們將介紹多元線性迴歸，其中有兩個或多個解釋變數。我們還將研究在這種情況下如何選擇最佳擬合模型，即模型選擇。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap5.html#references",
    "href": "chap5.html#references",
    "title": "5  An Introduction to Linear Regression using R",
    "section": "5.9 References",
    "text": "5.9 References\n\n\n\n\nBeckerman, Andrew, Dylan Childs, and Owen Petchey. 2017. Getting Started with R: An Introduction for Biologists. 2nd. ed. Oxford: Oxford University Press.\n\n\nBurnham, Kenneth P, and David. R Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. 2nd Ed. Springer New York.\n\n\nField, Andy, Jeremy Miles, and Zoe Field. 2012. Discovering Statistics Using R. 1st Ed. Sage Publications.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Sage Publications.\n\n\nIsmay, Chester, Albert Kim Y., and Arturo Valdivia. 2025. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. 2nd Ed. CRC Press, Chapman Hall.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using R : A Practical Guide. Chichester: Wiley and Sons.\n\n\nPeake, Anthony J., and G. P. Quinn. 1993. “Temporal Variation in Species-Area Curves for Invertebrates in Clumps of an Intertidal Mussel.” Ecography 16 (3): 269–77. https://doi.org/10.1111/j.1600-0587.1993.tb00216.x.\n\n\nQuinn, Gerry P., and Michael J. Keough. 2002. “Experimental Design and Data Analysis for Biologists.” Cambridge Aspire Website. https://www.cambridge.org/highereducation/books/experimental-design-and-data-analysis-for-biologists/BAF276114278FF40A7ED1B0FE77D691A; Cambridge University Press. https://doi.org/10.1017/CBO9780511806384.\n\n\n———. 2023. “Experimental Design and Data Analysis for Biologists.” Cambridge Aspire Website. https://www.cambridge.org/highereducation/books/experimental-design-and-data-analysis-for-biologists/7AA1811FE2E249CE6065ACD6F3B68C41; Cambridge University Press. https://doi.org/10.1017/9781139568173.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2024. “Broom: Convert Statistical Objects into Tidy Tibbles R (Version 1.0.7).”\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. “Skimr: Compact and Flexible Summaries of Data.”\n\n\nWei, Taiyun, and Viliam Simko. 2024. “R Package ’Corrplot’: Visualization of a Correlation Matrix.”\n\n\nZuur, Alain F., and Elena N. Ieno. 2016. “A Protocol for Conducting and Presenting Results of Regression-Type Analyses.” Methods in Ecology and Evolution 7 (6): 636–45. https://doi.org/10.1111/2041-210X.12577.\n\n\nZuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” Methods in Ecology and Evolution 1 (1): 3–14. https://doi.org/10.1111/j.2041-210X.2009.00001.x.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>An Introduction to Linear Regression using R</span>"
    ]
  },
  {
    "objectID": "chap6.html",
    "href": "chap6.html",
    "title": "6  Introduction to multivariate linear regression",
    "section": "",
    "text": "6.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to multivariate linear regression</span>"
    ]
  },
  {
    "objectID": "chap7.html",
    "href": "chap7.html",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "",
    "text": "7.1 Outline:\nToday we are going to introduce you to Generalised Linear Models (GLMs). These were developed in the 1970s but popularised in 80s by McCullagh and Nelder (1989) in their seminal book ‘Generalised Linear Models’ (1982, 2nd edition 1989). They are a group of regression models from the exponent family that generalise classical linear models. They are used in situations when the data you have collected have properties that do not conform to the requirements of linear regression. These sorts of data are much more common that you’d imagine, as we’ll explore below.\n今天，我们将向您介绍广义线性模型（GLMs）。这些模型最初在 1970 年代 被提出，并在 1980 年代 由 McCullagh 和 Nelder 在其经典著作《广义线性模型》（1982 年，第二版 1989 年）中推广。GLMs 是指数族回归模型的一类，它们对经典线性模型进行了推广。当您收集到的数据不符合线性回归的假设时，就可以使用 GLMs 进行建模。事实上，这类数据比您想象的要普遍得多，我们将在下面进一步探讨这一点。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#outline",
    "href": "chap7.html#outline",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "",
    "text": "7.1.1 Learning Outcomes:\n\nOutline the four main error structures (poisson, quasipoisson and negative binomial) associated with GLMs.\nEstablish when to use them during analyses.\nValidate their outcomes.\nPlot the summary tables and visualise the relationships of the models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#generalised-linear-models-glms",
    "href": "chap7.html#generalised-linear-models-glms",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.2 Generalised Linear Models (GLMs)",
    "text": "7.2 Generalised Linear Models (GLMs)\nI don’t propose to work through the mathematics of these in any depth but you need to have a solid understanding of mechanics of linear regression models, hence the level of detail in last two workshops. GLMs are an extension of classic linear models with additional link functions and error structures that provides more flexibility in their application to different types of response variables.\n我不打算深入探讨这些模型的数学原理，但您需要对线性回归模型的机制有扎实的理解，这也是前两次研讨会中详细讲解的原因。GLMs 是经典线性模型的扩展，通过引入链接函数（link function）和不同的误差结构，使其在处理不同类型的响应变量时更加灵活。\n\n7.2.1 GLM structure\nThere are three components to any GLM:\n\nRandom component or error structure - specifies the probability distribution of the response variable; e.g., normal (gaussian) distribution for in the case of a classical linear regression model, a binomial distribution for binary, or multinominal, ordinal logistic regression models, poisson, quasipoisson or negative binomial, in the case of a count data (there are other variants!) (Fig. 1).\nSystematic component - specifies the explanatory variables in the model, more specifically, their linear combination; e.g., as we have seen in a linear regression. This will be familiar to you as it is the same as in linear regression.\nLink function which specifies the link between the random and the systematic components. It indicates how the expected value of the response relates to the linear combination of explanatory variables; e.g., for classical regression, or for logistic regression. In all our cases this involves a log exponent link.\n\n该部分指定响应变量的概率分布，例如：\n\n正态（高斯）分布——适用于经典线性回归模型。\n二项分布——用于二分类、多项式或序数逻辑回归模型。\n泊松（Poisson）、准泊松（Quasi-Poisson）或负二项（Negative Binomial）分布——适用于计数数据（当然，还有其他变体！）（见图 1）。\n\n该部分指定模型中的解释变量，更具体地说，是它们的线性组合。\n\n例如，在线性回归中，我们已经见过类似的形式。\n这部分对您来说应该很熟悉，因为它与经典线性回归的结构相同。\n\n该部分建立随机成分与系统成分之间的联系，即响应变量的期望值如何与解释变量的线性组合相关联，例如：\n\n经典回归使用恒等函数。\n逻辑回归使用**对数几率（logit）**函数。\n在我们所有的案例中，都涉及一个对数指数（log-exponent）链接函数。\n\n\n\n7.2.2 GLM assumptions\nGLMs share some assumptions with linear regression models, such as:\n\nThe data are independently distributed, i.e., the cases are independent. Just a in a linear regression.\nThere is a linear relationship between the transformed expected response between the link function and the explanatory variables, for binary logistic regression and poisson regression.\n\n广义线性模型（GLMs）与线性回归模型共享一些假设，例如：\n\n数据是独立分布的，即各个观测值之间相互独立，这与线性回归的假设相同。\n转换后的期望响应值（通过链接函数转换）与解释变量之间存在线性关系，例如在二元逻辑回归和泊松回归中。\n\nBut not others:\n\nThe response variable does not need to be normally distributed, rather it must fits a distribution from an exponential family (e.g. Poisson, multinomial, normal, gamma etc.).\nExplanatory variables can be nonlinear transformations of some original variables.\nThe homogeneity of variance does not need to be satisfied.\nErrors need to be independent but not normally distributed.\nParameter estimates use maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) (do not worry about the details of this).\n响应变量不需要服从正态分布，但必须符合指数族分布（如泊松、多项式、正态、伽马等）。\n解释变量可以是原始变量的非线性变换，不必严格是线性关系。\n不需要满足方差齐性（homogeneity of variance）。\n误差需要独立，但不需要服从正态分布。\n参数估计使用最大似然估计（MLE），而不是普通最小二乘法（OLS）（不必担心具体细节）\n\nAs a result of the above GLMs have two key advantages over traditional linear regression:\n\nThe choice of link is separate from the choice of random component, giving us flexible models models which can be fitted to response data of different formats.\nThey allow for different error structures, accommodating heteroscedasticity and other complexities in the data and providing a more accurate representation of the variance.\nWe do not need to transform the response to have a normal distribution.\nThe models are fitted via maximum likelihood estimation, so likelihood functions and parameter estimates benefit from asymptotic normal and chi-square distributions.\nAll the inference tools and model checking that we will discuss for logistic and Poisson regression models (below) apply for other GLMs too; e.g., Deviance, residuals, confidence intervals, and overdispersion.\n\n✅ 链接函数的选择独立于随机成分的选择，因此模型更加灵活，可以适用于不同格式的响应变量。 ✅ 允许不同的误差结构，能够处理异方差性（heteroscedasticity）和其他数据复杂性，更准确地表示方差。 ✅ 不需要对响应变量进行转换以满足正态分布假设。 ✅ 模型通过最大似然估计拟合，其似然函数和参数估计受益于渐近正态分布和卡方分布。 ✅ 所有的推断工具和模型检验方法（如逻辑回归和泊松回归中的**偏差（Deviance）、残差（Residuals）、置信区间、过度离散（Overdispersion）**等），同样适用于其他 GLMs。\nLet’s conclude this theory section by reviewing types of data which GLMs can use, and their respective error structures and link functions. We will also suggest what kinds of social science datasets might fit these models (Fig. 7.1).\n让我们通过回顾 GLM 可以使用的数据类型及其各自的误差结构和链接函数来结束本理论部分。我们还将建议哪些类型的社会科学数据集可能适合这些模型 (Fig. 7.1).\n\n\n\n\n\n\n\n\n\nFig. 7.1: The mains types Error structures and their link functions mapped onto the different types of data that a GLM can be used to model.\nAs you can see we have 4 main data types (there are more of course!): continuous, counts, binary, and categorical.\n\nContinuous data relate phenomena that are measurable in some way e.g. height, body mass, temperature, river discharge, ppt, nutrients (e.g. total phosporus).\nCount data have particular properties as they cannot be negative and range from zero to infinity (theoretically at least!). Examples might include, diatom counts in lake, fish counts in rivers, counts of insects on riverbanks and so on; they also could be rates e.g. rates of fish deaths, births of birds etc.\nbinary data relate to yes/no categories. e.g. males or females in fish populations, bird species presence or absence at a site (used for species distribution models), presence of chytrid fungus on amphibians, pollinator visits to flowers and so on. These are depicted as either a 1 or a 0, so we are effectively modelling the probability of a particular outcome.\ncategorical data relate to multiclass outcomes of say plant community type at a site (Grassland, Forest, Wetland, Heathland), or animal diets (Herbivore, Carnivore or Omnivore). The can also be ordered, e.g. disturbance severity after a flood (None, Low, Medium, High) and so on.\n\n連續資料與某種可測量的現象相關，例如身高、體重、溫度、河流流量、ppt、營養物質（例如總磷）。\n計數資料具有特殊屬性，因為它們不能為負數，且範圍從零到無限大（至少理論上是如此！）。例如湖泊中的矽藻數量、河流中的魚類數量、河岸上的昆蟲數量等等；它們也可以是速率，例如魚類死亡率、鳥類出生率等。\n二元資料與是/否類別相關，例如魚類族群中的雄性或雌性、某個地點鳥類的存在或不存在（用於物種分佈模型）、兩棲動物身上壺菌的存在、傳粉者對花朵的訪問等等。這些被表示為 1 或 0，因此我們實際上是在模擬特定結果的機率。\n分類資料與多類別結果相關，例如某一地點的植物群落類型（草原、森林、濕地、石楠叢生地）或動物飲食（草食動物、食肉動物或雜食動物）。資料也可以依序排列，例如洪水後的干擾嚴重程度（無、低、中、高）等等。\nObserve also that the link functions for the count, binary and categorical data are log based. This keeps the values as positives as required by the error structure. It also means that to make meaningful inferences you need to back transform (exponentiate) the logged response values to their raw form.\n也要注意，計數、二分類和分類資料的連結函數都是基於對數的。這使得這些值保持為誤差結構所要求的正值。這也意味著，為了做出有意義的推斷，你需要將記錄的反應值反向轉換（指數化）為原始形式。\nTASK: Now take a few minutes to think about what other different types of data there are and what error structure and link functions you might apply to them [~ 5 min].\n任务：现在花几分钟思考一下还有哪些不同类型的数据，以及您可能对它们应用哪些错误结构和链接函数[~ 5 分钟]。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#mathematical-formulation-of-poisson-regression",
    "href": "chap7.html#mathematical-formulation-of-poisson-regression",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.3 Mathematical Formulation of Poisson Regression",
    "text": "7.3 Mathematical Formulation of Poisson Regression\nThe general form of a Poisson regression model is:\n\\(\\text{log}(\\lambda_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k\\)\nWhere: - \\(\\lambda_i\\) is the expected count (mean) for observation \\(i\\). - \\(\\beta_0\\) is the intercept. - \\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\) are the coefficients corresponding to predictor variables \\(x_1, x_2, \\ldots, x_k\\).\n泊松迴歸模型的一般形式為：\n\\(\\text{log}(\\lambda_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k\\)\n其中：- \\(\\lambda_i\\) 是觀測值 \\(i\\) 的預期計數（平均值）。 - \\(\\beta_0\\) 是截距。 - \\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\) 是預測變數 \\(x_1, x_2, \\ldots, x_k\\) 對應的係數。\n\n7.3.1 Interpretation of Coefficients:\n\n\\(\\beta_0\\): The log of the expected count when all predictors are zero.\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\): The change in log(count) associated with a one-unit increase in predictor \\(x_1, x_2, \\ldots, x_k\\).\n\nThe key to using GLMs is mapping the error structure to the structure of your response variable. We can only explore continuous data in today’s class. While this is a little limiting, we cannot realistically work through all the possible permutations of GLMs in one class. Please check the ‘Vignettes’ at the end of the workbook for an example of using binary data, analysed by logistic regression.\n\\(\\beta_0\\)：所有預測變數均為零時，預期計數的對數。\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\)：預測變數 \\(x_1, x_2, \\ldots, x_k\\) 每增加一個單位，對數（計數）的變化量。\n使用廣義線性模型 (GLM) 的關鍵在於將誤差結構對應到反應變數的結構。今天的課程我們只能探索連續數據。雖然這有一定的局限性，但我們實際上無法在一個課程中涵蓋所有可能的廣義線性模型 (GLM) 排列組合。請查看練習冊末尾的“範例”，其中有一個使用邏輯迴歸分析二值資料的範例。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#essential-reading",
    "href": "chap7.html#essential-reading",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.4 Essential reading:",
    "text": "7.4 Essential reading:\n\nhttps://en.wikipedia.org/wiki/Generalized_linear_model.\nRead chapter 6 in A. Zuur, Ieno, and Smith (2007). We have a online copy available via information services.\nhttps://www.utstat.toronto.edu/brunner/oldclass/2201s11/readings/glmbook.pdf. This is a download of the second edition of McCullagh and Nelder (1989). You are not required to read it cover to cover!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#todays-session",
    "href": "chap7.html#todays-session",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.5 Today’s Session",
    "text": "7.5 Today’s Session\n\n7.5.1 Load/install libraries\nList and install packages we need to today’s session.\n\n# List of packages\npackages &lt;- c(\"dplyr\", \"ggplot2\",\"tidyverse\", \"moderndive\", \n\"ggfortify\", \"performance\", \"car\", \"skimr\", \"gridExtra\", \"broom\", \n\"ggeffects\",\"MASS\",\"MuMIn\")\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nRegistered S3 method overwritten by 'MuMIn':\n  method        from \n  nobs.multinom broom\n\n\n[[1]]\n[1] \"dplyr\"     \"ggplot2\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n[[2]]\n[1] \"dplyr\"     \"ggplot2\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"purrr\"     \"readr\"     \"tidyr\"    \n [7] \"tibble\"    \"tidyverse\" \"dplyr\"     \"ggplot2\"   \"stats\"     \"graphics\" \n[13] \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[4]]\n [1] \"moderndive\" \"lubridate\"  \"forcats\"    \"stringr\"    \"purrr\"     \n [6] \"readr\"      \"tidyr\"      \"tibble\"     \"tidyverse\"  \"dplyr\"     \n[11] \"ggplot2\"    \"stats\"      \"graphics\"   \"grDevices\"  \"utils\"     \n[16] \"datasets\"   \"methods\"    \"base\"      \n\n[[5]]\n [1] \"ggfortify\"  \"moderndive\" \"lubridate\"  \"forcats\"    \"stringr\"   \n [6] \"purrr\"      \"readr\"      \"tidyr\"      \"tibble\"     \"tidyverse\" \n[11] \"dplyr\"      \"ggplot2\"    \"stats\"      \"graphics\"   \"grDevices\" \n[16] \"utils\"      \"datasets\"   \"methods\"    \"base\"      \n\n[[6]]\n [1] \"performance\" \"ggfortify\"   \"moderndive\"  \"lubridate\"   \"forcats\"    \n [6] \"stringr\"     \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[11] \"tidyverse\"   \"dplyr\"       \"ggplot2\"     \"stats\"       \"graphics\"   \n[16] \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[7]]\n [1] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"moderndive\" \n [6] \"lubridate\"   \"forcats\"     \"stringr\"     \"purrr\"       \"readr\"      \n[11] \"tidyr\"       \"tibble\"      \"tidyverse\"   \"dplyr\"       \"ggplot2\"    \n[16] \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"   \n[21] \"methods\"     \"base\"       \n\n[[8]]\n [1] \"skimr\"       \"car\"         \"carData\"     \"performance\" \"ggfortify\"  \n [6] \"moderndive\"  \"lubridate\"   \"forcats\"     \"stringr\"     \"purrr\"      \n[11] \"readr\"       \"tidyr\"       \"tibble\"      \"tidyverse\"   \"dplyr\"      \n[16] \"ggplot2\"     \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[21] \"datasets\"    \"methods\"     \"base\"       \n\n[[9]]\n [1] \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"     \"performance\"\n [6] \"ggfortify\"   \"moderndive\"  \"lubridate\"   \"forcats\"     \"stringr\"    \n[11] \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"      \"tidyverse\"  \n[16] \"dplyr\"       \"ggplot2\"     \"stats\"       \"graphics\"    \"grDevices\"  \n[21] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[10]]\n [1] \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"    \n [6] \"performance\" \"ggfortify\"   \"moderndive\"  \"lubridate\"   \"forcats\"    \n[11] \"stringr\"     \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[16] \"tidyverse\"   \"dplyr\"       \"ggplot2\"     \"stats\"       \"graphics\"   \n[21] \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[11]]\n [1] \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"        \n [6] \"carData\"     \"performance\" \"ggfortify\"   \"moderndive\"  \"lubridate\"  \n[11] \"forcats\"     \"stringr\"     \"purrr\"       \"readr\"       \"tidyr\"      \n[16] \"tibble\"      \"tidyverse\"   \"dplyr\"       \"ggplot2\"     \"stats\"      \n[21] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[26] \"base\"       \n\n[[12]]\n [1] \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"      \n [6] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"moderndive\" \n[11] \"lubridate\"   \"forcats\"     \"stringr\"     \"purrr\"       \"readr\"      \n[16] \"tidyr\"       \"tibble\"      \"tidyverse\"   \"dplyr\"       \"ggplot2\"    \n[21] \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"   \n[26] \"methods\"     \"base\"       \n\n[[13]]\n [1] \"MuMIn\"       \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"  \n [6] \"skimr\"       \"car\"         \"carData\"     \"performance\" \"ggfortify\"  \n[11] \"moderndive\"  \"lubridate\"   \"forcats\"     \"stringr\"     \"purrr\"      \n[16] \"readr\"       \"tidyr\"       \"tibble\"      \"tidyverse\"   \"dplyr\"      \n[21] \"ggplot2\"     \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[26] \"datasets\"    \"methods\"     \"base\"       \n\n\nThese are the new packages for today’s session:\n\nggfortify (Tang, Horikoshi, and Li 2016) - Creates validation plots for a range regression model outputs using ggplot wrappers: https://cran.r-project.org/web/packages/ggfortify/index.html\nperformance (Lüdecke et al. 2021) - Part of the ‘easystats’ ecosystem and provides tools for evaluating, comparing, and reporting statistical models. It is particularly useful for assessing the quality, assumptions, and goodness-of-fit of a wide range of regression models, including linear models, generalized linear models, and mixed-effects models: https://cran.r-project.org/web/packages/performance/readme/README.html\nMASS - (Venables and Ripley 2002) short for Modern Applied Statistics with S, authored by Venables and Ripley, provides functions and data sets related to modern statistical methods. The package is widely used for applied statistics and contains tools for various types of statistical analysis: https://cran.r-project.org/web/packages/MASS/index.html\n\n这些是今天课程的新软件包：\n\nggfortify - 使用 ggplot 包装器为范围回归模型输出创建验证图：https://cran.r-project.org/web/packages/ggfortify/index.html\nperformance - “easystats”生态系统的一部分，提供用于评估、比较和报告统计模型的工具。它对于评估各种回归模型的质量、假设和拟合优度特别有用，包​​括线性模型、广义线性模型和混合效应模型：https://cran.r-project.org/web/packages/performance/readme/README.html\nMASS - 带有 S 的现代应用统计学的缩写，由 Venables 和 Ripley 编写，提供与现代统计方法相关的函数和数据集。该软件包广泛用于应用统计，包含各种类型的统计分析工具：https://cran.r-project.org/web/packages/MASS/index.html\n\n\n\n7.5.2 Create your code file\nYou should know how to do this by now! Create it and call it something memorable that links to the week and it the content of the workshop and save to your ‘Codefiles directory’.\n您现在应该知道该怎么做了！创建它并将其命名为与本周和研讨会内容相关的令人难忘的名字，然后保存到您的“Codefiles 目录”。我使用了一种新方法，即使用 Quarto 文档创建 html 文档。您现在可以单击代码链接，代码将被复制到剪贴板。\n\n\n7.5.3 Set Working Directory\nYour WD will be different. This is mine:\n\nsetwd(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Codefiles/LM25556ModuleHandbook\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#glms-for-count-data-poisson-negative-binomial-and-quasipoisson",
    "href": "chap7.html#glms-for-count-data-poisson-negative-binomial-and-quasipoisson",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.6 GLMs for count data (poisson, negative binomial and quasipoisson)",
    "text": "7.6 GLMs for count data (poisson, negative binomial and quasipoisson)\n\n7.6.1 Generalised Linear (Poisson) regression with no overdispersion\nWe’ll start with an example of poisson regression using multiple predictors that is not overdispersed! Gotelli and Ellison (2002) investigated the biographical determinants of ant species richness at a regional scale. We’re going to replicate their poisson regression of ant species richness against latitude, elevation and habitat type. The code here is from Logan (2010) with modification by JPS. These data are ‘counts’ so we need to use a GLM (A. Zuur, Ieno, and Smith 2007). All the same checks are necessary in a poisson regression:\n我們將從一個使用多個預測因子且分佈不過度離散的泊松回歸範例開始！ Gotelli and Ellison (2002) 研究了區域尺度上螞蟻物種豐富度的傳記決定因素。我們將複製他們的泊松回歸模型，該模型將螞蟻物種豐富度與緯度、海拔和棲息地類型關聯起來。此處的代碼來自@Logan2010，並由 JPS 修改。這些數據是“計數”，因此我們需要使用廣義線性模型 (A. Zuur, Ieno, and Smith 2007)。所有相同的檢查在泊松回歸中都是必要的：\nSpecifically these are (note the acronym LINE) (Ismay, Kim, and Valdivia 2025):\n\nLinear regression fits a straight line so it assumes linearity of the data.\nIndependence of observations. We assume each observation has no relationship to another one. This isn’t always the case. We’ll cover this sort of situation later in the module (in week 9).\nNormality of distributions. Our expectation is that the data are drawn from a random pool of potential observations so conform to a normal distribution, which is usually depicted as a ‘bell-shaped’ curve.\nEquality or homogeneity of variances, called homoscedasticity. The assumption is that the variability between the data points is homogeneous and residuals from the regression are not patterned in any way.\n線性迴歸擬合的是一條直線，因此它假設資料呈線性。\n觀測值的獨立性。我們假設每個觀測值與其他觀測值之間沒有關聯。但情況並非總是如此。我們將在本模組的後續部分（第 9 週）討論這種情況。\n分佈的常態性。我們預期資料來自隨機的潛在觀測值池，因此符合常態分佈，通常以「鐘形」曲線來表示。\n變異數相等或同質性，稱為同方差性。假設資料點之間的變異性是同質的，且迴歸殘差沒有任何模式。\n\nBut we need to add some other checks for:\n\nMulticollinearity. We will use Variation Inflation Factors (VIFs) to do that (see week 6).\nOver- or underdispersion. We will test for that using the dispersion parameter (new for this week).\nThe presence of outliers (this is relevant to all regression) which can have a large influence on poisson regression. We’ll use Cook’s Distances \\(D\\) to check for that.\n\n但我們需要添加一些其他檢查：\n\n多重共線性。我們將使用變異膨脹因子 (VIF) 來檢驗（請參閱第 6 週）。\n過度離散或欠離散。我們將使用離散度參數（本週新增）來檢驗。\n異常值的存在（這與所有迴歸分析都相關），它會對泊松迴歸產生很大影響。我們將使用庫克距離來檢驗。\n\nFirst need load the datafiles:\n\n# Load data\ngotelli &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/gotelli.csv\")\n\nRows: 44 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Site, Habitat\ndbl (3): Srich, Latitude, Elevation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n7.6.1.1 Exploratory Data Analysis (EDA)\nRecall from last week that we did three exploratory steps (A. F. Zuur, Ieno, and Elphick 2010):\n\nInspecting a sample of raw values.\nComputing summary statistics.\nCreating data visualizations.\n\n回想一下上周我们做了三个探索性步骤：\n检查原始值样本。\n计算汇总统计数据。\n创建数据可视化。\nWe’d need glimpse() and skim() for steps 1 and 2.\nTASK: Now take a few minutes to look at the datafile. Pay attention to the data classes, missing values and so on [~ 5 min].\n任務：現在花幾分鐘時間查看資料檔。注意資料類別、缺失值等[~ 5 分鐘]。\n\nglimpse(gotelli)\n\nRows: 44\nColumns: 5\n$ Site      &lt;chr&gt; \"TPB\", \"HBC\", \"CKB\", \"SKP\", \"CB\", \"RP\", \"PK\", \"OB\", \"SWR\", \"…\n$ Srich     &lt;dbl&gt; 6, 16, 18, 17, 9, 15, 7, 12, 14, 9, 10, 10, 4, 5, 7, 7, 4, 6…\n$ Habitat   &lt;chr&gt; \"Forest\", \"Forest\", \"Forest\", \"Forest\", \"Forest\", \"Forest\", …\n$ Latitude  &lt;dbl&gt; 41.97, 42.00, 42.03, 42.05, 42.05, 42.17, 42.19, 42.23, 42.2…\n$ Elevation &lt;dbl&gt; 389, 8, 152, 1, 210, 78, 47, 491, 121, 95, 274, 335, 543, 32…\n\n\nNow let’s do some summaries to get a sense of missing data and the variability of the dataset.\n\nskim(gotelli)\n\n\nData summary\n\n\nName\ngotelli\n\n\nNumber of rows\n44\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSite\n0\n1\n2\n3\n0\n22\n0\n\n\nHabitat\n0\n1\n3\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSrich\n0\n1\n7.02\n4.25\n2.00\n4.00\n6.00\n8.25\n18.00\n▇▇▂▁▂\n\n\nLatitude\n0\n1\n43.02\n1.07\n41.97\n42.17\n42.56\n44.29\n44.95\n▇▂▁▂▃\n\n\nElevation\n0\n1\n232.73\n161.16\n1.00\n95.00\n223.00\n353.00\n543.00\n▇▇▅▅▃\n\n\n\n\n\nWe do not have any missing data but some of our variables appear to be quite variable and are likely to be not normally distributed (see the histograms).\n看看这些指标。我们没有任何缺失数据，但我们的一些变量似乎变化很大，可能不呈正态分布（见直方图）。 看看这些指标。我们没有任何缺失数据，但我们的一些变量似乎变化很大，可能不呈正态分布（见直方图）。\n\n\n7.6.1.2 Visualisation\nTo examine the variables in this file quickly, and simultaneously, we’ll use scatterplotMatrix() from the car package to visualise them and their relationships to each other (Fig. 7.2). We use the diagonal argument list(method =\"qqplot\") to create the QQ-plots. regLine and smooth set the colour, line type and line width for the regression and smoother lines.\n為了快速、同步地檢查此檔案中的變量，我們將使用 car 套件中的 scatterplotMatrix() 函數來視覺化它們及其相互關係（圖 7.2）。我們使用對角線參數 list(method =“qqplot”) 來建立 QQ 圖。 regLine 和 smooth 分別設定迴歸線和平滑線的顏色、線型和線寬。\n\nscatterplotMatrix(~Srich \n  + Latitude + Elevation,\n  diag = list(method = \"qqplot\"),\n  regLine = list(col = \"blue\", lwd = 2),   # Linear regression line in blue\n  smooth = list(col.smooth = \"blue\", lty.smooth = 2, lwd.smooth = 2), # Loess in blue, dashed\n   data = gotelli)\n\n\n\n\n\n\n\n\nFig. 7.2: Scattermatrix plot of the full suite of variables. The diagonal line shows the QQ-plots for each variable\nThis plot (Fig. 7.2) isn’t nearly as confusing at it looks! Firstly, look at the diagonal down middle. This shows a QQ-plot of all the variables in the dataset. Latitude and Srich do not conform to a normal distribution. Look up from the diagonal and you see the paired correlations of the variables against each other. Srich declines with increasing Latitude and increasing Elevation. This is pretty much what what macroecological theories might predict i.e. latitude and elevation gradient theories. There is a slightly humped relationship between Latitude and Elevation. Both the linear (solid blue line) and loess smoother (dashed blue line) sit within the CIs, indicating that a simple linear fit will suffice for the analysis.\n這張圖（圖 7.2）並沒有看起來那麼令人困惑！首先，看一下中間的對角線。它顯示了資料集中所有變數的 QQ 圖。緯度和 Srich 不符合常態分佈。從對角線上看，您會看到變數之間的成對相關性。 Srich 隨著緯度和海拔的增加而下降。這幾乎就是宏觀生態理論（即緯度和海拔梯度理論）可能預測的結果。緯度和海拔之間有略微駝峰的關係。線性（藍色實線）和 Loess 平滑線（藍色虛線）都位於信賴區間 (CI) 內，表示簡單的線性擬合足以進行分析。\nWe will plot a boxplot for the factor variable (Habitat) as it is not a numerical variable and scatterplotMatrix is meant for \\(x~y\\) variables (Fig. 7.3). 我們將為因子變量（棲息地）繪製一個箱線圖，因為它不是數值變量，而散點圖矩陣適用於 \\(x~y\\) 變量。\n\n# boxplot\nggplot(gotelli, aes(x=as.factor(Habitat), y=Srich)) + \ngeom_boxplot() +\n  theme_bw() +\n  theme(\n     panel.grid.minor = element_blank(), #remove the minor grids\n    panel.grid.major = element_blank(), #remove the minor grids \n  )\n\n\n\n\n\n\n\n\nFig. 7.3: Boxplot of ant species richness differences between Forest and Bog habitat\nThe boxplot shows that the Srich-Habitat response is likely to conform to homogeneous variability across the groups. The median is fairly central and the IRs are fine. Only one potential outlier is visible. We see that there are more ant speciea in Forest than Bog habitats.\n箱線圖顯示，豐富棲地反應很可能符合各組間的同質性變異。中位數相當居中，IR 也很好。只有一個潛在的異常值可見。我們發現森林棲息地的螞蟻種類比沼澤棲息地多。\nWhile EDA is an important step, allowing to visual likely relationships in the data and modify our analyses accordingly, what really matters is whether to model residuals conform to the assumptions we list above. So we’ll get on and run the model, testing for multicollinearity and overdispersion en route.\n雖然 EDA 是重要的一步，它能夠直觀地展現數據中可能的關係，並相應地修改我們的分析，但真正重要的是殘差模型是否符合我們上面列出的假設。因此，我們將開始運行模型，並在過程中測試多重共線性和過度離散。\n\n\n7.6.1.3 Multicollinearity\nFirst up, we’ll check for multicollinearity. In week 5 we used some correlations to look at the relationships between our pool of explanatory variables but this is better explored using Variation Inflation Factors (VIFs) as we discussed last week: https://online.stat.psu.edu/stat462/node/180/.\n首先，我們將檢查多重共線性。在第 5 週，我們使用了一些相關性來考察解釋變數池之間的關係，但更好的方法是使用變異膨脹因子 (VIF)，正如我們上週討論的那樣：https://online.stat.psu.edu/stat462/node/180/。\nThere is a debate about the use of VIFs in data science, especially where you have large numbers of data points. This dataset is tiny and has only 44 rows (i.w. n=44). As datasets become smaller &lt;100 we need to make some decisions about collinear predictor/explanatory variables. But how do we make a decision? If we are using VIFs then there is a rule of thumb that says if the values are 3 or less we retain the variables (A. F. Zuur and Ieno 2016). Let’s have a look; we use the vif() function from the car package. But even here, there is much debate. Some people suggest &lt;10 for as inclusion parameter. I am minded to be more conservative, especially with small datafiles, so will apply a rule of &lt;3 for inclusion.\n關於在數據科學中使用 VIF 存在爭議，尤其是在數據點數量眾多的情況下。這個資料集很小，只有 44 行（即 n=44）。隨著資料集變得越來越小（小於 100），我們需要對共線預測變數/解釋變數做出一些決策。但該如何做出決策呢？如果我們使用 VIF，那麼有一條經驗法則：如果值小於或等於 3，則保留變數 (A. F. Zuur and Ieno 2016)。讓我們來看看；我們使用 car 套件中的 vif() 函數。但即使在這裡，也存在著許多爭議。有些人建議使用小於 10 的包含參數。我傾向於更加保守，尤其是在資料檔案較小的情況下，因此將應用「小於 3」的包含規則。\nTo check the VIFs we run the glm regression. This is straightforward as it uses the same code as the last week except we replace the lm function with glm. The only other difference is that we need to specify what error structure we need to use with the family = argument. In this case it is poisson.\n这很简单，因为它使用的代码与上周相同，只是我们用 glm 替换了 lm 函数。唯一的区别是我们需要使用 family = 参数指定我们需要使用什么错误结构。在本例中它是 poisson。\nWe use the full model with all possible interactions. 我們使用具有所有可能交互作用的完整模型。\n\ngotelli.glm &lt;- glm(Srich ~ as.factor(Habitat) + Latitude + Elevation, \n                family = poisson, data  = gotelli)\n\nWe use the check_collinearity() function from the performance package for the calculations. Because provides generalised VIFs (Lüdecke et al. 2021). I prefer this to the VIF function in car but it is up to you.\n我們使用 performance 套件中的 check_collinearity() 函數進行計算。因為它提供了廣義方差函數 (Lüdecke et al. 2021)。相較於 car 中的 VIF 函數，我更喜歡這個函數，但最終還是由你決定。\n\ngvif_values &lt;- performance::check_collinearity(gotelli.glm)\nprint(gvif_values)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n               Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n as.factor(Habitat) 1.00 [1.00,     ]     1.00      1.00     [    , 1.00]\n           Latitude 1.06 [1.00, 9.61]     1.03      0.95     [0.10, 1.00]\n          Elevation 1.06 [1.00, 9.61]     1.03      0.95     [0.10, 1.00]\n\n\nThese are all fine. We can run with them. Had we used the interactions, the numbers would have been huge because the habitats are distributed unevenly along the elevation and latitudinal gradients. Have a go, replace the + with * in the script and rerun the model and the check_collinearity() function.\n這些都沒問題。我們可以用它們來運行。如果我們使用了交互作用，數字會非常大，因為棲息地在海拔和緯度梯度上的分佈並不均勻。試一試，將腳本中的“+”替換為“*”，然後重新運行模型和“check_collinearity()”函數。\nWe decide to centre the numerical predictor variables to reduce the impact of any model interactions, and we also they are scale them. This means we can directly compare the coefficients in the model with each other becauses the units are standard deviations. This is done with scale function and the scale and centre arguments from Base R with mutate() function. We add the two new ‘centre-scaled’ variables to our dataframe.\n我們決定將數值預測變數居中，以便它們被縮放到相同的值。這意味著我們可以直接比較模型中的係數。這可以透過 Base R 中的 scale 函數來實現。我們將兩個新的「縮放」變數加入到我們的資料框中。\n\ngotelli &lt;- gotelli %&gt;%\n  mutate(\n    cLatitude  = as.numeric(scale(Latitude,  center = TRUE, scale = TRUE)),\n    cElevation = as.numeric(scale(Elevation, center = TRUE, scale = TRUE))\n  )\n\nNo need to repeat the VIF code. They will be same. 無需重複 VIF 程式碼。它們將是相同的。\nNow we know there are no collinearity problems we move on and fit the full model with interactions. We continue to overwrite the model object name! 現在我們知道不存在共線性問題，我們繼續進行，並用交互項擬合整個模型。我們繼續覆蓋模型物件名稱！\n\ngotelli.glm &lt;- glm(Srich ~ as.factor(Habitat) * cLatitude * cElevation, \n                family = poisson, data  = gotelli)\n\nLets look at the summary table.\n\nsummary(gotelli.glm)\n\n\nCall:\nglm(formula = Srich ~ as.factor(Habitat) * cLatitude * cElevation, \n    family = poisson, data = gotelli)\n\nCoefficients:\n                                               Estimate Std. Error z value\n(Intercept)                                    1.523727   0.104428  14.591\nas.factor(Habitat)Forest                       0.628476   0.129210   4.864\ncLatitude                                     -0.241525   0.113340  -2.131\ncElevation                                    -0.105970   0.110845  -0.956\nas.factor(Habitat)Forest:cLatitude            -0.009535   0.140664  -0.068\nas.factor(Habitat)Forest:cElevation           -0.097558   0.137497  -0.710\ncLatitude:cElevation                           0.081362   0.124289   0.655\nas.factor(Habitat)Forest:cLatitude:cElevation -0.057740   0.154172  -0.375\n                                              Pr(&gt;|z|)    \n(Intercept)                                    &lt; 2e-16 ***\nas.factor(Habitat)Forest                      1.15e-06 ***\ncLatitude                                       0.0331 *  \ncElevation                                      0.3391    \nas.factor(Habitat)Forest:cLatitude              0.9460    \nas.factor(Habitat)Forest:cElevation             0.4780    \ncLatitude:cElevation                            0.5127    \nas.factor(Habitat)Forest:cLatitude:cElevation   0.7080    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 102.763  on 43  degrees of freedom\nResidual deviance:  39.772  on 36  degrees of freedom\nAIC: 216.13\n\nNumber of Fisher Scoring iterations: 4\n\n\nLook at the table. You’ll notice that (unlike a linear regression) we do not have F or T values, or an R-squared or an adjusted R-squared. We have something called deviance \\(D\\). In a Generalized Linear Model (GLM), deviance is a measure of the goodness-of-fit of the model. It compares the fit of the specified model to a saturated model, which is a hypothetical model that fits the data perfectly. i.e. if we drew the regression line it would be a 1:1 fit. The lower the deviance the better the model fit to the data. You can see that our model has a \\(D\\) of 102.763\nWe can estimate the fit by using the following equation which gives us something akin to an \\(R^2\\) value:\n看一下表格。你會注意到（與線性迴歸不同），我們沒有 F 值或 T 值，也沒有 R 平方或調整後的 R 平方。我們有一個叫做偏差 \\(D\\) 的東西。在廣義線性模型 (GLM) 中，偏差是衡量模型適配優度的指標。它將指定模型的適配度與飽和模型進行比較，飽和模型是一個假設的模型，可以完美地擬合資料。也就是說，如果我們畫出迴歸線，擬合度將是 1:1。偏差越低，模型與資料的適合度就越高。你可以看到，我們的模型的 \\(D\\) 為 102.763。\n我們可以使用以下公式估算擬合度，公式給出的值類似於 \\(R^2\\) 的值：\n\\[\n\\text{Proportion of Deviance Explained} = \\frac{\\text{Null Deviance} - \\text{Residual Deviance}}{\\text{Null Deviance}}\n\\]\n\\[\n\\text{偏差解释比例} = \\frac{\\text{零偏差} - \\text{残差}}{\\text{零偏差}}\n\\] 我们可以计算它： We can calculate it.\n\nnull_deviance &lt;- gotelli.glm$null.deviance #\nresidual_deviance &lt;- gotelli.glm$deviance\nproportion_explained &lt;- (null_deviance - residual_deviance) / null_deviance\n\n# Print result\ncat(\"Proportion of Deviance Explained:\", proportion_explained, \"\\n\")\n\nProportion of Deviance Explained: 0.6129791 \n\n\nThe estimate of deviance explained is moderately high at 0.6129791, indicating our model accounts for around 61% of the variation ant species richness across the global sample.\n解釋的偏差估計值較高，為 0.6129791，顯示我們的模型可以解釋全球樣本中螞蟻物種豐富度變異的約 61%。\nNow let’s return to the summary table. Because this is a Poisson GLM with a log link, the Estimate values are on the log scale. They represent the change in the log-count of species richness. When we finish our model selection we will exponentiate them to get the real values. Let’s look at each line:\n現在讓我們回到匯總表。由於這是一個具有對數關聯的泊松廣義線性模型 (GLM)，因此估計值採用對數刻度。它們表示物種豐富度對數計數的變化。完成模型選擇後，我們將對它們求冪以獲得真實值。讓我們看一下每一行：\n\nIntercept: This is the predicted log-count of species richness when all other predictors are at their baseline or zero level. We centered (and scaled) cLatitude and cElevation, so their zero level is their mean. Since Habitat is a factor, its baseline is the first level alphabetically ( the Bog” habitat type). The intercept then is the log-count for the baseline habitat at the average latitude and elevation. It is highly significant.\nas.factor(Habitat)Forest: This is the difference in the log-count between the “Forest” habitat and the baseline “Bog” habitat, holding latitude and elevation constant at their means. The estimate is positive (0.628), so the log-count of species richness is significantly higher in forests compared to the baseline.\ncLatitude is the main effect of latitude. It shows that a one-unit increase in centered latitude (moving one unit north or south of the average), the predicted log-count of species richness decreases by 0.24. This is a significant main effect.\ncElevation is the main effect of elevation. For a one-unit increase in centered elevation, the log-count of species richness decreases by 0.11. However, the p-value (0.3391) is large, so not a statistically significant.\nas.factor(Habitat)Forest:cLatitude: This is the two-way interaction term. It tests whether the relationship between latitude and species richness is different in forests compared to bogs. The coefficient is very close to zero and the p-value (0.9460) is very large. This means there is no significant interaction here; the effect of latitude is roughly the same in both habitats.\nas.factor(Habitat)Forest:cElevation: This is the two-way interaction term. It tests whether the relationship between elevation and species richness is different in forests compared to bogs. This interaction term (-0.0975) is the additional change to that slope when you are in a forest. As you know to get the partial slope for Forests you add this term to the main elevation term: -0.1059 + (-0.0975) = -0.2034. It is not significant.\ncLatitude:cElevation is another two-way term testing whether the effect of latitude gets stronger or weaker as you go up in elevation. For example, a significant interaction could mean that latitude has a strong negative effect at high elevations, but almost no effect at sea level. It is a small coefficient and not significant.\nas.factor(Habitat)Forest:cLatitude:cElevation is the 3-way interaction. These are notoriously difficult to interpret. It goes something like this:\n\nIn the baseline habitat, the effect of latitude gets stronger as elevation increases (a significant two-way interaction).\nBut in forests, the effect of latitude is constant across all elevations (no two-way interaction). It is not significant.\n\n截距：當所有其他預測因子都處於基線或零水平時，這是物種豐富度預測的對數計數。我們將緯度和海拔居中（並進行了縮放），因此它們的零水平是它們的平均值。由於「棲息地」是一個因子，其基線按字母順序排列為第一級（「沼澤」棲息地類型）。截距是基線棲息地在平均緯度和海拔處的對數計數。該值高度顯著。\nas.factor(Habitat)Forest：這是「森林」棲息地與基線「沼澤」棲息地之間對數計數的差值，保持緯度和海拔平均值不變。估計值為正值（0.628），因此森林棲息地的物種豐富度對數計數顯著高於基線棲息地。\ncLatitude 是緯度的主效果。它表明，中心緯度每增加一個單位（即在平均值以北或以南移動一個單位），預測的物種豐富度對數計數就會減少 0.24。這是一個顯著的主效果。\ncElevation 是海拔的主效果。中心海拔每增加一個單位，物種豐富度的對數數就會減少 0.11。然而，p 值（0.3391）很大，因此不具統計意義。\nas.factor(Habitat)Forest:cLatitude：這是一個雙向交互項。它檢驗森林與沼澤中緯度與物種豐富度之間的關係是否不同。此係數非常接近零，p值（0.9460）非常大。這意味著這裡不存在顯著的交互作用；緯度的影響在兩種棲息地中大致相同。\nas.factor(Habitat)Forest:cElevation：這是一個雙向交互項。它檢驗森林與沼澤中海拔與物種豐富度之間的關係是否不同。這個交互項（-0.0975）是當處於森林中時，該坡度的額外變化。眾所周知，要獲得森林的部分坡度，需要將此項添加到主海拔項：-0.1059 + (-0.0975) = -0.2034。結果並不顯著。\ncLatitude:cElevation 是另一個雙向項，用於檢驗緯度效應隨海拔升高而增強還是減弱。例如，顯著的交互作用可能意味著緯度在高海拔地區具有強烈的負面影響，但在海平面地區幾乎沒有影響。這是一個較小的係數，並不顯著。\nas.factor(Habitat)Forest:cLatitude:cElevation 是三向交互項。眾所周知，這類交互項很難解釋。其形式如下：\n在基線棲息地中，緯度效應隨海拔升高而增強（顯著的雙向交互作用）。\n但在森林中，緯度的影響在所有海拔高度上都是恆定的（沒有雙向交互作用）。因此並不顯著。\n\nWe’ll come back to model simplification after we have tested for over- or under-dispersion. 在測試了過度分散或不足分散之後，我們將回到模型簡化。\n\n\n7.6.1.4 Over-underdispersion\nOverdispersion occurs when the observed variability in the response variable is greater than what is expected given statistical model. It is only relevant to GLMs using count data or binary data, such as Poisson and binomial regressions.\n\nPoisson Regression: Assumes that the mean of the response variable equals its variance. Overdispersion occurs if the variance exceeds the mean. Underdispersion if the mean exceeds the variance.\nBinomial Regression: Assumes the variance is proportional to \\(p(1-p)\\), where \\(p\\) is the probability of success). Overdispersion arises if the observed variance is larger than this theoretical variance.\n\n當反應變數的觀測變異性大於給定統計模型的預期變異性時，就會出現過度離散。它只適用於使用計數資料或二元資料的廣義線性模型 (GLM)，例如泊松迴歸和二項迴歸。\n\n泊松迴歸：假設反應變數的平均值等於其變異數。如果變異數超過平均值，則會出現過度離散。如果平均值超過方差，則會出現欠離散。\n二項迴歸：假設變異數與 \\(p(1-p)\\) 成正比，其中 \\(p\\) 為成功機率。如果觀測到的變異數大於理論方差，則會出現過度離散。\n\nOverdispersion can be caused by a lot of things: an excessive numbers of zeros in our response data (i.e. zero inflation), lack of independence of data points (i.e. spatial structure or repeated measurements), missing covariates, clustering of correlated variables, non-linear relationships.\nWe need to check for this in our regression model using either tests in packages, such the performance package or calculate it directly with an equation. We’ll do both. First up the performance package.\n過度離散可能由多種因素造成：反應資料中零值數量過多（即零膨脹）、資料點缺乏獨立性（即空間結構或重複測量）、協變量缺失、相關變數聚類、非線性關係。\n我們需要在迴歸模型中檢查這些因素，可以使用套件中的檢定方法（例如 performance 套件）或直接用方程式計算。我們將兩種方法都用。首先使用 performance 套件。\n\ncheck_overdispersion(gotelli.glm)\n\n# Overdispersion test\n\n       dispersion ratio =  1.130\n  Pearson's Chi-Squared = 40.670\n                p-value =  0.272\n\n\nNo overdispersion detected.\n\n\nAnd hand calculated. 并手工计算。\n\ndispersion_stat &lt;- sum(residuals(gotelli.glm, type = \"pearson\")^2) / gotelli.glm$df.residual\ncat(\"Dispersion Statistic:\", dispersion_stat, \"\\n\")\n\nDispersion Statistic: 1.129723 \n\n\nIf we find a value over 1 then we need to fix this. The dispersion parameter is only slightly over 1, so there is no issue.\n如果我們發現一個大於 1 的值，那麼我們需要修正它。色散參數僅略大於 1，所以沒有問題。\n\n\n7.6.1.5 Refining the best fit model\nClearly, we have a model that is too complicated so it needs some simplification. We could do this using MuMIn as last week or you we can just chop out the interact terms and rerun the model. MuMIn does this anywhere!\n顯然，我們的模型過於複雜，需要進行一些簡化。我們可以像上週一樣使用「MuMIn」來實現，或者直接刪除交互項，重新運行模型。 MuMIn 可以在任何地方進行簡化！\n\noptions(na.action=na.fail) # set options in Base R concerning missing values\n\nsummary(model.avg(dredge(gotelli.glm), fit = TRUE, subset = TRUE))\n\nFixed term is \"(Intercept)\"\nFixed term is \"(Intercept)\"\nFixed term is \"(Intercept)\"\n\n\n\nCall:\nmodel.avg(object = get.models(object = dredge(gotelli.glm), subset = TRUE))\n\nComponent model call: \nglm(formula = Srich ~ &lt;19 unique rhs&gt;, family = poisson, data = \n     gotelli)\n\nComponent models: \n        df  logLik   AICc delta weight\n123      4 -100.52 210.07  0.00   0.43\n1234     5 -100.31 212.21  2.14   0.15\n1236     5 -100.34 212.26  2.19   0.14\n1235     5 -100.50 212.58  2.51   0.12\n12346    6 -100.13 214.54  4.47   0.05\n12345    6 -100.31 214.89  4.82   0.04\n12356    6 -100.32 214.91  4.84   0.04\n13       3 -105.32 217.24  7.17   0.01\n123456   7 -100.13 217.38  7.31   0.01\n135      4 -105.30 219.62  9.55   0.00\n1234567  8 -100.06 220.24 10.17   0.00\n12       3 -108.50 223.60 13.53   0.00\n124      4 -108.28 225.59 15.52   0.00\n23       3 -115.37 237.33 27.26   0.00\n1        2 -116.72 237.72 27.65   0.00\n236      4 -115.18 239.40 29.33   0.00\n3        2 -120.16 244.62 34.55   0.00\n2        2 -123.34 250.98 40.91   0.00\n(Null)   1 -131.56 265.21 55.14   0.00\n\nTerm codes: \n                     as.factor(Habitat)                              cElevation \n                                      1                                       2 \n                              cLatitude           as.factor(Habitat):cElevation \n                                      3                                       4 \n           as.factor(Habitat):cLatitude                    cElevation:cLatitude \n                                      5                                       6 \nas.factor(Habitat):cElevation:cLatitude \n                                      7 \n\nModel-averaged coefficients:  \n(full average) \n                                                Estimate Std. Error Adjusted SE\n(Intercept)                                    1.5292792  0.0995464   0.1026987\nas.factor(Habitat)Forest                       0.6300487  0.1214245   0.1252695\ncElevation                                    -0.1647961  0.0792477   0.0813992\ncLatitude                                     -0.2483057  0.0785800   0.0810422\nas.factor(Habitat)Forest:cElevation           -0.0196356  0.0704268   0.0721761\ncElevation:cLatitude                           0.0107758  0.0412472   0.0423015\nas.factor(Habitat)Forest:cLatitude            -0.0048191  0.0639284   0.0659522\nas.factor(Habitat)Forest:cElevation:cLatitude -0.0001542  0.0085061   0.0087661\n                                              z value Pr(&gt;|z|)    \n(Intercept)                                    14.891  &lt; 2e-16 ***\nas.factor(Habitat)Forest                        5.030    5e-07 ***\ncElevation                                      2.025  0.04291 *  \ncLatitude                                       3.064  0.00218 ** \nas.factor(Habitat)Forest:cElevation             0.272  0.78558    \ncElevation:cLatitude                            0.255  0.79893    \nas.factor(Habitat)Forest:cLatitude              0.073  0.94175    \nas.factor(Habitat)Forest:cElevation:cLatitude   0.018  0.98597    \n \n(conditional average) \n                                              Estimate Std. Error Adjusted SE\n(Intercept)                                    1.52928    0.09955     0.10270\nas.factor(Habitat)Forest                       0.63005    0.12142     0.12527\ncElevation                                    -0.16741    0.07709     0.07933\ncLatitude                                     -0.24847    0.07834     0.08081\nas.factor(Habitat)Forest:cElevation           -0.07942    0.12375     0.12776\ncElevation:cLatitude                           0.04444    0.07430     0.07671\nas.factor(Habitat)Forest:cLatitude            -0.02218    0.13575     0.14013\nas.factor(Habitat)Forest:cElevation:cLatitude -0.05774    0.15417     0.15953\n                                              z value Pr(&gt;|z|)    \n(Intercept)                                    14.891  &lt; 2e-16 ***\nas.factor(Habitat)Forest                        5.030    5e-07 ***\ncElevation                                      2.110  0.03483 *  \ncLatitude                                       3.075  0.00211 ** \nas.factor(Habitat)Forest:cElevation             0.622  0.53418    \ncElevation:cLatitude                            0.579  0.56233    \nas.factor(Habitat)Forest:cLatitude              0.158  0.87422    \nas.factor(Habitat)Forest:cElevation:cLatitude   0.362  0.71740    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\noptions(na.action = \"na.omit\") # reset base R options\n\nNow we have our final model with all three variables in it 1,2 and 3 (or Habitat + Latitude + Elevation). We run it and look at the coefficients.\n現在我們有了最終的模型，其中包含三個變數：1、2 和 3（即棲息地 + 緯度 + 海拔）。我們運行它並查看係數。\n\ngotelli.glm &lt;- glm(Srich ~ as.factor(Habitat) + cLatitude + cElevation, \n                   family=poisson, data = gotelli)\nsummary(gotelli.glm)\n\n\nCall:\nglm(formula = Srich ~ as.factor(Habitat) + cLatitude + cElevation, \n    family = poisson, data = gotelli)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               1.52744    0.09857  15.496  &lt; 2e-16 ***\nas.factor(Habitat)Forest  0.63544    0.11957   5.315 1.07e-07 ***\ncLatitude                -0.25229    0.06598  -3.824 0.000131 ***\ncElevation               -0.18390    0.06042  -3.044 0.002337 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 102.76  on 43  degrees of freedom\nResidual deviance:  40.69  on 40  degrees of freedom\nAIC: 209.04\n\nNumber of Fisher Scoring iterations: 4\n\n\nTASK: Have a go at sorting out the proportion of deviance explain (the code is above) [~ 5 min].\n任務：試著整理偏差解釋的比例（代碼在上面）[~ 5 分鐘]。\nWe need to validate the model before it can be interpreted.\n\n\n7.6.1.6 Model validation\nPoisson regression shares many of the properties of linear regression. Notwithstanding the flexibility of GLMs, we still have to validate the models by examining residual spreads (more below). A. F. Zuur and Ieno (2016) provide a clear protocol to follow in respect to these steps. Note that because it is a GLM we need to use Pearson residuals not standardised residuals to do this. We will use the autoplot() function from the ggfortify package to create our validation plots. We will add a method= argument to specify we ran a GLM model for clarity, but the package checks for model type (Fig. 7.4).\n泊松迴歸與線性迴歸有許多共同的性質。儘管廣義線性模型 (GLM) 非常靈活，我們仍然需要透過檢查殘差差值來驗證模型（詳見下文）。 A. F. Zuur and Ieno (2016) 提供了關於這些步驟的清晰協議。請注意，由於這是一個廣義線性模型 (GLM)，因此我們需要使用皮爾遜殘差而不是標準化殘差來執行此操作。我們將使用 ggfortify 套件中的 autoplot() 函數來建立驗證圖。為了清晰起見，我們將新增 method= 參數來指定我們執行的是 GLM 模型，但套件會檢查模型類型（圖 7.3）。\n\n## simulate residuals\nautoplot(gotelli.glm, method=\"glm\")\n\n\n\n\n\n\n\n## plot simulated residuals\n\nFig. 7.4: Validation plots for model gotelli.glm\nThis plot shows us a Q-Q plot output which we have seen before and plots the residuals as well. As before we do not want to see patterns in these. These look excellent.\nWe check directly for outliers using Cook’s distance (D) (Fig. 7.5). The which=4 argument isolates this plot. The ‘rule of thumb’ here is that we don’t want to see values of greater that 1.\n此圖展示了我們之前見過的 Q-Q 圖輸出，並繪製了殘差。和之前一樣，我們不希望在這些圖中看到任何模式。這些圖看起來非常棒。\n我們直接使用 Cook 距離 (D) 檢查異常值（圖 7.5）。 which=4 參數隔離了此圖。這裡的「經驗法則」是，我們不希望看到大於 1 的值。\n\nplot(gotelli.glm, which=4)\n\n\n\n\n\n\n\n\nFig. 7.5: Cook’s distance (D) plot for our model showing the lack of influential/outliers. The vertical bars on the X axis show the D for each data point.\nWe see no issues in terms of outliers, so now we have the final task of validating each variable in the model (Fig. 7.6). As this is a GLM with use the Pearson’s residuals.\n我們沒有發現任何異常值問題，所以現在我們要做的最後一項任務是驗證模型中的每個變數（圖 7.6）。由於這是一個廣義線性模型 (GLM)，因此使用了皮爾遜殘差。\n\npres &lt;- residuals.glm(gotelli.glm, type=\"pearson\") # strip the residuals\n#plot against the predictor variables in a panel plot\npar(mfrow = c(2, 2))  # Set 3 rows and 2 columns\nplot(pres ~ gotelli$cLatitude)\nplot(pres ~ gotelli$cElevation)\nboxplot(pres ~ as.factor(gotelli$Habitat))\n\n\n\n\n\n\n\n\nFig. 7.6: Residual patterns for all the explanatory (or predictor) variables.\nThere residual spreads for cLatitude and cElevation are fine and exhibit a random spread, balanced around zero. The boxplots show a little more variability in the medians and IRs but are also fine.\nRemember to reset your graphics window to a 1 x 1 panel. 記得將圖形視窗重設為 1 x 1 面板。\n\npar(mfrow = c(1, 1))\n\n\n\n7.6.1.7 Model interpretation\nWe can see that there are significant effects for all variables:\n\nThe Habitat (Bog v Forest) differs - Forests have higher species richness (Srich) values than Bogs (coefficient is a positive 0.63544).\nIncreases in Latitude lead to a decrease in species richness (slope coefficient = -0.25229).\nIncreases Elevation lead to a decrease in species richness (slope coefficient = -0.18390).\n\n我們可以看到，所有變數都存在顯著影響：\n棲息地（沼澤與森林）不同 - 森林的物種豐富度 (Srich) 值高於沼澤（係數為正 0.63544）。\n緯度的增加導致物種豐富度下降（坡度係數 = -0.25229）。\n海拔的增加導致物種豐富度下降（坡度係數 = -0.18390）。\nRemember the model coefficients are log values as a result of the log-link function the glm uses. We need to expontentiate them to interpret the model outcomes properly. We can do this in one line of code (see below). But before we do this, let’s clarify what exponentiation does. It does not convert them to standard deviation units, created by our centreing and scaling. Instead, it converts them from the log scale to a multiplicative scale. When exponentiating a coefficient we get an Incidence Rate Ratio (IRR).\nBefore exponentiation, the model is additive, so a one-unit increase in X leads to a \\(\\beta\\) addition to the log-count (of ant species richness): log(Count) = Intercept + \\(\\beta * X\\)\nAfter exponentiation, the relationship is multiplicative. A one-unit increase in X multiplies the expected count by exp(\\(\\beta\\)): Count = exp(Intercept) \\(* exp(\\beta)^X\\).\n請記住，模型係數是對數，這是 glm 使用的對數連結函數的結果。我們需要對它們進行指數運算才能正確解釋模型結果。我們可以用一行程式碼完成此操作（見下文）。但在執行此操作之前，讓我們先解釋一下指數運算的作用。它不會將它們轉換為由中心化和縮放產生的標準差單位。相反，它會將它們從對數刻度轉換為乘法刻度。對係數進行指數運算後，我們會得到發生率比 (IRR)。\n在指數運算之前，模型是加性的，因此 X 值每增加一個單位，對數計數（螞蟻物種豐富度）就會增加 \\(\\beta\\)：log(Count) = Intercept + \\(\\beta * X\\)\n指數運算之後，兩者的關係是乘性的。 X 值每增加一個單位，預期計數就會乘以 exp(\\(\\beta\\))：Count = exp(Intercept) \\(* exp(\\beta)^X\\)。\n\ntidy(gotelli.glm, exponentiate = TRUE)\n\n# A tibble: 4 × 5\n  term                     estimate std.error statistic  p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                 4.61     0.0986     15.5  3.68e-54\n2 as.factor(Habitat)Forest    1.89     0.120       5.31 1.07e- 7\n3 cLatitude                   0.777    0.0660     -3.82 1.31e- 4\n4 cElevation                  0.832    0.0604     -3.04 2.34e- 3\n\n\nA word on the code. The exponentiate = TRUE argument is in the tidy() function from the broom package and it creates a dataframe that automatically exponentiates the model coefficients from the gotelli.glm model object. We interpret the outcomes in this way:\n\nfor the intercept,\n\nLog-scale Estimate: 1.52744 (this number is the logged coefficient from the summary table)\nExponentiated Value: exp(1.52744) ≈ 4.6063520\nInterpretation: At the baseline level (the first habitat type = Bog, at the mean latitude and mean elevation), the model predicts a mean species richness of 4.6 species.\n\nfor as.factor(Habitat)Forest,\n\nLog-scale Estimate: 0.63544 (this number is the logged coefficient from the summary table)\nExponentiated Value: exp(0.63544) ≈ 1.8878505\nInterpretation: Holding all other variables constant, the predicted species richness in a “Forest” habitat is 1.89 times that of the baseline habitat. It’s an 89% increase on the Bog species richness.\n\nfor cLatitude,\n\nLog-scale Estimate: -0.25229 (this number is the logged coefficient from the summary table)\nExponentiated Value: exp(-0.25229) ≈ 0.7770181\nInterpretation: For every one-unit increase in centered latitude, the predicted species richness is multiplied by 0.777. This is equivalent to a 22.3% decrease (since 1 - 0.777 = 0.223).\n\nfor cElevation,\n\nLog-scale Estimate: -0.18390 (this number is the logged coefficient from the summary table)\nExponentiated Value: exp(-0.18390) ≈ 0.8320179\nInterpretation: For every one-unit increase in centered elevation, the predicted species richness is multiplied by 0.832. This is equivalent to a 16.8% decrease (since 1 - 0.832 = 0.168).\n\n\n關於程式碼，exponentiate = TRUE 參數位於 broom 套件的 tidy() 函數中，它會建立一個資料框，自動對 gotelli.glm 模型物件中的模型係數進行指數運算。我們這樣解釋結果：\n截距：\n對數尺度估計值：1.52744（此數字是總表中對數後的係數）\n指數值：exp(1.52744) ≈ 4.6063520\n解釋：在基線水平（第一個棲息地類型 = 沼澤，平均緯度和平均海拔），模型預測的平均物種豐富度為 4.6 個物種。\n對於 as.factor(Habitat)Forest，值為 1.8878505\n對數估計值：0.63544（此數字是總表中對數後的係數）\n指數值：exp(1.52744) ≈ 1.8878505\n解釋：假設其他變數保持不變，則「森林」棲息地的預測物種豐富度是基線棲息地的 1.89 倍。這比沼澤地的物種豐富度增加了 89%。\n對於 cLatitude，\n對數估計值：-0.25229（此數字是總表中對數後的係數）\n指數值：exp(-0.25229) ≈ 0.7770181\n解釋：中心緯度每增加一個單位，預測物種豐富度就會乘以 0.777。這相當於下降了 22.3%（因為 1 - 0.777 = 0.223）。\n對於 cElevation，\n對數尺度估計值：-0.18390（此數字是總表中對數後的係數）\n指數值：exp(-0.18390) ≈ 0.8320179\n解釋：中心海拔每增加一個單位，預測的物種豐富度就會乘以 0.832。這相當於下降了 16.8%（因為 1 - 0.832 = 0.168）。\n\n\n7.6.1.8 Plot a figure to support the story\nOur final task, now we have validated the model, is to create a figure that supports the story. To do this we need to calculate and plot each predictor to visualize their effect on the response variable while holding other predictors constant. This approach is called a marginal effects plot or partial dependence plot, and it’s a useful way to interpret the model. And you have encountered it before. We hold cElevation as the constant and plot Srich against Latitude, with Habitat displayed as separate regression lines. We need to calculate the means and standard deviations to create the predictions for each explanatory variable then plot the outcome. Because it is a glm, we’ll need to exponentiate them back to the measurement scale after calculating the predictions. Luckily, R has this covered with type = `response=` argument in the predict()` function! The predictions must be calculated on the scales that the model was trained on - i.e. or run using i.e. centred and scaled, before the exponentiation. This process is instructive because it is effectively the same process we use to generate the partial slopes (we did that last week).\n現在我們已經驗證了模型，我們的最終任務是建立一個圖表來支持我們的研究。為此，我們需要計算並繪製每個預測變量，以視覺化它們對反應變量的影響，同時保持其他預測變量不變。這種方法稱為邊際效應圖或部分依賴圖，它是解釋模型的有效方法。您之前也遇過這種方法。我們將海拔高度設為常數，並繪製海拔高度與緯度的關係圖，棲息地則顯示為單獨的迴歸線。我們需要計算平均值和標準差來為每個解釋變數建立預測值，然後繪製結果。由於它是一個全域線性模型 (GLM)，因此在計算預測值後，我們需要將它們指數化回到測量尺度。幸運的是，R 在 predict() 函數中使用 type = response= 參數解決了這個問題！預測值必須在模型訓練的尺度上計算，即在指數化之前，使用中心化和縮放的尺度來運行。這個過程很有啟發性，因為它實際上與我們用來產生部分斜率的過程相同（我們上週就這樣做了）。\nWe start by creating the predictions, ensuring that the data are centred and scaled.\n我們首先建立預測，確保資料居中且按比例縮放。\n\n# Ensure your categorical variable is a factor\ngotelli$Habitat &lt;- as.factor(gotelli$Habitat)\n\n# Calculate the mean and sd directly from the RAW data columns in our gotelli dataframe\nlat_center &lt;- mean(gotelli$Latitude, na.rm = TRUE)\nlat_scale  &lt;- sd(gotelli$Latitude, na.rm = TRUE)\nelev_mean  &lt;- mean(gotelli$Elevation, na.rm = TRUE)\nelev_center &lt;- mean(gotelli$Elevation, na.rm = TRUE)\nelev_scale  &lt;- sd(gotelli$Elevation, na.rm = TRUE)\n\n\n# Create a grid of predictor values for the plot\nprediction_grid &lt;- expand.grid(\n  Latitude = seq(42, 45, length.out = 100),\n  Habitat = levels(gotelli$Habitat), # it is essential the variable is a factor not a character class\n  Elevation = elev_mean\n)\n\n# Transform the grid to match the model's predictors \nprediction_grid_transformed &lt;- prediction_grid |&gt;\n  mutate(\n    cLatitude = (Latitude - lat_center) / lat_scale,\n    cElevation = (Elevation - elev_center) / elev_scale\n  )\n\n# Generate predictions \npredictions &lt;- predict(gotelli.glm, \n                       newdata = prediction_grid_transformed, \n                       type = \"response\", # this command exponentiates the logged values back to their measured state.\n                       se.fit = TRUE)\n\n# Combine everything into a final, clean prediction data frame\npredictions_df &lt;- cbind(prediction_grid, \n                        predicted_Srich = predictions$fit,\n                        se = predictions$se.fit) |&gt;\n  mutate(\n    lwr = predicted_Srich - 1.96 * se,\n    upr = predicted_Srich + 1.96 * se\n  )\n\nNow plot the figure (Fig. 7.7)! 現在繪製該圖形！\n\n# Create the plot \n# Notice that the first layer now uses our initial 'gotelli' data frame.\nggplot() +\n  \n  # Layer 1: The raw data points from the 'gotelli' data frame\n  geom_point(data = gotelli, aes(x = Latitude, y = Srich, shape = Habitat), size = 2.5) +\n  \n  # Layer 2 & 3: The predictions (from the new 'predictions_df')\n  geom_ribbon(data = predictions_df, \n              aes(x = Latitude, ymin = lwr, ymax = upr, fill = Habitat), \n              alpha = 0.2) +\n  geom_line(data = predictions_df, \n            aes(x = Latitude, y = predicted_Srich, color = Habitat), \n            linewidth = 1) +\n\n  # code for scales, labse) \n  scale_color_manual(values = c(\"Forest\" = \"black\", \"Bog\" = \"black\")) +\n  scale_fill_manual(values = c(\"Forest\" = \"gray50\", \"Bog\" = \"gray50\")) +\n  scale_shape_manual(values = c(\"Forest\" = 16, \"Bog\" = 21)) +\n  labs(\n    title = \"A Comparison of Global Ant Species Richness in Forest / Bog Habitats and Latitude\",\n    x = \"Latitude\",\n    y = \"Ant Species Richness\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"right\",\n    legend.box.background = element_rect(color = \"white\")\n  )\n\n\n\n\n\n\n\n\nFig. 7.7: The supporting graph for our gotelli.glm model\n\n\n\n7.6.2 Quasi-poisson and Negative Binomial regression: Dealing with Overdispersion\nWe are now going to repeat an analysis on a dataset that is overdispersed. How do we overdispersion if we find it? In short, we either find the culprit of the overdispersion, fix it, and (re)apply poisson regression, or we apply a different error structure to the data (see Fig. 7.1 in the Introduction section, clickable link to the right!), either, Quasi-poisson or Negative Binomial regression.\n我們現在要對一個過度離散的資料集重複分析。如果發現過度離散，該如何解決？簡而言之，我們要么找到過度離散的根源，修復它，然後（重新）應用泊松回歸，要么對數據應用不同的誤差結構（參見引言部分的圖 7.1，右側可點擊鏈接！），擬泊松或負二項式回歸。\nWe’re going to use a dataset on amphibian roadkills from A. F. Zuur et al. (2009). It was created for a study examining the impact of roads on amphibian habitat fragmentation. The dataset is called ‘RoadKills.csv’.\nLoad the data and do the normal run of ‘look sees’ - i.e. exploration.\n我們將使用來自@Zuur2009a的兩棲類路殺資料集。該資料集是為一項研究道路對兩棲類棲息地破碎化影響的研究創建的。資料集名為「RoadKills.csv」。\n載入資料並進行常規的「檢視」操作，即探索。\n\nroad &lt;- read_csv(\"~/Documents/GitHub/Teaching/LM_25556Environmental_Analysis/Data/RoadKills.csv\")\n\nRows: 52 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (23): Sector, X, Y, BufoCalamita, TOT.N, S.RICH, OPEN.L, OLIVE, MONT.S, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n7.6.2.1 EDA work\nWe will use the glimpse() and skim() functions to look at the variables.\n\nglimpse(road)\n\nRows: 52\nColumns: 23\n$ Sector       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ X            &lt;dbl&gt; 260181, 259914, 259672, 259454, 259307, 259189, 259092, 2…\n$ Y            &lt;dbl&gt; 256546, 256124, 255688, 255238, 254763, 254277, 253786, 2…\n$ BufoCalamita &lt;dbl&gt; 5, 1, 40, 27, 67, 56, 27, 37, 8, 16, 11, 15, 13, 3, 6, 4,…\n$ TOT.N        &lt;dbl&gt; 22, 14, 65, 55, 88, 104, 49, 66, 26, 47, 35, 55, 44, 30, …\n$ S.RICH       &lt;dbl&gt; 3, 4, 6, 5, 4, 7, 7, 7, 7, 6, 6, 8, 6, 6, 5, 6, 8, 7, 7, …\n$ OPEN.L       &lt;dbl&gt; 22.684, 24.657, 30.121, 50.277, 43.609, 31.385, 24.810, 5…\n$ OLIVE        &lt;dbl&gt; 60.333, 40.832, 23.710, 14.940, 35.353, 17.666, 9.786, 1.…\n$ MONT.S       &lt;dbl&gt; 0.000, 0.000, 0.258, 1.783, 2.431, 0.000, 0.000, 0.000, 1…\n$ MONT         &lt;dbl&gt; 0.653, 0.161, 10.918, 26.454, 11.330, 43.678, 60.660, 25.…\n$ POLIC        &lt;dbl&gt; 4.811, 2.224, 1.946, 0.625, 0.791, 0.054, 0.022, 11.263, …\n$ SHRUB        &lt;dbl&gt; 0.406, 0.735, 0.474, 0.607, 0.173, 0.325, 0.055, 0.092, 1…\n$ URBAN        &lt;dbl&gt; 7.787, 27.150, 28.086, 0.831, 2.452, 2.730, 1.001, 0.083,…\n$ WAT.RES      &lt;dbl&gt; 0.043, 0.182, 0.453, 0.026, 0.000, 0.039, 0.114, 0.224, 0…\n$ L.WAT.C      &lt;dbl&gt; 0.583, 1.419, 2.005, 1.924, 2.167, 2.391, 1.165, 2.428, 2…\n$ L.D.ROAD     &lt;dbl&gt; 3330.189, 2587.498, 2149.651, 4222.983, 2219.302, 1005.62…\n$ L.P.ROAD     &lt;dbl&gt; 1.975, 1.761, 1.250, 0.666, 0.653, 1.309, 0.685, 0.677, 0…\n$ D.WAT.RES    &lt;dbl&gt; 252.113, 139.573, 59.168, 277.842, 967.808, 560.000, 93.1…\n$ D.WAT.COUR   &lt;dbl&gt; 735.000, 134.052, 269.029, 48.751, 126.102, 344.444, 95.1…\n$ D.PARK       &lt;dbl&gt; 250.214, 741.179, 1240.080, 1739.885, 2232.130, 2724.089,…\n$ N.PATCH      &lt;dbl&gt; 122, 96, 67, 63, 59, 49, 35, 55, 52, 26, 24, 25, 17, 21, …\n$ P.EDGE       &lt;dbl&gt; 553.936, 457.142, 432.360, 421.292, 407.573, 420.289, 298…\n$ L.SDI        &lt;dbl&gt; 1.801, 1.886, 1.930, 1.865, 1.818, 1.799, 1.593, 1.627, 1…\n\n\nWe have 52 rows (i.e. individual counts of squashed amphibians on roads) and 23 variables (or columns). It is worth describing each variable:\n\nsector. This is road survey sector (numbered 1-52) where the road kills were counted.\nX. This is the \\(x\\) coordinate of the sector. UTM coordinates of the middle point of each segment (geographic location).\nY. This is \\(y\\) coordinate of the sector. UTM coordinates of the middle point of each segment (geographic location).\nBufoCalamita. The total number of kills of Natterjack Toad, Bufo calamita Laurenti, 1768.\nTOT.N. The total number of amphibian road kills counted during the survey work.\nS.RICH. The total number of different amphibian species killed.\nOPEN.L. Area (ha) of “Open lands” in the 2 km wide strip centred on the road segment.\nOLIVE. Area (ha) of olive groves in that strip.\nMONT.S. Area (ha) of “montado with shrubs” (i.e. open oak woodland with a shrub understory).\nMONT. Area (ha) of “montado without shrubs” (oak woodland without shrubs).\nPOLIC. Area (ha) of “policulture” – i.e. mixed land uses/polyled land use class.\nSHRUB. Area (ha) of shrubland in the strip.\nURBAN. Area (ha) of urban land in the strip.\nWAT.RES. Area (ha) of water reservoirs in the strip.\nL.WAT.C. Length (km) of water courses in the strip.\nL.D.ROAD. Length (m) of dirty / unpaved roads in the strip.\nL.P.ROAD. Length (km) of paved roads in the strip.\nD.WAT.RES. Distance from the segment centre to the nearest water reservoir (m).\nD.WAT.COUR. Distance from the segment centre to the nearest water course (m).\nD.PARK. Distance (along the road) to the Natural Park (S. Mamede Natural Park) – essentially how far the segment is from the preserved park area, which is particularly humid and well‐preserved.\nN.PATCH. Number of habitat patches (in the strip) i.e. patch count – landscape structure variable.\nP.EDGE. Perimeter of edges between different land‐cover types (i.e. the total edge length between habitat patches) – a measure of habitat heterogeneity.\nL.SDI. Landscape Shannon Diversity Index (SDI) – a diversity index of land cover classes in each strip, measuring heterogeneity.\n\nAll the variables are double (dbl) i.e. numerical; either decimal or integer. The response variable we will use is ‘TOT.N’, the total number of kills.\n所有變數均為雙精確度 (dbl)，即數值型；可以是小數，也可以是整數。我們將使用的反應變數是“TOT.N”，即總擊殺數。\n\nchecks &lt;- skim(road) # create tibble (a form dataframe created by dplyr)\nchecks\n\n\nData summary\n\n\nName\nroad\n\n\nNumber of rows\n52\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSector\n0\n1\n26.50\n15.15\n1.00\n13.75\n26.50\n39.25\n52.00\n▇▇▇▇▇\n\n\nX\n0\n1\n259240.81\n709.35\n258260.00\n258668.00\n259120.00\n259935.25\n260370.00\n▇▆▃▃▇\n\n\nY\n0\n1\n244211.00\n7408.67\n231911.00\n238006.50\n244162.00\n250495.75\n256546.00\n▇▇▇▇▇\n\n\nBufoCalamita\n0\n1\n8.13\n14.08\n0.00\n0.00\n3.00\n8.25\n67.00\n▇▁▁▁▁\n\n\nTOT.N\n0\n1\n25.90\n24.28\n2.00\n7.00\n17.50\n34.25\n104.00\n▇▃▂▁▁\n\n\nS.RICH\n0\n1\n4.69\n1.74\n1.00\n3.00\n5.00\n6.00\n8.00\n▂▃▇▂▃\n\n\nOPEN.L\n0\n1\n36.18\n26.50\n0.74\n15.70\n28.53\n51.76\n97.57\n▇▇▃▃▂\n\n\nOLIVE\n0\n1\n7.35\n12.76\n0.00\n0.00\n0.67\n8.47\n60.33\n▇▂▁▁▁\n\n\nMONT.S\n0\n1\n1.08\n2.08\n0.00\n0.00\n0.00\n1.18\n9.43\n▇▁▁▁▁\n\n\nMONT\n0\n1\n48.12\n30.36\n0.00\n25.05\n44.23\n75.40\n95.08\n▆▇▆▆▇\n\n\nPOLIC\n0\n1\n0.60\n1.72\n0.00\n0.00\n0.06\n0.33\n11.26\n▇▁▁▁▁\n\n\nSHRUB\n0\n1\n0.23\n0.34\n0.00\n0.03\n0.09\n0.21\n1.74\n▇▁▁▁▁\n\n\nURBAN\n0\n1\n2.77\n6.21\n0.00\n0.00\n0.20\n1.55\n28.09\n▇▁▁▁▁\n\n\nWAT.RES\n0\n1\n0.32\n0.96\n0.00\n0.00\n0.04\n0.21\n6.31\n▇▁▁▁▁\n\n\nL.WAT.C\n0\n1\n1.56\n1.00\n0.00\n0.81\n1.55\n2.20\n3.95\n▇▇▇▅▂\n\n\nL.D.ROAD\n0\n1\n2119.12\n1234.74\n105.31\n1062.08\n2098.05\n2983.80\n4891.47\n▇▇▇▃▃\n\n\nL.P.ROAD\n0\n1\n0.96\n0.53\n0.57\n0.64\n0.68\n1.18\n2.96\n▇▂▁▁▁\n\n\nD.WAT.RES\n0\n1\n703.52\n416.67\n59.17\n381.91\n674.61\n958.20\n1883.00\n▇▇▇▂▁\n\n\nD.WAT.COUR\n0\n1\n288.68\n281.68\n15.18\n91.58\n196.01\n359.58\n1165.00\n▇▂▂▁▁\n\n\nD.PARK\n0\n1\n12680.89\n7327.19\n250.21\n6573.45\n12719.63\n18780.09\n24884.80\n▇▇▇▇▇\n\n\nN.PATCH\n0\n1\n34.40\n21.50\n15.00\n19.00\n28.00\n40.25\n122.00\n▇▂▁▁▁\n\n\nP.EDGE\n0\n1\n289.10\n103.25\n125.23\n197.81\n257.38\n377.60\n553.94\n▇▇▅▆▁\n\n\nL.SDI\n0\n1\n1.60\n0.22\n1.12\n1.42\n1.60\n1.80\n1.96\n▂▇▆▅▇\n\n\n\n\n\nWe have no missing values, quite a number of zeros (see p0 column) is some variables, and the means an medians (p.50) are quite variable suggesting some normality issues. Click on the ‘checks’ tibble in ‘Environment’ window to View()` it.\n我們沒有缺失值，但有些變數中有相當多的零（請參閱 p0 列），且平均值和中位數（參見第 50 頁）變化較大，表示存在一些常態性問題。點擊“環境”視窗中的“檢查”選項卡，即可“查看”它。\nWe are scoping, so we could use scatterplotMatrix() to view them all at once (as we did above) but it would be a difficult to interpret with some many variables. Instead, we’ll create a matrix of Q-Q plots using ggplot2 and the facet_wrap() argument to eyeball for normality and scatterplot and correlations to examine the relationships between the variables. The data are in wide format currently, so we need to transform it long format to use in ggplot2. Note the use of dplyr::select() to avoid the conflict with the MASS package which also has a select() function. We need to retain TOT.N and BufoCalamita as possible response variables in our new dataframe, so we exclude using the cols argument” -c(TOT.N, BufoCalamita, S.RICH).\n我們正在確定範圍，因此可以使用 scatterplotMatrix() 一次查看所有資料（就像上面那樣），但如果變數太多，解釋起來會比較困難。因此，我們將使用 ggplot2 建立 Q-Q 圖矩陣，並使用 facet_wrap() 參數來觀察常態性，並使用散佈圖和相關性來檢查變數之間的關係。資料目前為寬格式，因此我們需要將其轉換為長格式以便在 ggplot2 中使用。請注意使用 dplyr::select() 以避免與 MASS 套件（它也包含 select() 函數）衝突。我們需要在新的資料框中保留 TOT.N 和 BufoCalamita 作為可能的反應變量，因此我們使用 cols 參數 -c(TOT.N, BufoCalamita, S.RICH) 進行排除。\n\n# Select only the columns you want to plot\n  # We need TOT.N (our response) plus all the desired X-variables (possible explanatories)\neda &lt;- road %&gt;% \ndplyr::select(\n    TOT.N, BufoCalamita, S.RICH, OPEN.L, OLIVE, MONT.S, MONT,\n    POLIC, SHRUB, URBAN, WAT.RES, L.WAT.C, L.D.ROAD, L.P.ROAD, \n    D.WAT.RES, D.WAT.COUR, D.PARK, N.PATCH, P.EDGE, L.SDI\n  ) %&gt;%\n  pivot_longer(\n    cols = -c(TOT.N, BufoCalamita, S.RICH),             # Pivot all columns EXCEPT TOT.N,BufoCalamita,S.RICH\n    names_to = \"variables\",    # New column for the name of the X-predictor\n    values_to = \"value\"       # New column for the value of the X-predictor\n  )\n\nPlot the Q-Q plots (Fig. 7.8). You done this before in week 3, have a look at the code. The only new element is the scales = \"free\" argument in the fact_wrap() function.\n繪製 Q-Q 圖（圖 7.8）。你之前在第 3 週已經完成過，請查看代碼。唯一的新元素是 fact_wrap() 函數中的 scales = \"free\" 參數。\n\nggplot(eda, aes(sample = value)) +\n  stat_qq() + \n  stat_qq_line(col=\"red\") +\n  facet_wrap(~ variables, scales = \"free\")\n\n\n\n\n\n\n\n\nFig. 7.8: Q-Q plots of the potential explanatory variables\nNot many of these look to conform to the normality assumption. However, we are only really interested in our response variable, TOT.N, in this instance. We excluded it from the pivot because we need it for the scatterplots in the next step. Lets look at it now (Fig. 7.9). We return the wide data for this, i.e. the road dataframe. We would need to do the same for BufoCalamita or S.RICH if we planned to use those as response variables.\n\nggplot(road, aes(sample = TOT.N)) +\ngeom_qq(color = \"black\", shape = 1) + # shape = 1 gives open circles\n  # This line shows where the points *should* fall if the data were perfectly normal.\n  geom_qq_line(color = \"blue\", linewidth = 1)\n\n\n\n\n\n\n\n\nFig. 7.9: Q-Q plot of our response variable TOT.N\nIt is a not looking great! Let’s continue exploring the data. We look for linearity next. We have the data in long format we can run scatterplots for TOT.N against each potential explanatory to examine the patterns and look for a linear response (Fig. 7.9). This is an important assumption for glm (remember the LINE acronym we discussed above?). To make a sensible interpretation, we add a loess smoother as well creating an additional regression line with method=\"loess\". If the smooth line sits within the CIs of the linear fit then we can assume a linear relationship. We add span = 1.2 to make the line wiggle less. Change it to, say 5, to see the difference and replot the figure.\n看起來不太好！讓我們繼續探索數據。接下來我們來尋找線性關係。我們有長格式的數據，可以針對每個潛在解釋變數繪製 TOT.N 的散點圖，以檢驗其模式並尋找線性響應（圖 7.9）。這是 glm 的一個重要假設（還記得我們上面討論過的 LINE 縮寫嗎？）。為了做出合理的解釋，我們還添加了一個 loess 平滑器，並使用 method=\"loess\" 創建了一條額外的回歸線。如果平滑線位於線性擬合的置信區間 (CI) 內，那麼我們可以假設它們之間存在線性關係。我們加上 span = 1.2 以減少線條的擺動。將其變更為 5，以查看差異並重新繪製圖表。\n\nggplot(eda, aes(x = value, y = TOT.N)) +\n \n# Layer 1: The scatter plot points\n  geom_point(alpha = 0.6) +\n  \n# Layer 2: The linear regression fit for each plot\n# se = TRUE for the linear model\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", formula = 'y ~ x') +\n# we add a smoother to compare against the linear plot\n geom_smooth(method = \"loess\", se = FALSE, color = \"red\", formula = 'y ~ x',\n    span = 1.2) +\n# This creates a separate plot for each of the X-variables.\n  facet_wrap(~ variables, scales = \"free\") +\n  \n# Add labels and a title\n  labs(\n    title = \"Total kills (TOT.N) v Explanatory Variables\",\n    x = \"Explanatory Variables\",\n    y = \"Total Road Kill per Sector of Road\"\n  ) +\n  \n# Change the theme\n  theme_bw() +\n  theme(\n    strip.background = element_rect(fill = \"lightblue\") # Adds stip labels as the facet titles, in lightblue\n  )\n\n\n\n\n\n\n\n\nFig. 7.9: Scatterplots of TOT.N against all the potential explanatory variables\nMany of you might now be saying, “isn’t that what scatterplotMatrix() does, but in much fewer lines of code!?”. Yes, that’s true but the output here is easier to visualise and now you know more about both approaches. The issue with scatterplotMatrix() where you have lots of variables is that you cannot switch off one side of the plot (i.e. the one below the diagonal), so it gets completely compressed.\n很多人現在可能會說：「這不就是 scatterplotMatrix() 的功能嗎，只不過程式碼行數少多了！？」 是的，沒錯，但這裡的輸出更容易可視化，而且現在你對這兩種方法有了更多的了解。如果變數很多，使用 scatterplotMatrix() 的問題在於，你無法關閉圖的某一側（即對角線下方的一側），所以它會被完全壓縮。\nSo back to the figure (Fig. 7.9). What does it show us? It indicates that most of the explanatory variables have a linear fit to the TOT.N response i.e. the loess smoother sits within the bounds of the CIs on the linear fit, so a glm model is appropriate. We’ll return to non-linear approaches next week. The scatters also indicate that some variables are likely to be poorly correlated due to amount of spread of points around the regression lines. We can see also that some variables have a lot of zeros counts (see the vertical clumps of points on the \\(x\\) axis near zero), which can cause issues in regression-based analyses.\n回到圖 7.9。它向我們展示了什麼？它表明大多數解釋變數與 TOT.N 響應呈線性擬合，即 Loess 平滑器位於線性擬合的置信區間 (CI) 範圍內，因此 GLM 模型是合適的。下週我們將回到非線性方法。散點圖還表明，由於迴歸線周圍點的分佈範圍較大，因此一些變數的相關性可能較差。我們也可以看到，有些變數的零點數量較多（參見 x 軸上靠近零點的垂直點群），這可能會在基於迴歸的分析中造成問題。\n\n\n7.6.2.2 Fitting the model\nWe are going to shortcut a number of the stages we went through above to move faster here. You will need to include them in your analyses for the module assessments, and any project work, or dissertation work you undertake subsequently. We have done some EDA so we select 7 variables that look to have reasonable relationships with the response variable.\n我們將簡化上述幾個步驟，以便更快完成。您需要將這些步驟納入模組評估的分析中，以及您隨後進行的任何專案工作或論文工作中。我們已經進行了一些 EDA，因此我們選擇了 7 個看起來與反應變數合理關係的變數。\n\nroad.glm &lt;- glm(TOT.N ~ OPEN.L + SHRUB + WAT.RES + L.WAT.C + L.P.ROAD +\n         D.WAT.COUR + D.PARK,family=poisson,data=road)\nsummary(road.glm)\n\n\nCall:\nglm(formula = TOT.N ~ OPEN.L + SHRUB + WAT.RES + L.WAT.C + L.P.ROAD + \n    D.WAT.COUR + D.PARK, family = poisson, data = road)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.049e+00  1.095e-01  36.970  &lt; 2e-16 ***\nOPEN.L      -6.353e-03  1.438e-03  -4.418 9.94e-06 ***\nSHRUB       -3.731e-01  9.231e-02  -4.042 5.30e-05 ***\nWAT.RES      6.992e-02  2.843e-02   2.459   0.0139 *  \nL.WAT.C      2.790e-01  4.129e-02   6.756 1.42e-11 ***\nL.P.ROAD     1.434e-01  5.583e-02   2.569   0.0102 *  \nD.WAT.COUR   5.411e-05  1.396e-04   0.388   0.6983    \nD.PARK      -1.100e-04  4.769e-06 -23.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1071.44  on 51  degrees of freedom\nResidual deviance:  316.44  on 44  degrees of freedom\nAIC: 571.83\n\nNumber of Fisher Scoring iterations: 5\n\n\nIt all looks pretty fab, lots of significant relationships but we need to move through our tests for:\n\nmulticollinearity.\nOverdispersion.\nModel simplification / selection.\nModel validation.\n\n\n\n7.6.2.3 Multicollinearity\nWe use the ‘performance’ package for this.\n一切看起來都很棒，有很多重要的關係，但我們需要以下測試：\n\n多重共線性。\n過度離散。\n模型簡化/選擇。\n模型驗證。\n\n\n\n7.6.2.4 多重共線性\n我們使用“性能”包進行此操作。\n\ngvif_values &lt;- performance::check_collinearity(road.glm)\nprint(gvif_values)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n       Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n     OPEN.L 1.20 [1.04, 1.92]     1.09      0.83     [0.52, 0.96]\n      SHRUB 1.43 [1.16, 2.10]     1.19      0.70     [0.48, 0.86]\n    WAT.RES 1.46 [1.19, 2.15]     1.21      0.68     [0.46, 0.84]\n    L.WAT.C 1.80 [1.39, 2.62]     1.34      0.56     [0.38, 0.72]\n   L.P.ROAD 1.17 [1.03, 1.96]     1.08      0.86     [0.51, 0.97]\n D.WAT.COUR 1.53 [1.23, 2.24]     1.24      0.65     [0.45, 0.82]\n     D.PARK 1.24 [1.06, 1.92]     1.11      0.81     [0.52, 0.94]\n\n\nNo problems with collinearity. We move on to overdispersion. 共線性沒有問題。我們繼續討論過度離散問題。\n\n\n7.6.2.5 Overdispersion test\n\ncheck_overdispersion(road.glm)\n\n# Overdispersion test\n\n       dispersion ratio =   6.578\n  Pearson's Chi-Squared = 289.415\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.\n\n\nThe ratio is 6.578. This is well above the threshold of ~ 1. Let’s look at the residuals. Our hunch they’ll be a mess, so the poisson model is a misspecification given the mean-variance relationships in the data. The validation plots will confirm this (7.10).\n比率為 6.578。這遠高於閾值“~”1。我們來看看殘差。我們預感它們會很亂，因此考慮到資料的平均值-變異數關係，泊松模型是錯誤的。驗證圖將證實這一點（7.10）。\n\nautoplot(road.glm, method=\"glm\")\n\n\n\n\n\n\n\n## plot simulated residuals\n\nFig. 7.10: R validation plots of the amphibian road kill data our initial glm model\nSome visible heterogeneity issues here. See the wedges in the ‘Residuals v Fitted’ and ‘Scale-Location’ plots? The Q-Q plot is not that terrible but samples 11 and 2 indicate a potential outliers problem (see the points are number isn’t that clear). We can confirm by selecting a different plot (7.11).\n這裡有一些明顯的異質性問題。看到「殘差與擬合值」和「尺度-位置」圖中的楔形了嗎？ Q-Q 圖還不錯，但樣本 11 和 2 顯示有潛在的異常值問題（點數不太明顯）。我們可以選擇其他圖（7.11）來確認。\n\nplot(road.glm, which =4)\n\n\n\n\n\n\n\n\nFig. 7.11: Cook’s distance plot for the first glm model\nSamples 1, 2 and especially 11 are way over 1. We need to something about it. How do we do this? Do we play around with the variables to seek a good model and then hope it isn’t overdispersed, or do we run with them all and fit the different error structures, and recheck the residual spreads to validate the models. The latter is definitely the angle to take.\n樣本 1、2，尤其是 11，都遠遠超過了 1。我們需要採取措施。該怎麼做呢？是嘗試不同的變數來尋找一個好的模型，然後祈禱它不會過度離散，還是嘗試所有變量，擬合不同的誤差結構，然後重新檢查殘差差值來驗證模型。後者絕對是可行的。\nLet’s fit a quasi-poisson error to the data. The code is essentially the same, we just select a different family= argument. We will rename the model object so we don’t lose track of where we are going.\n讓我們對數據進行擬泊松誤差擬合。程式碼基本上相同，只是我們選擇了不同的 family= 參數。我們將重命名模型對象，這樣我們就不會忘記接下來要做什麼。\n\nroad.glm1 &lt;- glm(TOT.N ~ OPEN.L + SHRUB + WAT.RES + L.WAT.C + L.P.ROAD +\n         D.WAT.COUR + D.PARK,family=quasipoisson,data=road)\nsummary(road.glm1)\n\n\nCall:\nglm(formula = TOT.N ~ OPEN.L + SHRUB + WAT.RES + L.WAT.C + L.P.ROAD + \n    D.WAT.COUR + D.PARK, family = quasipoisson, data = road)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.049e+00  2.809e-01  14.415  &lt; 2e-16 ***\nOPEN.L      -6.353e-03  3.688e-03  -1.723   0.0919 .  \nSHRUB       -3.731e-01  2.367e-01  -1.576   0.1222    \nWAT.RES      6.992e-02  7.292e-02   0.959   0.3428    \nL.WAT.C      2.790e-01  1.059e-01   2.634   0.0116 *  \nL.P.ROAD     1.434e-01  1.432e-01   1.002   0.3220    \nD.WAT.COUR   5.411e-05  3.580e-04   0.151   0.8806    \nD.PARK      -1.100e-04  1.223e-05  -8.993 1.59e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 6.577619)\n\n    Null deviance: 1071.44  on 51  degrees of freedom\nResidual deviance:  316.44  on 44  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nYou see we have fewer signifcant values, which is reassuring. We do not need to rerun the overdispersion tests because quasi-poisson accounts for it by setting the dispersion parameter to 6.577, which is effectively the overdispersion ratio we saw in the overdispersion test. To see if the model is good we need to plot the residuals (Fig. 7.12).\n可以看到，顯著值減少了，這令人放心。我們不需要重新運行過度離散檢驗，因為擬泊松模型透過將離散參數設為 6.577 來解釋這一點，這實際上就是我們在過度離散檢驗中看到的過度離散率。為了檢驗模型是否良好，我們需要繪製殘差圖（圖 7.12）。\n\nautoplot(road.glm1, method=\"glm\")\n\n\n\n\n\n\n\n\nFig. 7.12: R validation plots of the amphibian road kill data for our quasi-poisson model\nNope! It has not improved. We move onto a negative binomial model. This is fitted using the MASS package and glm.nb() function. The other bits of code are the same we just need to remove the family= argument.\n不行！它沒有改善。我們轉到負二項模型。這是使用 MASS 套件和 glm.nb() 函數擬合的。其他代碼相同，只需要刪除 family= 參數。\n\nroad.nb &lt;- glm.nb(TOT.N ~ OPEN.L + SHRUB + WAT.RES + L.WAT.C + L.P.ROAD +\n         D.WAT.COUR + D.PARK,data=road)\n\nLook at the summary. 看一下摘要。\n\nsummary(road.nb)\n\n\nCall:\nglm.nb(formula = TOT.N ~ OPEN.L + SHRUB + WAT.RES + L.WAT.C + \n    L.P.ROAD + D.WAT.COUR + D.PARK, data = road, init.theta = 5.205494769, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.283e+00  2.848e-01  15.038  &lt; 2e-16 ***\nOPEN.L      -1.106e-02  3.125e-03  -3.538 0.000403 ***\nSHRUB       -2.398e-01  2.403e-01  -0.998 0.318290    \nWAT.RES      7.096e-02  7.852e-02   0.904 0.366153    \nL.WAT.C      1.900e-01  9.956e-02   1.909 0.056322 .  \nL.P.ROAD     2.694e-01  1.380e-01   1.952 0.050963 .  \nD.WAT.COUR  -2.588e-05  3.258e-04  -0.079 0.936683    \nD.PARK      -1.190e-04  1.163e-05 -10.227  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(5.2055) family taken to be 1)\n\n    Null deviance: 204.415  on 51  degrees of freedom\nResidual deviance:  51.026  on 44  degrees of freedom\nAIC: 387.54\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  5.21 \n          Std. Err.:  1.29 \n\n 2 x log-likelihood:  -369.537 \n\n\nLooks fine. Let’s check the residuals (Fig. 7.13). 看起來不錯。讓我們檢查一下殘差（圖 7.13）。\n\nautoplot(road.nb, method=\"glm.nb\")\n\n\n\n\n\n\n\n\nFig. 7.13: R validation plots of the amphibian road kill data for initial negative binomial model\nThese look much better so we’ll push on with the model selection. 這些看起來好多了，所以我們將繼續進行模型選擇。\n\noptions(na.action=na.fail) # set options in Base R concerning missing values\nsummary(model.avg(dredge(road.nb), fit = TRUE, subset = TRUE))\n\nFixed term is \"(Intercept)\"\nFixed term is \"(Intercept)\"\nFixed term is \"(Intercept)\"\n\n\n\nCall:\nmodel.avg(object = get.models(object = dredge(road.nb), subset = TRUE))\n\nComponent model call: \nglm.nb(formula = TOT.N ~ &lt;128 unique rhs&gt;, data = road, init.theta = \n     &lt;128 unique values&gt;, link = log)\n\nComponent models: \n        df  logLik   AICc delta weight\n1345     6 -185.45 384.77  0.00   0.18\n145      5 -187.13 385.56  0.79   0.12\n135      5 -187.43 386.16  1.39   0.09\n13456    7 -185.17 386.88  2.11   0.06\n1235     6 -186.61 387.08  2.31   0.06\n13457    7 -185.28 387.11  2.34   0.05\n12345    7 -185.38 387.30  2.53   0.05\n1456     6 -187.02 387.91  3.14   0.04\n1457     6 -187.06 387.98  3.21   0.04\n1245     6 -187.06 387.98  3.21   0.04\n15       4 -189.72 388.28  3.51   0.03\n1356     6 -187.38 388.62  3.85   0.03\n1357     6 -187.42 388.71  3.94   0.02\n125      5 -188.73 388.77  4.00   0.02\n134567   8 -184.77 388.89  4.12   0.02\n123456   8 -185.12 389.58  4.81   0.02\n12356    7 -186.55 389.64  4.87   0.02\n12357    7 -186.61 389.76  4.99   0.01\n123457   8 -185.25 389.84  5.07   0.01\n14567    7 -186.87 390.29  5.52   0.01\n12456    7 -186.97 390.48  5.71   0.01\n12457    7 -187.01 390.57  5.80   0.01\n157      5 -189.70 390.70  5.93   0.01\n156      5 -189.71 390.72  5.95   0.01\n1257     6 -188.69 391.25  6.48   0.01\n13567    7 -187.36 391.26  6.49   0.01\n1256     6 -188.73 391.33  6.56   0.01\n1234567  9 -184.77 391.82  7.05   0.01\n123567   8 -186.54 392.43  7.66   0.00\n13       4 -191.93 392.71  7.94   0.00\n124567   8 -186.86 393.06  8.29   0.00\n1567     6 -189.68 393.23  8.46   0.00\n14       4 -192.35 393.55  8.78   0.00\n1        3 -193.55 393.59  8.82   0.00\n134      5 -191.16 393.62  8.85   0.00\n12567    7 -188.68 393.91  9.14   0.00\n123      5 -191.77 394.83 10.07   0.00\n136      5 -191.82 394.94 10.17   0.00\n137      5 -191.93 395.16 10.39   0.00\n12       4 -193.28 395.40 10.63   0.00\n1346     6 -190.87 395.61 10.84   0.00\n146      5 -192.24 395.78 11.01   0.00\n17       4 -193.52 395.89 11.12   0.00\n16       4 -193.55 395.94 11.17   0.00\n147      5 -192.34 395.99 11.22   0.00\n124      5 -192.35 396.00 11.23   0.00\n1347     6 -191.12 396.11 11.34   0.00\n1234     6 -191.16 396.18 11.41   0.00\n1236     6 -191.65 397.17 12.40   0.00\n1237     6 -191.76 397.40 12.63   0.00\n1367     6 -191.81 397.48 12.71   0.00\n127      5 -193.23 397.77 13.00   0.00\n126      5 -193.27 397.85 13.08   0.00\n13467    7 -190.71 397.96 13.19   0.00\n1467     6 -192.20 398.26 13.49   0.00\n12346    7 -190.86 398.26 13.49   0.00\n1246     6 -192.23 398.33 13.56   0.00\n167      5 -193.52 398.35 13.58   0.00\n1247     6 -192.34 398.54 13.77   0.00\n12347    7 -191.11 398.77 14.00   0.00\n12367    7 -191.65 399.84 15.07   0.00\n1267     6 -193.23 400.33 15.56   0.00\n123467   8 -190.65 400.64 15.87   0.00\n12467    7 -192.17 400.89 16.12   0.00\n56       4 -216.94 442.73 57.96   0.00\n256      5 -216.25 443.81 59.04   0.00\n5        3 -218.96 444.41 59.64   0.00\n356      5 -216.64 444.59 59.82   0.00\n456      5 -216.67 444.64 59.87   0.00\n567      5 -216.93 445.17 60.40   0.00\n25       4 -218.16 445.18 60.41   0.00\n35       4 -218.33 445.51 60.74   0.00\n45       4 -218.35 445.55 60.78   0.00\n2356     6 -215.94 445.75 60.98   0.00\n235      5 -217.47 446.25 61.48   0.00\n2456     6 -216.21 446.28 61.51   0.00\n2567     6 -216.24 446.35 61.58   0.00\n57       4 -218.87 446.60 61.83   0.00\n3456     6 -216.46 446.78 62.01   0.00\n245      5 -217.91 447.12 62.36   0.00\n3567     6 -216.64 447.15 62.38   0.00\n4567     6 -216.65 447.17 62.41   0.00\n345      5 -217.97 447.24 62.47   0.00\n6        3 -220.37 447.25 62.48   0.00\n457      5 -218.00 447.30 62.53   0.00\n257      5 -218.11 447.53 62.76   0.00\n357      5 -218.15 447.60 62.83   0.00\n(Null)   2 -221.83 447.91 63.14   0.00\n23456    7 -215.93 448.40 63.63   0.00\n23567    7 -215.94 448.42 63.65   0.00\n2357     6 -217.34 448.54 63.77   0.00\n2345     6 -217.40 448.67 63.90   0.00\n3457     6 -217.54 448.95 64.18   0.00\n24567    7 -216.21 448.96 64.19   0.00\n2457     6 -217.70 449.27 64.50   0.00\n26       4 -220.25 449.35 64.58   0.00\n36       4 -220.25 449.35 64.58   0.00\n34567    7 -216.42 449.38 64.61   0.00\n3        3 -221.45 449.40 64.63   0.00\n46       4 -220.36 449.56 64.79   0.00\n67       4 -220.37 449.60 64.83   0.00\n2        3 -221.67 449.85 65.08   0.00\n7        3 -221.71 449.93 65.16   0.00\n4        3 -221.82 450.13 65.36   0.00\n23457    7 -217.15 450.85 66.08   0.00\n234567   8 -215.92 451.20 66.43   0.00\n37       4 -221.25 451.36 66.59   0.00\n23       4 -221.28 451.41 66.64   0.00\n236      5 -220.12 451.55 66.78   0.00\n246      5 -220.15 451.60 66.83   0.00\n346      5 -220.21 451.72 66.95   0.00\n34       4 -221.45 451.75 66.98   0.00\n267      5 -220.24 451.79 67.02   0.00\n367      5 -220.25 451.80 67.03   0.00\n467      5 -220.35 452.00 67.24   0.00\n27       4 -221.59 452.03 67.26   0.00\n47       4 -221.66 452.16 67.39   0.00\n24       4 -221.67 452.19 67.43   0.00\n237      5 -221.12 453.55 68.78   0.00\n234      5 -221.21 453.73 68.96   0.00\n2346     6 -219.95 453.77 69.00   0.00\n347      5 -221.25 453.80 69.03   0.00\n2467     6 -220.10 454.06 69.29   0.00\n2367     6 -220.12 454.11 69.34   0.00\n3467     6 -220.21 454.28 69.51   0.00\n247      5 -221.58 454.47 69.70   0.00\n2347     6 -221.11 456.09 71.32   0.00\n23467    7 -219.93 456.40 71.63   0.00\n\nTerm codes: \n    D.PARK D.WAT.COUR   L.P.ROAD    L.WAT.C     OPEN.L      SHRUB    WAT.RES \n         1          2          3          4          5          6          7 \n\nModel-averaged coefficients:  \n(full average) \n              Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)  4.418e+00  2.583e-01   2.630e-01  16.799  &lt; 2e-16 ***\nD.PARK      -1.151e-04  1.122e-05   1.150e-05  10.006  &lt; 2e-16 ***\nL.P.ROAD     1.654e-01  1.666e-01   1.685e-01   0.982  0.32631    \nL.WAT.C      1.143e-01  1.064e-01   1.075e-01   1.063  0.28769    \nOPEN.L      -1.032e-02  3.513e-03   3.586e-03   2.877  0.00401 ** \nSHRUB       -3.112e-02  1.310e-01   1.337e-01   0.233  0.81588    \nD.WAT.COUR  -6.571e-05  2.041e-04   2.074e-04   0.317  0.75135    \nWAT.RES      6.945e-03  3.968e-02   4.054e-02   0.171  0.86399    \n \n(conditional average) \n              Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)  4.418e+00  2.583e-01   2.630e-01  16.799  &lt; 2e-16 ***\nD.PARK      -1.151e-04  1.122e-05   1.150e-05  10.006  &lt; 2e-16 ***\nL.P.ROAD     2.576e-01  1.395e-01   1.431e-01   1.800  0.07187 .  \nL.WAT.C      1.720e-01  8.431e-02   8.647e-02   1.990  0.04664 *  \nOPEN.L      -1.055e-02  3.185e-03   3.268e-03   3.229  0.00124 ** \nSHRUB       -1.290e-01  2.418e-01   2.478e-01   0.520  0.60276    \nD.WAT.COUR  -2.391e-04  3.319e-04   3.391e-04   0.705  0.48074    \nWAT.RES      3.031e-02  7.852e-02   8.041e-02   0.377  0.70620    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\noptions(na.action = \"na.omit\") # reset base R options\n\nIf you look at the component models matrix you can see that three models lie within &lt;2 AIC points. In these instances, following the guidance in Burnham and Anderson (2002), we select the model with the fewest terms and lowest AIC of the three. That models has variables 1,4 and 5 it it (i.e. D.PARK, L.WAT.C, OPEN.L).\nWe select those and rerun the model.\n如果您查看組件模型矩陣，您會發現三個模型的 AIC 點數小於 2。在這些情況下，按照 Burnham and Anderson (2002) 中的指導，我們選擇三個模型中項數最少且 AIC 最低的模型。模型包含變數 1、4 和 5（即 D.PARK、L.WAT.C 和 OPEN.L）。\n我們選擇這些模型並重新運行模型。\n\nroad.final &lt;- glm.nb(TOT.N ~ D.PARK + L.WAT.C + OPEN.L, data=road)\nsummary(road.final)\n\n\nCall:\nglm.nb(formula = TOT.N ~ D.PARK + L.WAT.C + OPEN.L, data = road, \n    init.theta = 4.725770366, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.467e+00  1.716e-01  26.022  &lt; 2e-16 ***\nD.PARK      -1.158e-04  1.082e-05 -10.708  &lt; 2e-16 ***\nL.WAT.C      1.867e-01  7.911e-02   2.361 0.018246 *  \nOPEN.L      -1.071e-02  3.146e-03  -3.405 0.000661 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.7258) family taken to be 1)\n\n    Null deviance: 189.709  on 51  degrees of freedom\nResidual deviance:  52.003  on 48  degrees of freedom\nAIC: 384.25\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.73 \n          Std. Err.:  1.16 \n\n 2 x log-likelihood:  -374.254 \n\n\n\n\n7.6.2.6 Model Validation\nWe plot the normal plots and then assess each explanatory variable in turn (Fig. 7.14). 我們繪製常態圖，然後依序評估每個解釋變數（圖 7.14）。\n\nop &lt;- par(mfrow = c(2, 2))\nplot(road.final) # no patterns....\n\n\n\n\n\n\n\npar(op) \n\nFig. 7.14: R validation plots of the amphibian road kill data for our final negative binomial model\nThese look excellent. No patterns and no high leverage (or outlier) points (see Residuals vs Leverage panel. We plot each term against the pearson residuals next (Fig. 7.15).\n這些看起來很棒。沒有模式，也沒有高槓桿率（或異常值）點（請參閱「殘差與槓桿率」面板）。接下來，我們將每一項與皮爾森殘差作圖（圖 7.15）。\n\nR &lt;- resid(road.final, type='pearson') # extract the pearson residuals\nop &lt;- par(mfrow = c(2, 2))\nplot(road$OPEN.L~R)\nplot(road$L.WAT.C~R)\nplot(road$D.PARK~R)\npar(op) \n\n\n\n\n\n\n\n\nFig. 7.15: Residual plots for the explanatory variables in the amphibian roadkill data for our final negative binomial model\nNo issues visible here either; we are able to interpret the model. 這裡沒有明顯的問題，我們能夠解釋該模型。\n\n\n7.6.2.7 Model interpretation\nThe model still uses a log-link function so we still need to exponentiate the coefficients to interpret the outputs. We have not centred or scaled the explanatory variables so we need to be careful not to directly compare the slope terms as they are all measured differently. Each term is baselined against zero. Recall that we assessed model fit in our glm using the captured and null deviance values. We cannot do that with a negative binomial model so we need a different approach. Luckily for us the performance package does this for us with the r()2 function. Here is the exponentiated data and a psuedo R-squared value.\n該模型仍然使用對數連結函數，因此我們仍然需要對係數進行指數運算來解釋輸出。我們沒有對解釋變數進行中心化或縮放，因此需要注意不要直接比較斜率項，因為它們的測量方式不同。每個斜率項都以零為基準。回想一下，我們在全域線性模型 (GLM) 中使用捕獲偏差值和零偏差值評估了模型擬合度。我們無法使用負二項式模型做到這一點，因此我們需要採用不同的方法。幸運的是，performance 套件使用 r()2 函數為我們完成了這項工作。以下是指數化資料和偽 R 平方值。\n\ntidy(road.final, exponentiate = TRUE)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   87.1   0.172         26.0  2.78e-149\n2 D.PARK         1.00  0.0000108    -10.7  9.35e- 27\n3 L.WAT.C        1.21  0.0791         2.36 1.82e-  2\n4 OPEN.L         0.989 0.00315       -3.41 6.61e-  4\n\nprint(r2(road.final))\n\n# R2 for Generalized Linear Regression\n  Nagelkerke's R2: 0.954\n\n\nNagelkerke’s R2 for the model is 0.954. Shows a very high fit. I have my doubts about so we will hand calculate an alternative that is more conservative i.e. McFadden’s R-squared. It’s based on the ratio of the log-likelihoods of your full model versus a null (intercept-only) model. Its values are typically much lower than Nagelkerke’s. A value between 0.2 and 0.4 for McFadden’s is considered to represent a very good model fit.\n此模型的 Nagelkerke R² 為 0.954，表示適合度非常高。我對此表示懷疑，因此我們將手動計算一個更保守的替代方法，即McFadden R 平方。它基於完整模型與零模型（僅截距）的對數似然比。它的值通常遠低於 Nagelkerke R 平方。 McFadden R 平方在 0.2 到 0.4 之間通常被認為代表模型適配度非常好。\n\n# Get the log-likelihood of your full model\nlogLik_full &lt;- logLik(road.final)\n\n# To get the log-likelihood of the null model, we need to fit it first.\n# The null model has only an intercept (e.g., Srich ~ 1).\nmodel_null &lt;- update(road.final, . ~ 1)\nlogLik_null &lt;- logLik(model_null)\n\n# Apply the formula and convert the final result to a simple number\nr2_mcfadden &lt;- as.numeric(1 - (logLik_full / logLik_null))\n\n# Print the result - it will now be a clean number\nprint(r2_mcfadden)\n\n[1] 0.1564486\n\n# [1] 0.1564486\n\nWe have an R-squared of ~ 0.16, so our model fit is reasonable for McFadden’s. You’ll see why it is like this when we plot the partials plots shortly.\n我們的 R 平方約為 0.16，因此我們的模型適合度對於 McFadden 樣本來說是合理的。稍後繪製偏函數圖時，您就會明白為什麼會這樣。\nWe interpret the model terms a little differently than above because we have not centred or scaled the data. Recall that centreing and scaling means the coefficients are centred on the mean and each unit change is measured in 1 SD units (useful for comparing model parameters). In this uncentred model (road.final) the parameters are baselined at zero. So here goes:\n由於我們沒有將資料中心化或縮放處理，因此我們對模型術語的解釋與上文略有不同。您還記得，中心化和縮放意味著係數以平均值為中心，每個單位的變化都以 1 個標準差為單位進行測量（這對於比較模型參數很有用）。在這個未中心化的模型（road.final）中，參數的基線為零。因此，如下圖所示：\n\nfor the intercept, all the terms are set to zero so the logged term (4.467) is 87.0626208. This means that there is a count of 87.06 road kills. This is a little unlikely and why we really ought to at least centre data so it based on the mean.\nFor D.PARK, where all other terms are held constant, the slope coefficient’s exponentiated value is approximately 0.999. This means that for a one-unit increase in D.PARK, the predicted road kill count is multiplied by 0.999, which corresponds to a 0.1% decrease in counts.\nfor or L.WAT.C, where all other terms are set to zero (held constant), the slope coefficient (1.867e-01) ≈ 1.2053157. So for one unit increase in L.WAT.C, the predicted kill is multiplied by 1.2053157, which corresponds to an 20% increase in kill counts.\nFor OPEN.L, where all other terms are held constant, the slope coefficient is -1.071e-02. After exponentiating, this gives an incidence rate ratio of approximately 0.989. This means that for a one-unit increase in OPEN.L, the predicted road kill count is multiplied by 0.989, which corresponds to a 1.1% decrease in counts.\n對於截距，所有項均設為零，因此對數項 (4.467) 為 87.0626208。這意味著路斃車數量為 87.06 輛。這不太可能，這也是為什麼我們至少應該將數據以平均值為中心的原因。\n對於 D.PARK，當所有其他項保持不變時，斜率係數的指數值約為 0.999。這意味著，D.PARK 每增加一個單位，預測的路斃車數量就會乘以 0.999，相當於數量減少 0.1%。\n對於 L.WAT.C，當所有其他項都設為零（保持不變）時，斜率係數 (1.867e-01) ≈ 1.2053157。因此，L.WAT.C 每增加一個單位，預測的路斃動物數量就會乘以 1.2053157，相當於路斃動物數量增加 20%。\n對於 OPEN.L，在其他所有項不變的情況下，斜率係數為 -1.071e-02。取冪後，發生率比約為 0.989。這意味著，OPEN.L 每增加一個單位，預測的路斃動物數量就會乘以 0.989，相當於路斃動物數量減少 1.1%。\n\n\n\n7.6.2.8 Plot a figure to support the story\nWe now need a nice plot to support our interpretation. We plot the slope partials, one for each variable in our model. We hand calculated them above and we can do so again (Fig. 7.16). There is a shortcut using the ggeffects package - have a look. This is a sequenced piece of code that:\n現在我們需要一個漂亮的圖來支持我們的解釋。我們繪製模型中每個變數的斜率偏函數。我們之前手動計算過，可以再重複一次（圖 7.16）。使用 ggeffects 套件有一個快捷方式——可以看看。這是一段依序排列的程式碼：\n\ncreates three dataframes, one for each variable.\n\nWe sequence along for variable for 100 points.\nCalculate the predictions for that variable when the others are held constant.\ntype = \"response\" exponentiates the data.\n\nWe use ggplot2 to create the plots adding geoms for the points, the regression lines, then ribbons for the CIs.\nWe plot the final figure using the grid.arrange() function from the gridExtra package.\n建立三個資料框，每個變數一個。\n我們對 100 個點進行排序。\n當其他變數保持不變時，計算該變數的預測值。\ntype = “response” 對資料進行指數運算。\n我們使用 ggplot2 建立圖表，為點新增幾何對象，為迴歸線新增迴歸線，然後為可信區間添加色帶。\n我們使用 gridExtra 套件中的 grid.arrange() 函數繪製最終圖形。\n\n\n# ======================================================================\n# 1. Partial Effect for Distance to Park (D.PARK)\n# ======================================================================\n# Create a data frame where D.PARK varies and others are held at their mean\npartial_d_park &lt;- data.frame(\n  D.PARK = seq(min(road$D.PARK, na.rm = TRUE), \n               max(road$D.PARK, na.rm = TRUE), length.out = 100),\n  L.WAT.C = mean(road$L.WAT.C, na.rm = TRUE),\n  OPEN.L = mean(road$OPEN.L, na.rm = TRUE)\n)\n\n# Predict response and confidence intervals\npredictions_d_park &lt;- predict(road.final, newdata = partial_d_park, type = \"response\", se.fit = TRUE)\npartial_d_park$predicted &lt;- predictions_d_park$fit\npartial_d_park$lower_ci &lt;- predictions_d_park$fit - 1.96 * predictions_d_park$se.fit\npartial_d_park$upper_ci &lt;- predictions_d_park$fit + 1.96 * predictions_d_park$se.fit\n\n# Plot 1\np1 &lt;- ggplot() +\n  # --- ADDED: Raw data points in the background ---\n  geom_point(data = road, aes(x = D.PARK, y = TOT.N), alpha = 0.5) +\n  # --- Prediction line and ribbon on top ---\n  geom_line(data = partial_d_park, aes(x = D.PARK, y = predicted), color = \"blue\", linewidth = 1) +\n  geom_ribbon(data = partial_d_park, aes(x = D.PARK, ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = \"blue\") +\n  labs(title = \"Partial Effect of Distance to Park\",\n       x = \"Distance to Park (D.PARK)\", y = \"Predicted Road Kills\") +\n  theme_minimal()\n\n# ======================================================================\n# 2. Partial Effect for Length of Water Course (L.WAT.C)\n# ======================================================================\n# Create a data frame where L.WAT.C varies and others are held at their mean\npartial_l_wat_c &lt;- data.frame(\n  L.WAT.C = seq(min(road$L.WAT.C, na.rm = TRUE), \n                max(road$L.WAT.C, na.rm = TRUE), length.out = 100),\n  D.PARK = mean(road$D.PARK, na.rm = TRUE),\n  OPEN.L = mean(road$OPEN.L, na.rm = TRUE)\n)\n\n# Predict and add columns\npredictions_l_wat_c &lt;- predict(road.final, newdata = partial_l_wat_c, type = \"response\", se.fit = TRUE)\npartial_l_wat_c$predicted &lt;- predictions_l_wat_c$fit\npartial_l_wat_c$lower_ci &lt;- predictions_l_wat_c$fit - 1.96 * predictions_l_wat_c$se.fit\npartial_l_wat_c$upper_ci &lt;- predictions_l_wat_c$fit + 1.96 * predictions_l_wat_c$se.fit\n\n# Plot 2\np2 &lt;- ggplot() +\n  # --- ADDED: Raw data points in the background ---\n  geom_point(data = road, aes(x = L.WAT.C, y = TOT.N), alpha = 0.5) +\n  # --- Prediction line and ribbon on top ---\n  geom_line(data = partial_l_wat_c, aes(x = L.WAT.C, y = predicted), color = \"blue\", linewidth = 1) +\n  geom_ribbon(data = partial_l_wat_c, aes(x = L.WAT.C, ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = \"blue\") +\n  labs(title = \"Partial Effect of Water Course Length\",\n       x = \"Length of Water Course (L.WAT.C)\", y = \"Predicted Road Kills\") +\n  theme_minimal()\n\n# ======================================================================\n# 3. Partial Effect for Open Land (OPEN.L)\n# ======================================================================\n# Create a data frame where OPEN.L varies and others are held at their mean\npartial_open_l &lt;- data.frame(\n  OPEN.L = seq(min(road$OPEN.L, na.rm = TRUE), \n               max(road$OPEN.L, na.rm = TRUE), length.out = 100),\n  D.PARK = mean(road$D.PARK, na.rm = TRUE),\n  L.WAT.C = mean(road$L.WAT.C, na.rm = TRUE)\n)\n\n# Predict and add columns\npredictions_open_l &lt;- predict(road.final, newdata = partial_open_l, type = \"response\", se.fit = TRUE)\npartial_open_l$predicted &lt;- predictions_open_l$fit\npartial_open_l$lower_ci &lt;- predictions_open_l$fit - 1.96 * predictions_open_l$se.fit\npartial_open_l$upper_ci &lt;- predictions_open_l$fit + 1.96 * predictions_open_l$se.fit\n\n# Plot 3\np3 &lt;- ggplot() +\n  # --- ADDED: Raw data points in the background ---\n  geom_point(data = road, aes(x = OPEN.L, y = TOT.N), alpha = 0.5) +\n  # --- Prediction line and ribbon on top ---\n  geom_line(data = partial_open_l, aes(x = OPEN.L, y = predicted), color = \"blue\", linewidth = 1) +\n  geom_ribbon(data = partial_open_l, aes(x = OPEN.L, ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = \"blue\") +\n  labs(title = \"Partial Effect of Open Land\",\n       x = \"Percentage of Open Land (OPEN.L)\", y = \"Predicted Road Kills\") +\n  theme_minimal()\n\n# ======================================================================\n# Combine all plots into a grid\n# ======================================================================\ngrid.arrange(p1, p2, p3, ncol = 2)\n\n\n\n\n\n\n\n\nFig. 7.16: The partial (marginal) slope plots for our amphibian road kills data generated by the final negative binomial model\nFig. 7.16 shows the partial slopes each variable when the other terms are held constant. Recall from above that the psuedo \\(R^2\\) is ~ 0.16, and this taken together with the other model outputs, shows a significant but noisy model.\n圖 7.16 顯示了當其他項保持不變時，每個變數的偏斜率。回想一下上文，偽 $R^2$ 約為 0.16，將其與其他模型輸出結合起來，可以看出這是一個顯著但雜訊很大的模型。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#class-exercises",
    "href": "chap7.html#class-exercises",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.7 Class Exercises",
    "text": "7.7 Class Exercises\nJust one task this week. Create a GLM (of some form) with this dataset: Bham_birds.csv. 本週只有一個任務。使用以下資料集建立一個（某種形式的）GLM：Bham_birds.csv。\n\n7.7.1 Data description\n\nThese unpublished data from a UoB PhD thesis by Dr Emma Rosenfeld - available as an E-thesis from the library.\nIt is one summer study of 70 sites distributed across Birmingham and Black Country conurbation in the summer of 2010-12. These data relate to the 2011 summer census period only. Reduced to 66 for this work.\n70 500mx500m sites across the urban gradient were surveyed for birds using methods based on the BTO’s Breeding Bird Survey.There were two 500m transects per site, which were subdivided into 100m sections.\nThe sites were statified by landuse using data from OS master map landuse and land cover (LULC) and remote sensing data from Landsat. The LULC classification is based on the methods of Hale et al. (2012).\n這些未發表的資料來自伯明翰大學 Emma Rosenfeld 博士的博士論文，可從圖書館取得電子論文。\n這是一項夏季研究，研究對象為分佈在伯明罕和黑鄉都市圈的 70 個地點，研究時間為 2010-12 年夏季。這些數據僅與 2011 年夏季人口普查期相關。本研究將樣本量縮減至 66 個。\n使用基於英國鳥類調查局 (BTO) 繁殖鳥類調查的方法，對城市梯度範圍內 70 個 500 公尺 x 500 公尺的地點進行了鳥類調查。每個地點設置兩條 500 公尺長的橫斷面，並細分為 100 公尺長的橫斷面。\n使用來自 OS 主地圖土地利用和土地覆蓋 (LULC) 的數據以及來自 Landsat 的遙感數據，按土地利用情況對地點進行了統計。 LULC 分類是基於 Hale et al. (2012) 的方法。\n\n\n7.7.1.1 Response variables\n\nAbundance. Abundance/counts of bird species per 500m square.\nSR. Species richness or the number of different bird species in each square.\n豐富度。每500平方公尺範圍內鳥類物種的豐富度/數量。\nSR。物種豐富度，即每個方格內不同鳥類物種的數量。\n\n\n\n7.7.1.2 Explanatory/predictive/Covariables\n\nSite. The site number 66 in total.\nSite_Name, Site names / label\nSite_type. Type of site as indicated by the prevailing landuse. URBAN = urban, DSU = Dense suburban, LSU = Light suburban, SUB = suburban, RURAL = rural land\nLat. Latitude of site.\nLong. Longitude of the site.\nTreeCover_500. Tree cover within 500m of the site centroid (%). NOTE: this is a subset of the green landuse cover emphasising structural diversity (canopy layer above 3m).\nBuilt_Cov_500. Buildcover within 500m of the site centroid (%).\nGreenCover_500. Green cover within 500m of the site centroid (%).\nSite。共 66 個站點。\nSite_Name，網站名稱/標籤\nSite_type。根據目前土地利用情況所確定的站點類型。 URBAN = 城市，DSU = 密集郊區，LSU = 輕度郊區，SUB = 郊區，RURAL = 鄉村土地\nLat。站點緯度。\nLong。站點經度。\nTreeCover_500。站點質心 500 公尺範圍內的樹木覆蓋率 (%)。注意：這是綠色土地利用覆蓋的子集，強調結構多樣性（冠層高度超過 3 公尺）。\nBuilt_Cov_500。站點質心 500 公尺範圍內的建築覆蓋率 (%)。\nGreenCover_500。站點質心 500 公尺範圍內的綠色覆蓋率 (%)。\n\n\n\n7.7.1.3 Your analytical tasks\n\nImport the data.\nCentre the explanatory variables so the mean is zero and the SD is one.\nExamine the data structure and summaries.\nDo some visualisation. Create the pictures.\nUse Abundance as your response variable and Site_type, TreeCover_500, Built_Cov_500 and GreenCover_500 as your explanatory variables.\nRun an initial GLM.\n\nCheck the summary.\nCheck for multicollinearity.\nCheck for Overdispersion.\nValidate the model.\nSelect an appropriate error (model).\nDo some model selection.\nValidate your model.\nCreate a final figure.\nInterpret the findings.\n\n\nYou have all the code you need with some little twists in the code snippets above.\n\n導入資料。\n將解釋變數置中，使平均值為零，標準差為一。\n檢查資料結構和摘要。\n進行可視化。建立圖片。\n使用 Abundance 作為反應變量，Site_type、TreeCover_500、Built_Cov_500 和 GreenCover_500 作為解釋變數。\n運行初始 GLM。\n檢查摘要。\n檢查是否有多重共線性。\n檢查是否有過度離散。\n驗證模型。\n選擇合適的誤差（模型）。\n選擇模型。\n驗證您的模型。\n建立最終圖表。\n解釋結果。\n\n您已擁有所需的所有程式碼，只需對上面的程式碼片段進行一些小改動即可。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#further-work",
    "href": "chap7.html#further-work",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.8 Further Work",
    "text": "7.8 Further Work\n\nFinish the class exercise.\nRead chapter 7 in A. Zuur, Ieno, and Smith (2007). It is an introduction Generalised Additive Models (GAMs).\n完成課堂練習。\n閱讀@Zuur2007的第7章。這是一篇關於廣義可加模型（GAM）的介紹。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#next-week",
    "href": "chap7.html#next-week",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.9 Next Week",
    "text": "7.9 Next Week\nNext week we will be looking at Generalised Additive Models (GAMs). 下週我們將研究廣義加性模型 (GAM)。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#references",
    "href": "chap7.html#references",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.10 References",
    "text": "7.10 References\n\n\n\n\nBurnham, Kenneth P, and David. R Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. 2nd Ed. Springer New York.\n\n\nGotelli, Nicholas J., and Aaron M. Ellison. 2002. “Biogeography at a Regional Scale: Determinants of Ant Species Density in New England Bogs and Forests.” Ecology 83 (6): 1604–9. https://doi.org/10.1890/0012-9658(2002)083[1604:BAARSD]2.0.CO;2.\n\n\nHale, James D, Alison J Fairbrass, Tom J Matthews, and Jon P Sadler. 2012. “Habitat Composition and Connectivity Predicts Bat Presence and Activity at Foraging Sites in a Large UK Conurbation.” PLoS ONE 7 (3): e33300. https://doi.org/10.1371/journal.pone.0033300.\n\n\nIsmay, Chester, Albert Kim Y., and Arturo Valdivia. 2025. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. 2nd Ed. CRC Press, Chapman Hall.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using R : A Practical Guide. Chichester: Wiley and Sons.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Dominique Makowski. 2021. “Performance: An R Package for Assessment, Comparison and Testing of Statistical Models.” Journal of Open Source Software 6 (60): 3139. https://doi.org/10.21105/joss.03139.\n\n\nMcCullagh, P., and J. A. Nelder. 1989. Generalized Linear Models. 2nd Edition.\n\n\nTang, Yuan, Masaaki Horikoshi, and Wenxuan Li. 2016. “Ggfortify: Unified Interface to Visualize Statistical Results of Popular R Packages.” The R Journal 8 (2): 474–85.\n\n\nVenables, W. N., and B. D. Ripley. 2002. “Modern Applied Statistics with S  SpringerLink.” https://link.springer.com/book/10.1007/978-0-387-21706-2.\n\n\nZuur, Alain F., and Elena N. Ieno. 2016. “A Protocol for Conducting and Presenting Results of Regression-Type Analyses.” Methods in Ecology and Evolution 7 (6): 636–45. https://doi.org/10.1111/2041-210X.12577.\n\n\nZuur, Alain F., Elena N. Ieno, and Chris S. Elphick. 2010. “A Protocol for Data Exploration to Avoid Common Statistical Problems.” Methods in Ecology and Evolution 1 (1): 3–14. https://doi.org/10.1111/j.2041-210X.2009.00001.x.\n\n\nZuur, Alain F., Elena N. Ieno, Neil Walker, Anatoly A. Saveliev, and Graham M. Smith. 2009. Mixed Effects Models and Extensions in Ecology with R. Statistics for Biology and Health. New York, NY: Springer. https://doi.org/10.1007/978-0-387-87458-6.\n\n\nZuur, Alain, Elena N. Ieno, and Graham M. Smith. 2007. Analyzing Ecological Data. New York, NY, UNITED STATES: Springer New York.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap7.html#follow-up-work",
    "href": "chap7.html#follow-up-work",
    "title": "7  An Introduction to Generalised Linear Models (GLMs)",
    "section": "7.8 Follow-up Work",
    "text": "7.8 Follow-up Work\n\nFinish the class exercise.\nRead chapter 7 in A. Zuur, Ieno, and Smith (2007). It is an introduction Generalised Additive Models (GAMs).\n完成課堂練習。\n閱讀@Zuur2007的第7章。這是一篇關於廣義可加模型（GAM）的介紹。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>An Introduction to Generalised Linear Models (GLMs)</span>"
    ]
  },
  {
    "objectID": "chap9.html",
    "href": "chap9.html",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "",
    "text": "9.1 Today’s Session\nLast week we covered the application of Generalised Additive Models (GAMs) to data that did not fit a key assumption of GLMs, i.e. linearity or the \\(x\\) ~ \\(y\\) relationship. This week, for our final session, we focus how we might deal with situations where we have structural dependencies in our data that do no meet the key assumption for linear, GLM and GAM models: independence of sample points.\nLet’s explore how this might come about. If you are unlucky it arises because you have messed up your sample design. Hopefully, this is not the case, rather, it is a response to dependence issues that are factored into your design. Figure 9.1. shows two simple scenarios where the application of mixed modelling (either ANOVA or regression models) is a necessity. In panel (A), we have a block experiment that mirrors our fertilizer example from week 4 with one important difference. We have 3 subsamples from within each treatment within each block. The subsamples are nested with the treatments and not independent from each other. We must accommodate this in our regression.\nTASK: Take a few mins chatting with your neighbour thinking about why this is the case. HINT: we know that co-located samples are much more likely to be similar to each other i.e. within each treatment and each treatment block than between treatments and blocks (this characteristic is know as spatial autocorrelation [~ 5 min].\nPanel (B), shows the situation where we have repeated measurement at each site over three time periods. Let’s say we are counting bird species at each site in Spring, Summer and Autumn and we take one sample in each season. We need to account for this because repeated measurements from the individual sites are not independent. The samples across the seasons, within each site will be more similar to each other seasonal samples between the sites (a characteristic know as temporal autocorrelation).\nFig. 9.1: Mixed model example sample designs where some form of mixed model (either ANOVA or regression) is necessary\nToday’s work is all about sorting out these potential analytical problems. And in doing so, hopefully supporting you in your project analyses over summer!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  },
  {
    "objectID": "chap9.html#a-mixed-model-is-a-regression-model-that-includes-both",
    "href": "chap9.html#a-mixed-model-is-a-regression-model-that-includes-both",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "9.2 A mixed model is a regression model that includes both:",
    "text": "9.2 A mixed model is a regression model that includes both:\n\nfixed effects and\nrandom effects. Random effects allow certain parameters to vary across groups / clusters / units (e.g. intercepts, slopes for individual subjects, plots, regions). They model correlation or non-independence in the data.\n\nMixed models are useful when data are hierarchically structured (e.g. measurements nested within subjects, spatial clusters, repeated measurements over time) or when you have grouped / clustered observations.\nYou will remember from numerous sessions that a regression generates two coefficients, an intercept and a slope (for all the explanatory variables). Mixed models use the random structure in the model to capture variability in two ways. You can use them to generate:\n\na random intercept model - where we allow the intercepts of our explanatory variables to vary, or\na random intercept and slope model - where both the intercept and slopes can vary (Fig. 9.1).\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\nFig. 9.1: Examples of a random intercept (left panel) and a random intercept and random slope model (right panel). The black line = fixed effect (population regression line). The coloured lines represent the various groups.\nWe can represent these models in same mathematical way as we did with LMs and GLMs:\nGeneral Form (Linear Mixed Model, LMM):\n[ y_{ij} = *0 +* *1 x*{ij} + u{0j} + u_{1j}x_{ij} + _{ij} ]\nWhere:\n\n( y_{ij} ): outcome for observation i in group j\n\n( _0, _1 ): fixed effects (population-level intercept and slope)\n\n( u_{0j}, u_{1j} ): random effects (group-level deviations, e.g. each subject has its own intercept/slope)\n\n( _{ij} ): residual error (within-group variability)\n\nGeneral Form (Generalized Linear Mixed Model, GLMM):\n[ g(*{ij}) =* *0 +* *1 x*{ij} + u{0j} + u{1j}x_{ij} ]\nWhere:\n\n( g() ): link function (e.g. logit for binary data, log for counts)\n\n( _{ij} = E[y_{ij}] ): expected outcome for observation i in group j\n\n( _0, _1 ): fixed effects (population-level intercept and slope)\n\n( u_{0j}, u_{1j} ): random effects (group-level deviations)\n\n( y_{ij} ): observed response, assumed to follow a distribution from the exponential family\n\ne.g. Binomial (for binary), Poisson (for counts), Gaussian (for continuous)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  },
  {
    "objectID": "chap9.html#essential-reading",
    "href": "chap9.html#essential-reading",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "9.2 Essential Reading",
    "text": "9.2 Essential Reading\nRead these three introductory papers on mixed modelling, if you haven’t already done so (after the class would make sense!):\n\nStart with chapter 8, ‘Mixed Models’, in Zuur, Ieno, and Smith (2007).\nThen Harrison et al. (2018).\nFinally, the classic introduction by Bolker et al. (2009).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  },
  {
    "objectID": "chap9.html#todays-session",
    "href": "chap9.html#todays-session",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "",
    "text": "9.1.1 Learning Outcomes\nBy the end of this session you will be able to:\n\nExplore data to look for dependencies and potential autocorrelative structures\\\nUse straightforward linear mixed effects models to resolve the potential issues\nAnything more complex and we will need to discuss options and approaches!\n\n\n\n9.1.2 Load libraries\n\n# List of packages\npackages &lt;- c(\"tidyverse\", \"ggfortify\", \"performance\", \"car\", \"skimr\", \"gridExtra\", \"broom\", \n\"ggeffects\",\"MASS\",\"MuMIn\", \"lme4\",\"glmmTMB\", \"DHARMa\")\n# Load all packages and install the packages we have no previously installed on the system\nlapply(packages, library, character.only = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nRegistered S3 method overwritten by 'MuMIn':\n  method        from \n  nobs.multinom broom\n\nLoading required package: Matrix\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nThis is DHARMa 0.4.7. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\n\n[[1]]\n [1] \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"     \"readr\"    \n [7] \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"     \"graphics\" \n[13] \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[2]]\n [1] \"ggfortify\" \"lubridate\" \"forcats\"   \"stringr\"   \"dplyr\"     \"purrr\"    \n [7] \"readr\"     \"tidyr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"    \n [6] \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[11] \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"  \n[16] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[4]]\n [1] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"  \n [6] \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"      \n[11] \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"      \n[16] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[21] \"base\"       \n\n[[5]]\n [1] \"skimr\"       \"car\"         \"carData\"     \"performance\" \"ggfortify\"  \n [6] \"lubridate\"   \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"      \n[11] \"readr\"       \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"  \n[16] \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"   \n[21] \"methods\"     \"base\"       \n\n[[6]]\n [1] \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"     \"performance\"\n [6] \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"     \"dplyr\"      \n[11] \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"      \"ggplot2\"    \n[16] \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"      \n[21] \"datasets\"    \"methods\"     \"base\"       \n\n[[7]]\n [1] \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"    \n [6] \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"    \n[11] \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[16] \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"  \n[21] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[8]]\n [1] \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"        \n [6] \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"    \n[11] \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"      \n[16] \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"   \n[21] \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[9]]\n [1] \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"      \n [6] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"  \n[11] \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"      \n[16] \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"      \n[21] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[26] \"base\"       \n\n[[10]]\n [1] \"MuMIn\"       \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"  \n [6] \"skimr\"       \"car\"         \"carData\"     \"performance\" \"ggfortify\"  \n[11] \"lubridate\"   \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"      \n[16] \"readr\"       \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"  \n[21] \"stats\"       \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"   \n[26] \"methods\"     \"base\"       \n\n[[11]]\n [1] \"lme4\"        \"Matrix\"      \"MuMIn\"       \"MASS\"        \"ggeffects\"  \n [6] \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"         \"carData\"    \n[11] \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"     \"stringr\"    \n[16] \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"       \"tibble\"     \n[21] \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"    \"grDevices\"  \n[26] \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[12]]\n [1] \"glmmTMB\"     \"lme4\"        \"Matrix\"      \"MuMIn\"       \"MASS\"       \n [6] \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"       \"car\"        \n[11] \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"   \"forcats\"    \n[16] \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"       \"tidyr\"      \n[21] \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"       \"graphics\"   \n[26] \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"     \"base\"       \n\n[[13]]\n [1] \"DHARMa\"      \"glmmTMB\"     \"lme4\"        \"Matrix\"      \"MuMIn\"      \n [6] \"MASS\"        \"ggeffects\"   \"broom\"       \"gridExtra\"   \"skimr\"      \n[11] \"car\"         \"carData\"     \"performance\" \"ggfortify\"   \"lubridate\"  \n[16] \"forcats\"     \"stringr\"     \"dplyr\"       \"purrr\"       \"readr\"      \n[21] \"tidyr\"       \"tibble\"      \"ggplot2\"     \"tidyverse\"   \"stats\"      \n[26] \"graphics\"    \"grDevices\"   \"utils\"       \"datasets\"    \"methods\"    \n[31] \"base\"       \n\n\nThe new packages this week are lme4, glmmTMB and DHARMa:\n\nlme4 - The lme4 package provides functions to fit and analyze linear mixed models, generalized linear mixed models and nonlinear mixed models (Bates et al. 2015).\nglmmTMB - An extremely powerful and complex multi-level modelling package for Generalised Linear Mixed Models (GLMMs) (Brooks et al. 2017). Permits the application of zero-inflated mixed models.\nDHARMa - Provides functions that provide simulated residual diagnostics for mixed models along with numerous tests for zero-inflation, over dispersion and so (Hartig 2024).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  },
  {
    "objectID": "chap8.html",
    "href": "chap8.html",
    "title": "8  Non-linear modelling: Generalised Additive Models (GAMs)",
    "section": "",
    "text": "8.1 Introduction\nYou will recall that a key assumption in the application of Generalised Linear Models was that the \\(y\\) ~ \\(x\\) relationship conformed to a linear, or, straight line fit. In the natural world there are numerous occasions/instances where this simply does not hold. In such cases, we need to apply different models. There are a number ways of doing this. We could:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-linear modelling: Generalised Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "chap8.html#introduction",
    "href": "chap8.html#introduction",
    "title": "8  Non-linear modelling: Generalised Additive Models (GAMs)",
    "section": "",
    "text": "vbdffe\nfvdv\nOr apply Generalised Additive Models to the data. We will focus on this approach because it works very much like GLMs; we have the same family = error structures and functional calls.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-linear modelling: Generalised Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "chap8.html#what-are-generalised-additive-models",
    "href": "chap8.html#what-are-generalised-additive-models",
    "title": "8  Non-linear modelling: Generalised Additive Models (GAMs)",
    "section": "8.2 What are Generalised Additive Models?",
    "text": "8.2 What are Generalised Additive Models?\n\n8.2.1 Essential Readings\n\nRead Chapter 7, ‘Generalised Additive Models’, in Zuur, Ieno, and Smith (2007).\n\n\n\n8.2.2 Today’s Session\n\n\n8.2.3 Learning Outcomes\nBy the end of this session you will be able to:\n\nExplore\nUse\nAnything approaches!\n\n\n\n8.2.4 Load libraries",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-linear modelling: Generalised Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "chap8.html#follow-up-work",
    "href": "chap8.html#follow-up-work",
    "title": "8  Non-linear modelling: Generalised Additive Models (GAMs)",
    "section": "8.3 Follow up work",
    "text": "8.3 Follow up work\n\nFinish the class exercises if you have not already done so.\nRead these three introductory papers on mixed modelling:\n\nStart with chapter 8 in Zuur, Ieno, and Smith (2007).\nThen Harrison et al. (2018).\nFinally, the classic introduction of Bolker et al. (2009).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-linear modelling: Generalised Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "chap8.html#next-week",
    "href": "chap8.html#next-week",
    "title": "8  Non-linear modelling: Generalised Additive Models (GAMs)",
    "section": "8.4 Next week",
    "text": "8.4 Next week\nWe conclude our statistical journey next week by examining how we can deploy mixed models, sometimes called random effects models, or hierarchical models, to account for heterogeneity in our data caused by temporal or spatial autocorrelation, or other structural issues (usually down to sampling errors).\n\n\n\n\nBolker, Benjamin M, Mollie E Brooks, Connie J Clark, Shane W Geange, John R Poulsen, Henry MH Stevens, and Jada-Simone S White. 2009. “Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution.” Trends in Ecology & Evolution 24 (3): 127–35. https://doi.org/10.1016/j.tree.2008.10.008.\n\n\nHarrison, Xavier A, Lynda Donaldson, Maria Correa-Cano, Julian Evans, David N Fisher, Cecily Goodwin, Beth S Robinson, David J Hodgson, and Richard Inger. 2018. “A Brief Introduction to Mixed Effects Modelling and Multi-Model Inference in Ecology.” PeerJ 6: e4794. https://doi.org/10.7717/peerj.4794.\n\n\nZuur, Alain, Elena N. Ieno, and Graham M. Smith. 2007. Analyzing Ecological Data. New York, NY, UNITED STATES: Springer New York.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-linear modelling: Generalised Additive Models (GAMs)</span>"
    ]
  },
  {
    "objectID": "chap9.html#what-are-mixed-models",
    "href": "chap9.html#what-are-mixed-models",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "9.3 What are mixed models?",
    "text": "9.3 What are mixed models?\nYou are all familiar with applying regression models to data using fixed effects, the \\(x\\) (or explanatory) components in the models. Fixed effects are effects of predictors that are assumed to be the same across all units (e.g. treatment, temperature, altitude). A mixed model is a regression model that includes both:\n\nFixed effects and\nRandom effects. Random effects allow certain parameters to vary across groups / clusters / units (e.g. intercepts, slopes for individual subjects, plots, regions). They model correlation or non-independence in the data.\n\nThey are useful when data are hierarchically structured (e.g. measurements nested within subjects, spatial clusters, repeated measurements over time) or when you have grouped / clustered observations.\n\n9.3.1 The key Idea: Variance Partitioning\n\nFixed effects explain systematic, population-wide variation.\nRandom effects explain the clustering/nesting, and the\nResiduals capture leftover within-group variation.\n\nSo the model partitions variance into:\n\nBetween-groups (random effects)\nWithin-groups (residuals).\n\n\n\n9.3.2 Visualising a mixed model\nYou will remember from numerous sessions that a regression generates two coefficients, an intercept and a slope (for all the explanatory variables). Mixed models use the random structure in the model to capture variability in two ways. You can use them to generate:\n\na random intercept model - where we allow the intercepts of our explanatory variables to vary, or\na random intercept and slope model - where both the intercept and slopes can vary (Fig. 9.2).\n\n\n\n\n\n\n\n\n\n\nFig. 9.2: Examples of a random intercept (left panel) and a random intercept and random slope model (right panel).The black line = fixed effect (population regression line). The coloured lines represent the various groups\nThe left panel in Figure 9.2 shows the situation where the slopes of the various group measurements are the same but where the intercepts differ. Look at where the lines sit on the Y axis. The panel on the right shows the situation where both the intercepts and the slopes vary. You have seen this before in week 4 where you examined the ‘RIKX.csv’ dataset. We will revisit those data today.\n\n\n9.3.3 A little bit of maths\nWe can represent these models in same mathematical way as we did with LMs and GLMs. The extra element is the addition of the grouping variable (this will be a factor class variable):\nGeneral Form (Linear Mixed Model, LMM)\n\\[\ny_{ij} = \\beta_0 + \\beta_1 x_{ij} + u_{0j} + u_{1j}x_{ij} + \\varepsilon_{ij}\n\\] Where:\n\n\\(y_{ij}\\): outcome for observation i in group j\n\n\\(\\beta_0, \\beta_1\\): fixed effects (population-level intercept and slope)\n\n\\(u_{0j}, u_{1j}\\): random effects (group-level deviations, e.g. each subject has its own intercept/slope)\n\n\\(\\varepsilon_{ij}\\): residual error (within-group variability)\n\nGeneral Form (Generalized Linear Mixed Model, GLMM):\nThe model for the expected outcome \\(\\mu_{ij} = E[y_{ij}]\\) is:\n\\[\ng(\\mu_{ij}) = \\beta_0 + \\beta_1 x_{ij} + u_{0j} + u_{1j}x_{ij}\n\\]\nWhere:\n\n\\(y_{ij}\\): the observed response (e.g., a count of events).\n\\(g(\\cdot)\\): link function (e.g., logit for binary data, log for counts).\n\\(\\beta_0, \\beta_1\\): fixed effects (population-level intercept and slope).\n\\(u_{0j}, u_{1j}\\): random effects (group-level deviations).\n\n\n\n9.3.4 Take a deep breath and bookmark this\nI appreciate this is a little complex, but it is very similar to the ANCOVA examples we covered earlier in the module, in week 6, with the key difference being that we place the group (factor) variable in the random rather than fixed structure of the model. Luckily this stuff is well covered in the web. Check out the code driven example of how to use these models using the made up example of dragons on mountain tops. This site is a superb resource, please explore.\n\n\n9.3.5 Quick Checklist for Students\n\nOne observation per group? → fixed effect.\nMany observations per group? → random effects might be needed.\nDo you want inference about the group itself (fixed) or about variation among groups (random)?\nHow many levels do we need need in a random effect? → According to many certainly 3 or more levels, but realistically ~ 10 or more.\n\nRemember: if you only have one replicate / measurement you cannot calculate anything!!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  },
  {
    "objectID": "chap9.html#next-week",
    "href": "chap9.html#next-week",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "9.4 Next Week",
    "text": "9.4 Next Week\nNext week I will introduce you to the datasets you’ll be using for your assessment for Part A of this module. There is a choice of four. The data are simulated to conform to particular hydro-ecological scenarios.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  },
  {
    "objectID": "chap9.html#references",
    "href": "chap9.html#references",
    "title": "9  Dealing with data heterogeneity using mixed models",
    "section": "9.5 References",
    "text": "9.5 References\n\n\n\n\nBates, Adam J, Poppy Fraser, Lucy Robinson, John C Tweddle, Jon P Sadler, Sarah E West, Simon Norman, Martin Batson, and Linda Davies. 2015. “The OPAL Bugs Count Survey: Exploring the Effects of Urbanisation and Habitat Characteristics Using Citizen Science.” Urban Ecosystems. https://doi.org/10.1007/s11252-015-0470-8.\n\n\nBolker, Benjamin M, Mollie E Brooks, Connie J Clark, Shane W Geange, John R Poulsen, Henry MH Stevens, and Jada-Simone S White. 2009. “Generalized Linear Mixed Models: A Practical Guide for Ecology and Evolution.” Trends in Ecology & Evolution 24 (3): 127–35. https://doi.org/10.1016/j.tree.2008.10.008.\n\n\nBrooks, Mollie E., Kasper Kristensen, Koen J. van Benthem, Arni Magnusson, Casper W. Berg, Anders Nielsen, Hans J. Skaug, Martin Mächler, and Benjamin M. Bolker. 2017. “glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling.” The R Journal 9 (2): 378–400.\n\n\nHarrison, Xavier A, Lynda Donaldson, Maria Correa-Cano, Julian Evans, David N Fisher, Cecily Goodwin, Beth S Robinson, David J Hodgson, and Richard Inger. 2018. “A Brief Introduction to Mixed Effects Modelling and Multi-Model Inference in Ecology.” PeerJ 6: e4794. https://doi.org/10.7717/peerj.4794.\n\n\nHartig, Florian. 2024. “DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression Models. R Package Version 0.4.7.”\n\n\nZuur, Alain, Elena N. Ieno, and Graham M. Smith. 2007. Analyzing Ecological Data. New York, NY, UNITED STATES: Springer New York.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with data heterogeneity using mixed models</span>"
    ]
  }
]